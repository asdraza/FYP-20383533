{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795d4be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6709803969580043\n",
      "RMSE: 1.1802692335254894\n",
      "R-squared: 0.9905780981775942\n",
      "RAE: 0.07007735214500166\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(columns=['Eligible'])\n",
    "    y = data['Eligible']\n",
    "    return X, y\n",
    "\n",
    "def train_svr_model(X_train, y_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    svr_model = SVR()\n",
    "    svr_model.fit(X_train_scaled, y_train)\n",
    "    return svr_model, scaler\n",
    "\n",
    "# Load and train on VehicleTrainingDataset.csv\n",
    "X_train, y_train = load_and_preprocess_data('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "svr_model, scaler = train_svr_model(X_train, y_train)\n",
    "\n",
    "# Predict eligibility scores on 1000VehicleDataset.csv\n",
    "vehicles_df = pd.read_csv('1000VehicleDataset_Noisy_0.01.csv')\n",
    "X_test = vehicles_df.drop(columns=['Eligible'])\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "predicted_scores = svr_model.predict(X_test_scaled)\n",
    "\n",
    "# Assuming you have access to actual scores, replace this line with the actual score loading logic if available\n",
    "y_actual = vehicles_df['Eligible']  # This would be prior to overwriting with predictions if you run this block again\n",
    "\n",
    "# Replace actual scores with predicted ones\n",
    "vehicles_df['Eligible'] = predicted_scores  \n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_actual, predicted_scores)\n",
    "rmse = np.sqrt(mean_squared_error(y_actual, predicted_scores))\n",
    "r_squared = r2_score(y_actual, predicted_scores)\n",
    "rae = np.sum(np.abs(y_actual - predicted_scores)) / np.sum(np.abs(y_actual - np.mean(y_actual)))\n",
    "\n",
    "# Output the results\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"RAE: {rae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c73f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -194.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 3.933333333333333\n",
      "All assignments history: [7, 5, 6, 4, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -188     |\n",
      "| time/              |          |\n",
      "|    fps             | 171      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -134.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.1\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -187         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 169          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077712727 |\n",
      "|    clip_fraction        | 0.0664       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | -0.257       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.86         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0435      |\n",
      "|    value_loss           | 17.8         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -162.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.177777777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008715391 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.0339     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.83        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0458     |\n",
      "|    value_loss           | 12.5        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.816666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008210508 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | -0.00714    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.44        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    value_loss           | 10.8        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.64\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008691443 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0299      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.34        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 9.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.533333333333335\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -187       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 37         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00954874 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.49      |\n",
      "|    explained_variance   | 0.00351    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.23       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    value_loss           | 8.22       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -134.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.466666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010143226 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0217      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    value_loss           | 6.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.258333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009758196 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.0293      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.08        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    value_loss           | 6.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.251851851851853\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -186       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 56         |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01132318 |\n",
      "|    clip_fraction        | 0.2        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.47      |\n",
      "|    explained_variance   | 0.0325     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.7        |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0445    |\n",
      "|    value_loss           | 5.74       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.06\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112780575 |\n",
      "|    clip_fraction        | 0.208        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.47        |\n",
      "|    explained_variance   | 0.0487       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.76         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0434      |\n",
      "|    value_loss           | 5.55         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.8969696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013364592 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.0578      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.58        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0487     |\n",
      "|    value_loss           | 4.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.17777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013779466 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.073       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 5.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.96410256410256\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016142784 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.28        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 4.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.352380952380955\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 86         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01603437 |\n",
      "|    clip_fraction        | 0.327      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.45      |\n",
      "|    explained_variance   | 0.156      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.73       |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.054     |\n",
      "|    value_loss           | 4.95       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.34222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016389698 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 4.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.12916666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013207305 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.979       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 4.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.73725490196078\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 104        |\n",
      "|    total_timesteps      | 17408      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01274107 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.44      |\n",
      "|    explained_variance   | 0.342      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.438      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0504    |\n",
      "|    value_loss           | 4.65       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.18888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011728017 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.49        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 4.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.498245614035085\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010523299 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 4.25        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    value_loss           | 4.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.663333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 117         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010529513 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.09        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 4.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.698412698412696\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 175         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011077741 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.35        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 4.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.56969696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 178         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010938469 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 4.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.405797101449274\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010245932 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.83        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 4.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.09444444444444\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 184        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 133        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00930264 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.38      |\n",
      "|    explained_variance   | 0.526      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.691      |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0454    |\n",
      "|    value_loss           | 4.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.818666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009892894 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.541       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.662       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0493     |\n",
      "|    value_loss           | 4.21        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.55384615384615\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009088651 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.36       |\n",
      "|    explained_variance   | 0.555       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.74        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    value_loss           | 4.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.264197530864195\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008661218 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.34       |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 4.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.833333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009399448 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.33       |\n",
      "|    explained_variance   | 0.587       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.502       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 4.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.41379310344828\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007945607 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.32       |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 4.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.99111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009521697 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.3        |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.515       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0474     |\n",
      "|    value_loss           | 4.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.47741935483871\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008613924 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.29       |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.79        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 3.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.92916666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008564874 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.26       |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    value_loss           | 4.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.36363636363637\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 166         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008286214 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.23       |\n",
      "|    explained_variance   | 0.629       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.869       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    value_loss           | 3.96        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.80196078431373\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008597241 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.2        |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 3.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.19619047619047\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008699527 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.17       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 3.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.62592592592593\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009648142 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.13       |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.799       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 3.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.016216216216215\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 179         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008816883 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.1        |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.38        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 3.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.45263157894737\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 182         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008936709 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.06       |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.426       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 3.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.8974358974359\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008925639 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.02       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.568       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0474     |\n",
      "|    value_loss           | 3.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.41833333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -164       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 189        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00834257 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.96      |\n",
      "|    explained_variance   | 0.69       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.64       |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0484    |\n",
      "|    value_loss           | 3.63       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.86991869918699\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -162         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 218          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 192          |\n",
      "|    total_timesteps      | 41984        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075304676 |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.91        |\n",
      "|    explained_variance   | 0.716        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.938        |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0455      |\n",
      "|    value_loss           | 3.66         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.37777777777778\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -160         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 219          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 195          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086672455 |\n",
      "|    clip_fraction        | 0.159        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.85        |\n",
      "|    explained_variance   | 0.735        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.76         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.0516      |\n",
      "|    value_loss           | 3.48         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.82945736434109\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -158        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 199         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008199937 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.78       |\n",
      "|    explained_variance   | 0.769       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.558       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 3.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.27424242424242\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -156        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 222         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 202         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010059409 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.73       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.996       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 3.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.70074074074074\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -154        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 224         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009804735 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.7        |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.31        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.26376811594203\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -151        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 225         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 209         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010045996 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.64       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.754       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.791489361702126\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -149       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 212        |\n",
      "|    total_timesteps      | 48128      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00827891 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.61      |\n",
      "|    explained_variance   | 0.739      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.45       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.051     |\n",
      "|    value_loss           | 3.39       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.35694444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -147        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008678613 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.57       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 3.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.94421768707483\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -144       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 219        |\n",
      "|    total_timesteps      | 50176      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01082236 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.5       |\n",
      "|    explained_variance   | 0.747      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.678      |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0583    |\n",
      "|    value_loss           | 3.08       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.40533333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -141        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 222         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008944392 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.43       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.9032679738562\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -139        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009577664 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.38       |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.61        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.47820512820513\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -136        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 229         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009860482 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.496       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.03270440251572\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -133       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 233        |\n",
      "|    iterations           | 53         |\n",
      "|    time_elapsed         | 232        |\n",
      "|    total_timesteps      | 54272      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01016693 |\n",
      "|    clip_fraction        | 0.201      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.33      |\n",
      "|    explained_variance   | 0.826      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.852      |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0524    |\n",
      "|    value_loss           | 2.43       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.56666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -131        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009263724 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.32       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.705       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.12727272727273\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -128        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 239         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008884899 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.28       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.834       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.66666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -125        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009248562 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.22       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.667       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.18362573099415\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -122        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 245         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010151224 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.22       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.769       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.68620689655172\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -120        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 249         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009881029 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.18       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.798       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.2\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -117        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009468974 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.16       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.579       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.71222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -115        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 255         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010349899 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.13       |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.722       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.23497267759562\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -113        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009723033 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.13       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.78        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.70322580645161\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -111        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 262         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010982738 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.13       |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.799       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.17777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -109        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 265         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009681167 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.11       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.628       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.63958333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -107        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009671142 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.09       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.851       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.08820512820513\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -105        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 271         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010781136 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.09       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.586       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.51010101010101\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -104        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 275         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010937917 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.07       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.515       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.95024875621891\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -102        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 278         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010161036 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.06       |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.662       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0565     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.36764705882354\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -100        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009088611 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.06       |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.58        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.78743961352657\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -98.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 284         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009469252 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.03       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.734       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.20571428571428\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -97.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 287         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010252035 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5          |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.523       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.57089201877935\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -96.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 290         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010240409 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.99       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.862       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.95\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -95.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 292         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009255936 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.95       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.608       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0565     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.34246575342466\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -94.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009068509 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.94       |\n",
      "|    explained_variance   | 0.719       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.43        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.74324324324324\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -92.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 297         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010006799 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.89       |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.693       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.14666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -91.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 300         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009558918 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.86       |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.496       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0573     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.56929824561404\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -90.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 303         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010617823 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.83       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.724       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.95757575757575\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -89.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 257         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009386281 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.81       |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.793       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.36666666666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -88.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 259        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 308        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00937644 |\n",
      "|    clip_fraction        | 0.2        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.79      |\n",
      "|    explained_variance   | 0.725      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.726      |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.054     |\n",
      "|    value_loss           | 2.11       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.7451476793249\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -87.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 259         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 311         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010617539 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.77       |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.816       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.13916666666667\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -86.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 260          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 314          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096364245 |\n",
      "|    clip_fraction        | 0.214        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.77        |\n",
      "|    explained_variance   | 0.751        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.733        |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.0543      |\n",
      "|    value_loss           | 1.93         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.54156378600823\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -84.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010135216 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.76       |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.6         |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.9390243902439\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -83.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 319         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009767408 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.73       |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.713       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.30040160642571\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -82.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009369418 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.74       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.688       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.67698412698412\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -82         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 324         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009677706 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.72       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.555       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.05882352941177\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -80.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 265         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 327         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009260451 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.66       |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.923       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0574     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.43643410852714\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -79.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 266         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009104404 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.63       |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.853       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.8\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -78.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 267         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 332         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009814685 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.58       |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.986       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.18484848484849\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -77         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 268         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 335         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011093767 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.56       |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.577       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.57153558052434\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -75.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 269         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 338         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009483234 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.56       |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.757       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.95555555555555\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -74.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 270        |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 341        |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00895617 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.55      |\n",
      "|    explained_variance   | 0.579      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.716      |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.0528    |\n",
      "|    value_loss           | 2.51       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.34065934065934\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -72.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 270         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008980272 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.52       |\n",
      "|    explained_variance   | 0.617       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.581       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.72971014492754\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -70.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 346         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008575346 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.45       |\n",
      "|    explained_variance   | 0.57        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.10035842293907\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -68.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 349         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008426032 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.4        |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.933       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.46312056737588\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -67.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 273         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 351         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009753719 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.35       |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.629       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.82666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -65.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 354         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009065479 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.587       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.702       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.19375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -63.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 275         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 357         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009673972 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.783       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.56563573883162\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -61.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 275         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 360         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009956603 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.27       |\n",
      "|    explained_variance   | 0.591       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.822       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.91360544217687\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -60.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 362         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009812795 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0.624       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.632       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.26868686868687\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -58.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 365         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009437362 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.21       |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.604\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -56.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 278          |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 367          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090875495 |\n",
      "|    clip_fraction        | 0.174        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.22        |\n",
      "|    explained_variance   | 0.578        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.637        |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.0486      |\n",
      "|    value_loss           | 2.26         |\n",
      "------------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 20.44\n",
      "Overall Average Successful Assignments: 61.75048652162318\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcac0c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Testing Summary ----\n",
      "Mean Reward: 60.0\n",
      "Standard Deviation of Reward: 0.0\n",
      "Average Successful Assignments in Testing: 130.0\n"
     ]
    }
   ],
   "source": [
    "# Load new task dataset for testing\n",
    "new_tasks_df = pd.read_csv('RandomTasks200Test.csv')\n",
    "new_tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "test_env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, new_tasks_df), n_envs=1)\n",
    "\n",
    "# Evaluate the model on the new test environment\n",
    "mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=10)\n",
    "# Extract the successful assignments history from the test environment\n",
    "successful_assignments = test_env.envs[0].env.get_average_success()\n",
    "\n",
    "print(\"---- Testing Summary ----\")\n",
    "print(f\"Mean Reward: {mean_reward}\")\n",
    "print(f\"Standard Deviation of Reward: {std_reward}\")\n",
    "print(f\"Average Successful Assignments in Testing: {successful_assignments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cbb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
