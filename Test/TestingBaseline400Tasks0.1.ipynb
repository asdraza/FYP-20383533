{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c311d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -394.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 4.666666666666667\n",
      "All assignments history: [16, 10, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -374     |\n",
      "| time/              |          |\n",
      "|    fps             | 307      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -294.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.416666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008463504 |\n",
      "|    clip_fraction        | 0.0563      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.319      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.53        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 17.9        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -382.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.02777777777778\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -376       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 263        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01054409 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | -0.0616    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.76       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0547    |\n",
      "|    value_loss           | 13.2       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -346.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.145833333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -376        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 258         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012532168 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0892      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.14        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 10.8        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -316.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.316666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -376        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014016572 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.00394    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.28        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 9.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -396.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.84722222222222\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -376         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0139839295 |\n",
      "|    clip_fraction        | 0.246        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | 0.0183       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.05         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0558      |\n",
      "|    value_loss           | 8.71         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -396.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.452380952380953\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -376        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015179821 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.0182     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 7.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -396.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.65625\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -375       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 240        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 34         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01739198 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.00394    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.38       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0625    |\n",
      "|    value_loss           | 5.95       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -396.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.222222222222221\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017837733 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.00652    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 5.34        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -394.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.175\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -375       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 235        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 43         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01912384 |\n",
      "|    clip_fraction        | 0.41       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.00205    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.977      |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0621    |\n",
      "|    value_loss           | 4.45       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -384.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.712121212121213\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020056639 |\n",
      "|    clip_fraction        | 0.442       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | -0.00328    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.866       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 3.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -292.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.520833333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021208784 |\n",
      "|    clip_fraction        | 0.452       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00551     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 3.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -274.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.46794871794872\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -374       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 59         |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02295197 |\n",
      "|    clip_fraction        | 0.505      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.00277    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.96       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0644    |\n",
      "|    value_loss           | 3.09       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -194.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.392857142857142\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024794245 |\n",
      "|    clip_fraction        | 0.542       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00426     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.03        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 3.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -310.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.43888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025700007 |\n",
      "|    clip_fraction        | 0.565       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.000742    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.596       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -210.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.921875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027365644 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00158     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.5735294117647\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027414175 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0038      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.323      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -198.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.504629629629626\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -374       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 83         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02930717 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0165     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.498      |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0642    |\n",
      "|    value_loss           | 2.94       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -244.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.13157894736842\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026271813 |\n",
      "|    clip_fraction        | 0.555       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0712      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.338      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -256.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.3375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 220         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025217649 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -240.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.73412698412698\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025264926 |\n",
      "|    clip_fraction        | 0.521       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.298      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.571969696969695\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025186235 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.348      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.64855072463768\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023235187 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.168       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.266      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -204.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.208333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022547789 |\n",
      "|    clip_fraction        | 0.451       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.308      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.34\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020838987 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.704       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0587     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.03525641025641\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 220         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021966716 |\n",
      "|    clip_fraction        | 0.457       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.728       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 3.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.092592592592595\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 219         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020953277 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.23        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0593     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.657738095238095\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020155929 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.249       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0292      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.03448275862068\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019491924 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.249       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.725       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.3638888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021035425 |\n",
      "|    clip_fraction        | 0.441       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.52150537634408\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020273786 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.571       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0629     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.03125\n",
      "All assignments history: []\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 400      |\n",
      "|    ep_rew_mean          | -372     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 214      |\n",
      "|    iterations           | 32       |\n",
      "|    time_elapsed         | 152      |\n",
      "|    total_timesteps      | 32768    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.020947 |\n",
      "|    clip_fraction        | 0.396    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | -8.26    |\n",
      "|    explained_variance   | 0.28     |\n",
      "|    learning_rate        | 0.00018  |\n",
      "|    loss                 | 1.25     |\n",
      "|    n_updates            | 310      |\n",
      "|    policy_gradient_loss | -0.0628  |\n",
      "|    value_loss           | 2.87     |\n",
      "--------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.5050505050505\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 158        |\n",
      "|    total_timesteps      | 33792      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02030847 |\n",
      "|    clip_fraction        | 0.395      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.287      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.12       |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0625    |\n",
      "|    value_loss           | 2.83       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.41911764705883\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020045806 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.637       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.12142857142857\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018683434 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.638       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0628     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.95370370370371\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018700425 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.13        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.5608108108108\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018443441 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.147      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.40131578947368\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 186        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01815002 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.293      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0754    |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0585    |\n",
      "|    value_loss           | 2.85       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.15598290598291\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013217533 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.38541666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015969995 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.70325203252033\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014336481 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.213       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 91.04960317460318\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 209         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015960108 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.81        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 92.15697674418605\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 216         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013255373 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.635       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 93.3030303030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 222         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014078885 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.188       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 94.67592592592592\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013359177 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 95.58333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 233         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013619894 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.997       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 96.7517730496454\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 238         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015589641 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.196      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0606     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.890625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016425282 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.22278911564626\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 249         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014403728 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.68        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0591     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 100.22\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 255         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015163818 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 100.96078431372548\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 261         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015249768 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0647      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 101.89583333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 266         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013498133 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0024      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 102.75628930817611\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -370         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 199          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 272          |\n",
      "|    total_timesteps      | 54272        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135940295 |\n",
      "|    clip_fraction        | 0.238        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.23        |\n",
      "|    explained_variance   | 0.387        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | -0.0327      |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0572      |\n",
      "|    value_loss           | 2.62         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 103.59722222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 277         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015859794 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.15       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 104.48484848484848\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 283         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015460211 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 105.38541666666667\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -369      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 198       |\n",
      "|    iterations           | 56        |\n",
      "|    time_elapsed         | 289       |\n",
      "|    total_timesteps      | 57344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0188152 |\n",
      "|    clip_fraction        | 0.301     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.22     |\n",
      "|    explained_variance   | 0.414     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.00931   |\n",
      "|    n_updates            | 550       |\n",
      "|    policy_gradient_loss | -0.0576   |\n",
      "|    value_loss           | 2.58      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.29385964912281\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016143009 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.461       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0604     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 107.183908045977\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 300         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016699702 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.263       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0621     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.09180790960453\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017117595 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.9875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 311         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017111765 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0173     |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.76502732240438\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 317         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015729586 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.226       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.45564516129032\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016717626 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.31        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.1984126984127\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 328         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015071468 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0868      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.92057291666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -368       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 196        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 333        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01655458 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.434      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.432      |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0587    |\n",
      "|    value_loss           | 2.5        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 112.78461538461538\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015164802 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.246      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.69318181818181\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 344         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016935049 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.803       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 114.64676616915423\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 349         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014790566 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0593     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 115.46200980392157\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 354         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016795214 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.872       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.2427536231884\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 360         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015501219 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.582       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 117.18095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 366         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016277354 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.04929577464789\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 371         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015251499 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.527       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.83333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 376         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015975896 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.57990867579909\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 382         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015065096 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.41554054054055\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -364       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 195        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 387        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01701872 |\n",
      "|    clip_fraction        | 0.341      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.15      |\n",
      "|    explained_variance   | 0.443      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.45       |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0629    |\n",
      "|    value_loss           | 2.5        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.22333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -364       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 195        |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 392        |\n",
      "|    total_timesteps      | 76800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01688416 |\n",
      "|    clip_fraction        | 0.295      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.15      |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.132      |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0604    |\n",
      "|    value_loss           | 2.43       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.01425438596492\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016680893 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.594       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.78787878787878\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 403         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015986018 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.92        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.56089743589743\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -362       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 195        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 409        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01898621 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.13      |\n",
      "|    explained_variance   | 0.46       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.467      |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.0677    |\n",
      "|    value_loss           | 2.41       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.35548523206751\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 414         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018329803 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.311       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.12083333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 420         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016579578 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.22        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.84362139917695\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 425         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016922578 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.884       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.5640243902439\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 431         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016526401 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.504       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.27409638554217\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 436         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018597074 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.06       |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0677     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.96329365079364\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -360      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 194       |\n",
      "|    iterations           | 84        |\n",
      "|    time_elapsed         | 441       |\n",
      "|    total_timesteps      | 86016     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0160502 |\n",
      "|    clip_fraction        | 0.321     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.06     |\n",
      "|    explained_variance   | 0.487     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.332     |\n",
      "|    n_updates            | 830       |\n",
      "|    policy_gradient_loss | -0.0633   |\n",
      "|    value_loss           | 2.27      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.63039215686274\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 447         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016979951 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.116      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.27325581395348\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 452         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016574867 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.04       |\n",
      "|    explained_variance   | 0.424       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.161       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.9022988505747\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 458         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018522061 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.03       |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.125      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.5284090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -357        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 464         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018168423 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.01       |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.08426966292134\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 469         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018534157 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.99       |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.63055555555556\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -356       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 193        |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 475        |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01763021 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.98      |\n",
      "|    explained_variance   | 0.418      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0793     |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.0686    |\n",
      "|    value_loss           | 2.47       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.17399267399267\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -354        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 481         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017445091 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.96       |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.191       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.7264492753623\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -354        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 486         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018091211 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.391       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0746     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.247311827957\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -353      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 193       |\n",
      "|    iterations           | 93        |\n",
      "|    time_elapsed         | 492       |\n",
      "|    total_timesteps      | 95232     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0172283 |\n",
      "|    clip_fraction        | 0.324     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -7.92     |\n",
      "|    explained_variance   | 0.424     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | -0.128    |\n",
      "|    n_updates            | 920       |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 2.35      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.78191489361703\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -352        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 497         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017174598 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.89       |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.564       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.31929824561402\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -351        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 503         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018600931 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.84       |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.985       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0738     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.86545138888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -350        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 507         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017281968 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.81       |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.215       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0743     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.38144329896906\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -349        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 512         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019326065 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.79       |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.48        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.8971088435374\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -347        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 517         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019396825 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.74       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.618       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0735     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.3989898989899\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -346       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 194        |\n",
      "|    iterations           | 99         |\n",
      "|    time_elapsed         | 521        |\n",
      "|    total_timesteps      | 101376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01794425 |\n",
      "|    clip_fraction        | 0.389      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.7       |\n",
      "|    explained_variance   | 0.395      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.365      |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0757    |\n",
      "|    value_loss           | 2.62       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.9225\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -344        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 526         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019794052 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.66       |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0739     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -79.88\n",
      "Overall Average Successful Assignments: 87.85565124280647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Testing Summary ----\n",
      "Mean Reward: 50.0\n",
      "Standard Deviation of Reward: 0.0\n",
      "Average Successful Assignments in Testing: 225.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n",
    "\n",
    "# Load new task dataset for testing\n",
    "new_tasks_df = pd.read_csv('RandomTasks400Test.csv')\n",
    "new_tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "test_env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, new_tasks_df), n_envs=1)\n",
    "\n",
    "# Evaluate the model on the new test environment\n",
    "mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=10)\n",
    "# Extract the successful assignments history from the test environment\n",
    "successful_assignments = test_env.envs[0].env.get_average_success()\n",
    "\n",
    "print(\"---- Testing Summary ----\")\n",
    "print(f\"Mean Reward: {mean_reward}\")\n",
    "print(f\"Standard Deviation of Reward: {std_reward}\")\n",
    "print(f\"Average Successful Assignments in Testing: {successful_assignments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b8963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f0498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
