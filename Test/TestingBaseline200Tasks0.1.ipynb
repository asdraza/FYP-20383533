{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c311d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.133333333333333\n",
      "All assignments history: [9, 2, 5, 7, 4, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -189     |\n",
      "| time/              |          |\n",
      "|    fps             | 473      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -162.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.666666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 391         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008559162 |\n",
      "|    clip_fraction        | 0.0576      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.302      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.54        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 16.6        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.488888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013230387 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0518     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.45        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 10.6        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.15\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014453124 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.00805     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 9.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.24\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 379         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015454106 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0635      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 8.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.466666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 375         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015977997 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0812      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.56        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0628     |\n",
      "|    value_loss           | 7.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.18095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 370         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016640062 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0924      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0616     |\n",
      "|    value_loss           | 7.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.066666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 363         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018005813 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.41        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 5.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.348148148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 356         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018684236 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.77        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 5.53        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.2\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019440947 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 5.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.78181818181818\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 350         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021331582 |\n",
      "|    clip_fraction        | 0.472       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.45        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 4.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -96.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.433333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021820925 |\n",
      "|    clip_fraction        | 0.494       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.52        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 4.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.487179487179485\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 341         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022315338 |\n",
      "|    clip_fraction        | 0.5         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.297       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.772       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 4.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.252380952380953\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 42         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02323788 |\n",
      "|    clip_fraction        | 0.514      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.393      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.31       |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0646    |\n",
      "|    value_loss           | 3.99       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.093333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 333         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023440983 |\n",
      "|    clip_fraction        | 0.497       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 4.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.783333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 332         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024640506 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 4.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -122.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.615686274509805\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 328         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027149538 |\n",
      "|    clip_fraction        | 0.566       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.413       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0736     |\n",
      "|    value_loss           | 4.16        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.414814814814815\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 323         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023443457 |\n",
      "|    clip_fraction        | 0.512       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.976       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 3.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.09122807017544\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 320         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023236405 |\n",
      "|    clip_fraction        | 0.524       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.581       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0722     |\n",
      "|    value_loss           | 3.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.713333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 315         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022181781 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.592       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 3.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.812698412698413\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 312         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019492755 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.594       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.39393939393939\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 312        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 72         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02045717 |\n",
      "|    clip_fraction        | 0.462      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.604      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.08       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0676    |\n",
      "|    value_loss           | 4.1        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.47826086956522\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 309         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018768076 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.637       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 3.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.4\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -182      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 307       |\n",
      "|    iterations           | 24        |\n",
      "|    time_elapsed         | 79        |\n",
      "|    total_timesteps      | 24576     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0200961 |\n",
      "|    clip_fraction        | 0.436     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.26     |\n",
      "|    explained_variance   | 0.661     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.377     |\n",
      "|    n_updates            | 230       |\n",
      "|    policy_gradient_loss | -0.0716   |\n",
      "|    value_loss           | 3.51      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.41866666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 303         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019076634 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0659     |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.18205128205128\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018640783 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.715       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 3.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.18271604938272\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 296         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018204926 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.533       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.72857142857143\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 293        |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 97         |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01755349 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.717      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.751      |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0651    |\n",
      "|    value_loss           | 3.26       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.154022988505744\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 292         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018323284 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.63777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 291         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017241077 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0506     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 3.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.22150537634408\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 289         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018578086 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.80416666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 289         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016954204 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0656     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.505050505050505\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 284         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018739477 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.556       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.272549019607844\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018985314 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.42        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.99619047619048\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015658442 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.714       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.68333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018446982 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.972       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.39099099099099\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019525848 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.802       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.93333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018248016 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0331      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.567521367521366\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018266723 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.821       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.211666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 275         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016117342 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.534       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.75121951219512\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 273         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014923503 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.186       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.285714285714285\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 157         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015595194 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.606       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.07       |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.83255813953488\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -181         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 271          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 162          |\n",
      "|    total_timesteps      | 44032        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0151478015 |\n",
      "|    clip_fraction        | 0.284        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.24        |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.188        |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0688      |\n",
      "|    value_loss           | 2.36         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.372727272727275\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 268         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017009106 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0703     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.854814814814816\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 267         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014896796 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.751       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.31304347826087\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016915608 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.48        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.687943262411345\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 183         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015348986 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.94305555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018227458 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0358     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0733     |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.35374149659864\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015975192 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0393      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.79333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 259         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016476307 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.152       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.20130718954248\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 258         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 202         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017777063 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.604       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 1.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.61923076923077\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 257         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 206         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016286137 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.132       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.03522012578616\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 212         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013774727 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.311       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0658     |\n",
      "|    value_loss           | 1.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.43703703703704\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 217         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015589618 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.473       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 1.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.8169696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 221         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016558975 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.159       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.17619047619048\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017299538 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0173     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 1.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.49356725146199\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 230         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016711276 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0878      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.07       |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.826436781609196\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016399387 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0067     |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0748     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.12994350282486\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017324492 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.241       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0766     |\n",
      "|    value_loss           | 1.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.416666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 245         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017849913 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.254       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0754     |\n",
      "|    value_loss           | 1.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.690710382513664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 250         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017641053 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0304     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0748     |\n",
      "|    value_loss           | 1.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.92365591397849\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 254         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016455675 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0265     |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0737     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.17037037037037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 260         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016135897 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.356       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.47291666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 265         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017773245 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.198       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.076      |\n",
      "|    value_loss           | 1.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.76512820512821\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 270         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016386285 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0385     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0745     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.04242424242424\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 275         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017131506 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.257       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.075      |\n",
      "|    value_loss           | 1.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.31741293532338\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 280         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017337076 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00494     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0773     |\n",
      "|    value_loss           | 1.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.51274509803922\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 285         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019579563 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00238    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0815     |\n",
      "|    value_loss           | 1.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.745893719806766\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 242        |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 291        |\n",
      "|    total_timesteps      | 70656      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01885732 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.918      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0718    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.077     |\n",
      "|    value_loss           | 1.31       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.90761904761905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017545108 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00621     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0787     |\n",
      "|    value_loss           | 1.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.04882629107981\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 300         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017821398 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.084       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0784     |\n",
      "|    value_loss           | 1.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.21018518518518\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 241        |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 304        |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01901379 |\n",
      "|    clip_fraction        | 0.398      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.921      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.116     |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.0798    |\n",
      "|    value_loss           | 1.25       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.40365296803653\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018244691 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0996     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.076      |\n",
      "|    value_loss           | 1.35        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.567567567567565\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016990984 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.244       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0771     |\n",
      "|    value_loss           | 1.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.75644444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 319         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018127536 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0793     |\n",
      "|    value_loss           | 1.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.94122807017544\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 324         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019584496 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0561      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0813     |\n",
      "|    value_loss           | 1.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.114285714285714\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 329         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018733632 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0488     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0809     |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.27350427350427\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 334         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016510913 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.243      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.52405063291139\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 340         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019442229 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0235      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0803     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.740833333333335\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 236        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 345        |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01786365 |\n",
      "|    clip_fraction        | 0.387      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.921      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0355     |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | -0.0824    |\n",
      "|    value_loss           | 1.34       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.93251028806584\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019102335 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0962      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0837     |\n",
      "|    value_loss           | 1.32        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.139024390243904\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 355         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019762127 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0681     |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.081      |\n",
      "|    value_loss           | 1.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.35261044176707\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 360         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020092484 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.102      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0807     |\n",
      "|    value_loss           | 1.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.5563492063492\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 365         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017910782 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.12        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0817     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.75686274509804\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 371         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019315146 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0591     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.084      |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.972093023255816\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 378         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017631449 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.252      |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0781     |\n",
      "|    value_loss           | 1.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.18390804597701\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018772602 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.156       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0832     |\n",
      "|    value_loss           | 1.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.432575757575755\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 389         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022213165 |\n",
      "|    clip_fraction        | 0.458       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.216      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0862     |\n",
      "|    value_loss           | 1.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.655430711610485\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 395         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019491045 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00784     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0806     |\n",
      "|    value_loss           | 1.24        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.873333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 400         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019333366 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0843     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0846     |\n",
      "|    value_loss           | 1.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.081318681318685\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -170       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 91         |\n",
      "|    time_elapsed         | 406        |\n",
      "|    total_timesteps      | 93184      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02107492 |\n",
      "|    clip_fraction        | 0.442      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.07      |\n",
      "|    explained_variance   | 0.924      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.133     |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0858    |\n",
      "|    value_loss           | 1.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.323188405797104\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 412         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017144082 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.06       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0355      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0808     |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.556989247311826\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 418         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017889194 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.04       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0808     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0808     |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.777304964539006\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 226         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 424         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020222813 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0862     |\n",
      "|    value_loss           | 1.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.98315789473684\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 226         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 430         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017660357 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.99       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.183      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0827     |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.19861111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 225         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018288136 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.97       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.148      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0814     |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.44398625429553\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 225         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 440         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018135812 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.95       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0879     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0832     |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.70204081632653\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 225         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 445         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017686658 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.114       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0806     |\n",
      "|    value_loss           | 1.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.96026936026936\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 224         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 450         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020939428 |\n",
      "|    clip_fraction        | 0.442       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.9        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.201      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0864     |\n",
      "|    value_loss           | 1.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.216\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -163       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 456        |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01871309 |\n",
      "|    clip_fraction        | 0.379      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.87      |\n",
      "|    explained_variance   | 0.892      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0408     |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.0791    |\n",
      "|    value_loss           | 1.52       |\n",
      "----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -22.1\n",
      "Overall Average Successful Assignments: 46.75150516860241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Testing Summary ----\n",
      "Mean Reward: 18.0\n",
      "Standard Deviation of Reward: 0.0\n",
      "Average Successful Assignments in Testing: 109.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n",
    "\n",
    "# Load new task dataset for testing\n",
    "new_tasks_df = pd.read_csv('RandomTasks200Test.csv')\n",
    "new_tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "test_env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, new_tasks_df), n_envs=1)\n",
    "\n",
    "# Evaluate the model on the new test environment\n",
    "mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=10)\n",
    "# Extract the successful assignments history from the test environment\n",
    "successful_assignments = test_env.envs[0].env.get_average_success()\n",
    "\n",
    "print(\"---- Testing Summary ----\")\n",
    "print(f\"Mean Reward: {mean_reward}\")\n",
    "print(f\"Standard Deviation of Reward: {std_reward}\")\n",
    "print(f\"Average Successful Assignments in Testing: {successful_assignments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b8963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f0498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
