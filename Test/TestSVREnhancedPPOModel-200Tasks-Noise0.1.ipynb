{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1eabb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.403494699070916\n",
      "RMSE: 1.9135086312664382\n",
      "R-squared: 0.9752350615552381\n",
      "RAE: 0.14658131997050208\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(columns=['Eligible'])\n",
    "    y = data['Eligible']\n",
    "    return X, y\n",
    "\n",
    "def train_svr_model(X_train, y_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    svr_model = SVR()\n",
    "    svr_model.fit(X_train_scaled, y_train)\n",
    "    return svr_model, scaler\n",
    "\n",
    "# Load and train on VehicleTrainingDataset.csv\n",
    "X_train, y_train = load_and_preprocess_data('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "svr_model, scaler = train_svr_model(X_train, y_train)\n",
    "\n",
    "# Predict eligibility scores on 1000VehicleDataset.csv\n",
    "vehicles_df = pd.read_csv('1000VehicleDataset_Noisy_0.1.csv')\n",
    "X_test = vehicles_df.drop(columns=['Eligible'])\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "predicted_scores = svr_model.predict(X_test_scaled)\n",
    "\n",
    "# Assuming you have access to actual scores, replace this line with the actual score loading logic if available\n",
    "y_actual = vehicles_df['Eligible']  # This would be prior to overwriting with predictions if you run this block again\n",
    "\n",
    "# Replace actual scores with predicted ones\n",
    "vehicles_df['Eligible'] = predicted_scores  \n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_actual, predicted_scores)\n",
    "rmse = np.sqrt(mean_squared_error(y_actual, predicted_scores))\n",
    "r_squared = r2_score(y_actual, predicted_scores)\n",
    "rae = np.sum(np.abs(y_actual - predicted_scores)) / np.sum(np.abs(y_actual - np.mean(y_actual)))\n",
    "\n",
    "# Output the results\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"RAE: {rae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e4c77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 8.866666666666667\n",
      "All assignments history: [8, 11, 8, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -183     |\n",
      "| time/              |          |\n",
      "|    fps             | 187      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.733333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007873634 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.365      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.29        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    value_loss           | 17.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.755555555555556\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -183         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086816475 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | -0.0801      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.67         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0456      |\n",
      "|    value_loss           | 12.7         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.85\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008834034 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | -0.0153     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 8.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.88\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008313468 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0244      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.97        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    value_loss           | 9.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -122.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008428613 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0381      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.01        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 8.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.35238095238095\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009377984 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0582      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.9         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    value_loss           | 7.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.916666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009941315 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.0592      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    value_loss           | 6.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -156.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.4\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009629935 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.0666      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.79        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 6.1         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -96.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.98\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110326735 |\n",
      "|    clip_fraction        | 0.2          |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.47        |\n",
      "|    explained_variance   | 0.094        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.57         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0451      |\n",
      "|    value_loss           | 5.1          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.169696969696968\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011685632 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.834       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    value_loss           | 4.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.377777777777776\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013012256 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.207       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 4.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.646153846153847\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014682993 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 4.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.623809523809523\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 176         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012740446 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0498     |\n",
      "|    value_loss           | 4.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.022222222222226\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 179        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 85         |\n",
      "|    total_timesteps      | 15360      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01507846 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.46      |\n",
      "|    explained_variance   | 0.401      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 4.08       |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0578    |\n",
      "|    value_loss           | 4.19       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.025\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014614672 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.478       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.64        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 4.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.51372549019608\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013626566 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 3.9         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.007407407407406\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013551492 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.289       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 3.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.29824561403509\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013582101 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 3.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.70333333333333\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -184         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 197          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 103          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0132940095 |\n",
      "|    clip_fraction        | 0.266        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.44        |\n",
      "|    explained_variance   | 0.592        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.43         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.057       |\n",
      "|    value_loss           | 3.85         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.87619047619047\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013019985 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.841       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 3.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.88181818181818\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011533155 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 3.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.802898550724635\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -183         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 23552        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110313175 |\n",
      "|    clip_fraction        | 0.199        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.43        |\n",
      "|    explained_variance   | 0.655        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.13         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0532      |\n",
      "|    value_loss           | 3.69         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.56944444444444\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -183         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0109975375 |\n",
      "|    clip_fraction        | 0.199        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.43        |\n",
      "|    explained_variance   | 0.66         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.897        |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0526      |\n",
      "|    value_loss           | 3.54         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.33866666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010359852 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 3.2         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.02307692307692\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010646006 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.717       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 3.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.762962962962966\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010758225 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.721       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 3.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.523809523809526\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011142402 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.306       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.239080459770115\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 218         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010622111 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.458       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.94\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 139        |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01016618 |\n",
      "|    clip_fraction        | 0.181      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.36      |\n",
      "|    explained_variance   | 0.752      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.33       |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.0532    |\n",
      "|    value_loss           | 2.84       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.65806451612903\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010422608 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.4375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 225         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 145         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010648094 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.34       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.967       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.22828282828283\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010412961 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.33       |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.368       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.043137254901964\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010009427 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.31       |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.598       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.8\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -176      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 230       |\n",
      "|    iterations           | 35        |\n",
      "|    time_elapsed         | 155       |\n",
      "|    total_timesteps      | 35840     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0101094 |\n",
      "|    clip_fraction        | 0.198     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -6.29     |\n",
      "|    explained_variance   | 0.794     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.09      |\n",
      "|    n_updates            | 340       |\n",
      "|    policy_gradient_loss | -0.0555   |\n",
      "|    value_loss           | 2.42      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.53333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010197804 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.27       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.817       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.29369369369369\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009586556 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.23       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.63        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0565     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.06666666666667\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -172         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 235          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 165          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073004896 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.2         |\n",
      "|    explained_variance   | 0.806        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.397        |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0489      |\n",
      "|    value_loss           | 2.34         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.796581196581194\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008878473 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.15       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.894       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.51833333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009418698 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.1        |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.578       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.229268292682924\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009731118 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.05       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.39        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.93809523809524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010275322 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6          |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.765       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.69612403100775\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008761885 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.96       |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.424       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.43787878787879\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -163        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008265362 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.92       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.981       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.13925925925926\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -161        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 188         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010166607 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.87       |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.323       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.84782608695652\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -159        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 191         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009638697 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.8        |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.782       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.61702127659574\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -157       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 246        |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 195        |\n",
      "|    total_timesteps      | 48128      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00908998 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.76      |\n",
      "|    explained_variance   | 0.763      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.01       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.051     |\n",
      "|    value_loss           | 2.56       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.38472222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -154        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 198         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008465413 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.69       |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.577       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.114285714285714\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -152        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 202         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009600423 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.61       |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.763       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.81466666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -149        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009427591 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.53       |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.517       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.56732026143791\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -146        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 208         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009673441 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.48       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.856       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.28076923076924\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -143        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 212         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009684042 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.46       |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.539       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.99748427672957\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -140        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009916797 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.4        |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.624       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.66172839506173\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -137        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009030621 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.37       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.917       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.34181818181818\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -134      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 253       |\n",
      "|    iterations           | 55        |\n",
      "|    time_elapsed         | 222       |\n",
      "|    total_timesteps      | 56320     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0088496 |\n",
      "|    clip_fraction        | 0.172     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -5.32     |\n",
      "|    explained_variance   | 0.714     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.562     |\n",
      "|    n_updates            | 540       |\n",
      "|    policy_gradient_loss | -0.0539   |\n",
      "|    value_loss           | 2.22      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.03214285714286\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -130        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010849104 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.27       |\n",
      "|    explained_variance   | 0.721       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.678       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.70409356725146\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -128        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011258669 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.22       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.578       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.37931034482759\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -125        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 231         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009676741 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.19       |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.741       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.0316384180791\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -121        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 257         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 234         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008897329 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.13       |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.881       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.69333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -118       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 258        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 237        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00946754 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.06      |\n",
      "|    explained_variance   | 0.652      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.07       |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0587    |\n",
      "|    value_loss           | 2.45       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.33661202185792\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -115        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 259         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009246204 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.59        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.93763440860215\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -112        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008326235 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.97       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.862       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.051      |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.5968253968254\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -108        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 247         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010815957 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.92       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.457       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.24583333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -105        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 249         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010182726 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.88       |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.734       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.88820512820513\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -102        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009416277 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.82       |\n",
      "|    explained_variance   | 0.568       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.792       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.52727272727273\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -98.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 264          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 255          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0100050885 |\n",
      "|    clip_fraction        | 0.219        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.77        |\n",
      "|    explained_variance   | 0.624        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.549        |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.0564      |\n",
      "|    value_loss           | 2.15         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.12935323383084\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -95.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 266        |\n",
      "|    iterations           | 67         |\n",
      "|    time_elapsed         | 257        |\n",
      "|    total_timesteps      | 68608      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00964243 |\n",
      "|    clip_fraction        | 0.197      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.72      |\n",
      "|    explained_variance   | 0.619      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.672      |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0526    |\n",
      "|    value_loss           | 2.34       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.72058823529412\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -93          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 267          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 260          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101659205 |\n",
      "|    clip_fraction        | 0.217        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.66        |\n",
      "|    explained_variance   | 0.594        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.803        |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.0561      |\n",
      "|    value_loss           | 2.54         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.30628019323672\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -90         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 268         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 263         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009866304 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.62       |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.709       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.91809523809523\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -86.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 270        |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 265        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00954576 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.58      |\n",
      "|    explained_variance   | 0.591      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.66       |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.0535    |\n",
      "|    value_loss           | 2.5        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.47136150234742\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -84.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 267         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009977046 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.55       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.736       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.04444444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -81.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 270         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009729121 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.5        |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.66        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.59908675799086\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -78.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 273        |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 273        |\n",
      "|    total_timesteps      | 74752      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00915378 |\n",
      "|    clip_fraction        | 0.197      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.45      |\n",
      "|    explained_variance   | 0.61       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.756      |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0537    |\n",
      "|    value_loss           | 2.81       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.13063063063063\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -75.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010511536 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.849       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.67466666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -72.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 279         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010263911 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.38       |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.793       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.22894736842105\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -69.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 275         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 282         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010550692 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.34       |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.974       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.76796536796537\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -65.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 284         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011411654 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.35       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.565       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.27692307692308\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -63.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 287         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010011177 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.31       |\n",
      "|    explained_variance   | 0.472       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.789       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.79071729957806\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -60.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 279         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 289         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010710172 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.27       |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.89        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.285\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -58         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 292         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010525936 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.931       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.75637860082304\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -55.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011307605 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.762       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.2349593495935\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -52.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 297         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011431148 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.22       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.748       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.71164658634538\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -50.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 300         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010425596 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.17       |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.614       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.17301587301587\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -49          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 283          |\n",
      "|    iterations           | 84           |\n",
      "|    time_elapsed         | 303          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108201355 |\n",
      "|    clip_fraction        | 0.24         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.12        |\n",
      "|    explained_variance   | 0.461        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.02         |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.0559      |\n",
      "|    value_loss           | 2.66         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.64470588235294\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -46.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 284         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010184452 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.06       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.836       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.13255813953488\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -44.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 285          |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 308          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098754205 |\n",
      "|    clip_fraction        | 0.225        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.02        |\n",
      "|    explained_variance   | 0.53         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.84         |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.0531      |\n",
      "|    value_loss           | 2.79         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.61992337164752\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -41.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 286        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 311        |\n",
      "|    total_timesteps      | 89088      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01016839 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4         |\n",
      "|    explained_variance   | 0.459      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.12       |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0526    |\n",
      "|    value_loss           | 2.74       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.0878787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -39.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 286         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011278439 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.97       |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 3.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.55205992509363\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -37         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 287         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012485401 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.99       |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.958       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.00888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -35.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 288         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 319         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010710815 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4          |\n",
      "|    explained_variance   | 0.49        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.45128205128205\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -32.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 289         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009986729 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.98       |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.903       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.88478260869566\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -31.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 289         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 324         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009957418 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.99       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.886       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.2910394265233\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -30.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 327         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009594604 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.99       |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.70283687943262\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -28.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 291         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011115525 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4          |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.665       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.11649122807017\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -27.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 292         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 333         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009683834 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.97       |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.673       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.51736111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -26.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 292         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 335         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009768157 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.95       |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.93333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -25.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 293         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 338         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010959537 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.93       |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 3.02        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.33469387755102\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -23.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 294          |\n",
      "|    iterations           | 98           |\n",
      "|    time_elapsed         | 340          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091792215 |\n",
      "|    clip_fraction        | 0.199        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.91        |\n",
      "|    explained_variance   | 0.424        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.97         |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.0497      |\n",
      "|    value_loss           | 2.71         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.72121212121212\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -22.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 295        |\n",
      "|    iterations           | 99         |\n",
      "|    time_elapsed         | 342        |\n",
      "|    total_timesteps      | 101376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01040441 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -3.91      |\n",
      "|    explained_variance   | 0.457      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.887      |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0516    |\n",
      "|    value_loss           | 2.59       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.10733333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -21.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 296         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 345         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010123579 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.87       |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 3.2         |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 25.44\n",
      "Overall Average Successful Assignments: 59.888231272209666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7585f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Testing Summary ----\n",
      "Mean Reward: 38.0\n",
      "Standard Deviation of Reward: 0.0\n",
      "Average Successful Assignments in Testing: 119.0\n"
     ]
    }
   ],
   "source": [
    "# Load new task dataset for testing\n",
    "new_tasks_df = pd.read_csv('RandomTasks200Test.csv')\n",
    "new_tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "test_env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, new_tasks_df), n_envs=1)\n",
    "\n",
    "# Evaluate the model on the new test environment\n",
    "mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=10)\n",
    "# Extract the successful assignments history from the test environment\n",
    "successful_assignments = test_env.envs[0].env.get_average_success()\n",
    "\n",
    "print(\"---- Testing Summary ----\")\n",
    "print(f\"Mean Reward: {mean_reward}\")\n",
    "print(f\"Standard Deviation of Reward: {std_reward}\")\n",
    "print(f\"Average Successful Assignments in Testing: {successful_assignments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32981cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
