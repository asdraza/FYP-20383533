{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee91ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -366.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.666666666666668\n",
      "All assignments history: [15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -370     |\n",
      "| time/              |          |\n",
      "|    fps             | 307      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -324.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.291666666666668\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2048       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00803951 |\n",
      "|    clip_fraction        | 0.0563     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | -0.277     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.33       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0498    |\n",
      "|    value_loss           | 16.1       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -234.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.638888888888886\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010882957 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.204      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0577     |\n",
      "|    value_loss           | 12          |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -362.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.895833333333336\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -374       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 277        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01170852 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | 0.0213     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.35       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0568    |\n",
      "|    value_loss           | 10.5       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -264.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013136433 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0367     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.14        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 9.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -326.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.5\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -375         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 275          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0147967115 |\n",
      "|    clip_fraction        | 0.271        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | -0.004       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.18         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.058       |\n",
      "|    value_loss           | 8.03         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -324.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.79761904761905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015871752 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0156     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 6.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -292.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.958333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 267         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016908953 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.0243     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 5.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -384.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.629629629629626\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 266         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018369475 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.000891   |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.449       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 4.77        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -298.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.55\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018954322 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.000534    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.454       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0629     |\n",
      "|    value_loss           | 4.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -318.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.583333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 262         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021122094 |\n",
      "|    clip_fraction        | 0.458       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.00104    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.484       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 3.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -316.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.6875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 258         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022096882 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00318     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0667     |\n",
      "|    value_loss           | 3.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -344.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.782051282051285\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023892388 |\n",
      "|    clip_fraction        | 0.531       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00219     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00201     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -326.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.61904761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022639293 |\n",
      "|    clip_fraction        | 0.491       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00651     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.794       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 2.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -354.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.71111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028092775 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00633     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.173      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -342.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.151041666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029691786 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0125      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.398       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -308.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.52450980392157\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026608761 |\n",
      "|    clip_fraction        | 0.578       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0392      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -248.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.26851851851852\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029300708 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0809      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.91        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -290.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.89912280701754\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027204648 |\n",
      "|    clip_fraction        | 0.57        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0933     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 3.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -230.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.74166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027136555 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.08        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.6031746031746\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026912697 |\n",
      "|    clip_fraction        | 0.569       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0491      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -230.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.02272727272727\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 232        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 96         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02631457 |\n",
      "|    clip_fraction        | 0.549      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.2        |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.231      |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0643    |\n",
      "|    value_loss           | 2.92       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -168.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.416666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024014432 |\n",
      "|    clip_fraction        | 0.476       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.956       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.729166666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022933863 |\n",
      "|    clip_fraction        | 0.439       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.443333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024758114 |\n",
      "|    clip_fraction        | 0.503       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.222       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0804      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.81730769230769\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 115        |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02366012 |\n",
      "|    clip_fraction        | 0.491      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.245      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.83       |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0649    |\n",
      "|    value_loss           | 3.23       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.97839506172839\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021172667 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.259       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.00892857142857\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -372      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 230       |\n",
      "|    iterations           | 28        |\n",
      "|    time_elapsed         | 124       |\n",
      "|    total_timesteps      | 28672     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0219784 |\n",
      "|    clip_fraction        | 0.424     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.27     |\n",
      "|    explained_variance   | 0.276     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.891     |\n",
      "|    n_updates            | 270       |\n",
      "|    policy_gradient_loss | -0.0603   |\n",
      "|    value_loss           | 2.76      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.29885057471265\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017120868 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.25\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019175842 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.246      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.71774193548387\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019017713 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.158       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.16666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018335382 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0577     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.61868686868686\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018821746 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.45098039215686\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 225         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017593559 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.302       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.55238095238096\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 224         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016560173 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.1         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.43055555555556\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 164        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01728078 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.377      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.165      |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.0578    |\n",
      "|    value_loss           | 2.65       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.03378378378379\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015598707 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.24780701754386\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 222         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016802013 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.419       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 91.72435897435898\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014390401 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 94.07291666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 186        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01435961 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.374      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.147     |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0521    |\n",
      "|    value_loss           | 2.7        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 96.26422764227642\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 219         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 191         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017277597 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.441       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.2936507936508\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 218         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014833454 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.521       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 100.27713178294573\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -371         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 217          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 202          |\n",
      "|    total_timesteps      | 44032        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0149798235 |\n",
      "|    clip_fraction        | 0.256        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.24        |\n",
      "|    explained_variance   | 0.408        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.16         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0538      |\n",
      "|    value_loss           | 2.53         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 102.08712121212122\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 208         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016208958 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0587     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 103.98703703703704\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 214         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013730682 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.99        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 105.8659420289855\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014670025 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0577     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 107.68617021276596\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014654533 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.33506944444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 230         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015172311 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00392     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.72278911564626\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016620602 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.305       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 112.17833333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 243         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013836836 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.35       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.49673202614379\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 249         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014947072 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.442       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.641       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 114.85897435897436\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 254         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017326694 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.431       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.19496855345912\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 260         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015610485 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 117.45987654320987\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 266         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016294181 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.424       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0359     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.69848484848485\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 271        |\n",
      "|    total_timesteps      | 56320      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01595111 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.453      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.169      |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0583    |\n",
      "|    value_loss           | 2.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.97321428571429\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 277         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014581103 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.744       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.21345029239767\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 282         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015764028 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0567     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.44827586206897\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 288         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015876496 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.67372881355932\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 293         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015663723 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.151       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0569     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.83888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 299         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014654893 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.97267759562841\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014644914 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.44        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.3        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.9489247311828\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 310         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014838213 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.466       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.221      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.77116402116403\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017355792 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.269      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0616     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.51171875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015708044 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.285       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.50897435897437\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 327         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015988307 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.338       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.60858585858585\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -367       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 203        |\n",
      "|    iterations           | 66         |\n",
      "|    time_elapsed         | 332        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01632578 |\n",
      "|    clip_fraction        | 0.29       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.493      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.08       |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | -0.0613    |\n",
      "|    value_loss           | 2.45       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.8582089552239\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 338         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015060123 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.9485294117647\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014891112 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.63        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.92391304347825\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 349         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013883956 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.162      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0579     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.02380952380952\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 355         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015462099 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.268       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.0950704225352\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 360         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015516257 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.84        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0624     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.11574074074073\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 365         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015574908 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.593       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.08904109589042\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 370         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015699366 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.501       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.45        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 139.05405405405406\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 376         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016125081 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0684     |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.03666666666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -365       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 201        |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 381        |\n",
      "|    total_timesteps      | 76800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01525311 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.15      |\n",
      "|    explained_variance   | 0.512      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.08       |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0616    |\n",
      "|    value_loss           | 2.43       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.98684210526315\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 387         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017914195 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.519       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.257       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0616     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 141.88203463203465\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 392         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016081044 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.599       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0653     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 142.72649572649573\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014405412 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.514       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.865       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0624     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.55590717299577\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 403         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014923265 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.286       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 144.36979166666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 408         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015476305 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.114      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.19444444444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 414         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016333364 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0288      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0706     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.9959349593496\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 419         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017059248 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.787       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.76807228915663\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 424         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017372154 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.514       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.836       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 147.47916666666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -359       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 199        |\n",
      "|    iterations           | 84         |\n",
      "|    time_elapsed         | 430        |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01758615 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.05      |\n",
      "|    explained_variance   | 0.489      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.149      |\n",
      "|    n_updates            | 830        |\n",
      "|    policy_gradient_loss | -0.0697    |\n",
      "|    value_loss           | 2.45       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 148.22254901960784\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 436         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016511282 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.04       |\n",
      "|    explained_variance   | 0.532       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.666       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 148.9515503875969\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 441         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017462537 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.03       |\n",
      "|    explained_variance   | 0.504       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.266       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0699     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 149.65325670498083\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -357        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 446         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018258909 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.82        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 96.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 150.36837121212122\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 452         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017171074 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.01       |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.268       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.07       |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 151.04494382022472\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -355        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 457         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019928422 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.98       |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0754     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 151.72222222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -354        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 462         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018000253 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.96       |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.42        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 152.38736263736263\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -354        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 468         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017289402 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.869       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 153.03079710144928\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -352        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 474         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017875409 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.91       |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 153.747311827957\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -351        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 479         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016632652 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.87       |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.558       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 154.38386524822695\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -350        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 485         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015588873 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.83       |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.681       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 155.02719298245614\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -348        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 490         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018281717 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.8        |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.453       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0755     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 155.73177083333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -347        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 496         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018401725 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.76       |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0501      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0743     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 156.446735395189\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -345        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 502         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017945008 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.72       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0768     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 157.11479591836735\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -344        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 507         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017511066 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.68       |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.22        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 157.7895622895623\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -342        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 513         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018393483 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.65       |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.576       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0784     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 158.43166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -341        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 519         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016941033 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.61       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.665       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0744     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -28.66\n",
      "Overall Average Successful Assignments: 100.58114759866709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Testing Summary ----\n",
      "Mean Reward: 86.0\n",
      "Standard Deviation of Reward: 0.0\n",
      "Average Successful Assignments in Testing: 243.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n",
    "\n",
    "# Load new task dataset for testing\n",
    "new_tasks_df = pd.read_csv('RandomTasks400Test.csv')\n",
    "new_tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "test_env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, new_tasks_df), n_envs=1)\n",
    "\n",
    "# Evaluate the model on the new test environment\n",
    "mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=10)\n",
    "# Extract the successful assignments history from the test environment\n",
    "successful_assignments = test_env.envs[0].env.get_average_success()\n",
    "\n",
    "print(\"---- Testing Summary ----\")\n",
    "print(f\"Mean Reward: {mean_reward}\")\n",
    "print(f\"Standard Deviation of Reward: {std_reward}\")\n",
    "print(f\"Average Successful Assignments in Testing: {successful_assignments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869a805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8266b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
