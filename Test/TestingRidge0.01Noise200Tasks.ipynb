{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc8fa0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.09766677979988178\n",
      "RMSE: 0.12209370048784808\n",
      "R-squared: 0.9998991762339906\n",
      "RAE: 0.010200341696916998\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(columns=['Eligible'])\n",
    "    y = data['Eligible']\n",
    "    return X, y\n",
    "\n",
    "def train_ridge_model(X_train, y_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    ridge_model = Ridge()\n",
    "    ridge_model.fit(X_train_scaled, y_train)\n",
    "    return ridge_model, scaler\n",
    "\n",
    "# Load and train on VehicleTrainingDataset.csv\n",
    "X_train, y_train = load_and_preprocess_data('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "ridge_model, scaler = train_ridge_model(X_train, y_train)\n",
    "\n",
    "# Predict eligibility scores on 1000VehicleDataset.csv\n",
    "vehicles_df = pd.read_csv('1000VehicleDataset_Noisy_0.01.csv')\n",
    "X_test = vehicles_df.drop(columns=['Eligible'])\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "predicted_scores = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Assuming you have access to actual scores, replace this line with the actual score loading logic if available\n",
    "y_actual = vehicles_df['Eligible']  # This would be prior to overwriting with predictions if you run this block again\n",
    "\n",
    "# Replace actual scores with predicted ones\n",
    "vehicles_df['Eligible'] = predicted_scores  \n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_actual, predicted_scores)\n",
    "rmse = np.sqrt(mean_squared_error(y_actual, predicted_scores))\n",
    "r_squared = r2_score(y_actual, predicted_scores)\n",
    "rae = np.sum(np.abs(y_actual - predicted_scores)) / np.sum(np.abs(y_actual - np.mean(y_actual)))\n",
    "\n",
    "# Output the results\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"RAE: {rae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78798e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 6.8\n",
      "All assignments history: [7, 8, 3, 8, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -187     |\n",
      "| time/              |          |\n",
      "|    fps             | 249      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.566666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007642906 |\n",
      "|    clip_fraction        | 0.0629      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | -0.0491     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.19        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    value_loss           | 16          |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.955555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008704297 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | -0.048      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0481     |\n",
      "|    value_loss           | 11.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.583333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008563455 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.0711      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    value_loss           | 9.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -168.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.88\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 198          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 5120         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078035565 |\n",
      "|    clip_fraction        | 0.0957       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.48        |\n",
      "|    explained_variance   | 0.0858       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.43         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0405      |\n",
      "|    value_loss           | 9.4          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.433333333333334\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -186      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 207       |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 29        |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0090902 |\n",
      "|    clip_fraction        | 0.14      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -6.48     |\n",
      "|    explained_variance   | 0.0701    |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.93      |\n",
      "|    n_updates            | 50        |\n",
      "|    policy_gradient_loss | -0.0456   |\n",
      "|    value_loss           | 7.92      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.114285714285714\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009757284 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.0874      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.34        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    value_loss           | 6.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.258333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010378183 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.09        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.95        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    value_loss           | 6.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.16296296296296\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010938115 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    value_loss           | 5.48        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.573333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011085967 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.73        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 5.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.35757575757576\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012917822 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.75        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 4.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.444444444444443\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016614426 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.95        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0577     |\n",
      "|    value_loss           | 4.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.61538461538461\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014040963 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.2         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 4.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.38095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014012465 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.7         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 4.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.08888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012907783 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.051      |\n",
      "|    value_loss           | 4.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.520833333333336\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -184         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 193          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0124969445 |\n",
      "|    clip_fraction        | 0.227        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.43        |\n",
      "|    explained_variance   | 0.405        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.47         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0493      |\n",
      "|    value_loss           | 4.3          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.015686274509804\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009867545 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.04        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 4.71        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.13703703703704\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 190        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 96         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00813324 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.43      |\n",
      "|    explained_variance   | 0.463      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.29       |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0415    |\n",
      "|    value_loss           | 4.27       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.473684210526315\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008558793 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    value_loss           | 4.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.64666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009792629 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.523       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 4.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.78095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008972492 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.546       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.14        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 4.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.85151515151515\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -183         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072563053 |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.41        |\n",
      "|    explained_variance   | 0.576        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.13         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0399      |\n",
      "|    value_loss           | 4            |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.05797101449275\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 180         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009445391 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.48        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 4.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.31111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 178         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008453394 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.579       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.34        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 4.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.504\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 178        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 143        |\n",
      "|    total_timesteps      | 25600      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00872428 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.39      |\n",
      "|    explained_variance   | 0.594      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.83       |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 4.02       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.47179487179487\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 177         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008605143 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.602       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    value_loss           | 4.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.58518518518518\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 177         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 155         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007929813 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 3.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.80714285714286\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 177         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008091218 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.36       |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.821       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    value_loss           | 3.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.94252873563219\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 176         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008252477 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.77        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 4.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.95111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 175         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008891743 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.34       |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.783       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0482     |\n",
      "|    value_loss           | 3.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.96774193548387\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 174         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008303172 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.33       |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 3.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.85208333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 187        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00836963 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.31      |\n",
      "|    explained_variance   | 0.664      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.58       |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0469    |\n",
      "|    value_loss           | 3.78       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.7030303030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008250815 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.3        |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    value_loss           | 3.49        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.59607843137255\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 172         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 201         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009652317 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.3        |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 3.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.43238095238095\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 208        |\n",
      "|    total_timesteps      | 35840      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00876081 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.28      |\n",
      "|    explained_variance   | 0.667      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.54       |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0492    |\n",
      "|    value_loss           | 3.66       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.22222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007891803 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.26       |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.77        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    value_loss           | 3.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.04684684684685\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 222         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008305902 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.24       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    value_loss           | 3.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.83157894736842\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -173         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 170          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 228          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097461045 |\n",
      "|    clip_fraction        | 0.168        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.22        |\n",
      "|    explained_variance   | 0.691        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.11         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0497      |\n",
      "|    value_loss           | 3.82         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.495726495726494\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008900486 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.21       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    value_loss           | 3.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.17666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010247204 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.18       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.771       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 3.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.86991869918699\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 246         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009269516 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.15       |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.885       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 3.63        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.56031746031746\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 253         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008321722 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.12       |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0505     |\n",
      "|    value_loss           | 3.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.2108527131783\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -166         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 169          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 259          |\n",
      "|    total_timesteps      | 44032        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095544085 |\n",
      "|    clip_fraction        | 0.181        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.09        |\n",
      "|    explained_variance   | 0.727        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.4          |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0535      |\n",
      "|    value_loss           | 3.11         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.91363636363636\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -165       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 266        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01010809 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.06      |\n",
      "|    explained_variance   | 0.73       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.07       |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.0519    |\n",
      "|    value_loss           | 3.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.59851851851852\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 274         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009131509 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.03       |\n",
      "|    explained_variance   | 0.71        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 3.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.23478260869565\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -162        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009290269 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.99       |\n",
      "|    explained_variance   | 0.721       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.8468085106383\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 288         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008579175 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.96       |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.73        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 3.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.44305555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -159        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 296         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008848356 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.93       |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 3.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.08707482993198\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -157        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 303         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010233322 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.9        |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 3.22        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.67333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -155        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 313         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008880468 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.85       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.2326797385621\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -153        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 320         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009691605 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.81       |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.619       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.82179487179488\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -150        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 329         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008598426 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.77       |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.694       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.3622641509434\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -148        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 336         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008718973 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.71       |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 3.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.95185185185186\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -145        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 345         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009952567 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.66       |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.624       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.51878787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -143        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 355         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009895793 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.62       |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.61        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.08452380952382\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -140        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010722198 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.54       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.982       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.63859649122807\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -138        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 370         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009529749 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.48       |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.16206896551724\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -135        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 377         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008993264 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.4        |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.68926553672317\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -132       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 384        |\n",
      "|    total_timesteps      | 60416      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01021743 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.36      |\n",
      "|    explained_variance   | 0.706      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.69       |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0545    |\n",
      "|    value_loss           | 2.84       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.19888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -130        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 392         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009460514 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.31       |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.775       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.68524590163935\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -127        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009151659 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.28       |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.849       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.16881720430108\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -125        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 404         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010218445 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.2        |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.6920634920635\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -121        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 410         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010195168 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.14       |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.722       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.18229166666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -118        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 415         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009093616 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.11       |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.908       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0505     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.67897435897436\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -115        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 421         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009838569 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.04       |\n",
      "|    explained_variance   | 0.71        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.14444444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -112        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 427         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008991628 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5          |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.64477611940299\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -108        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 432         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009396248 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.95       |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.924       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.12647058823529\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -105        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 438         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009673171 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.92       |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.704       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.60193236714976\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -102        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009023275 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.86       |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.932       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.06761904761905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -99.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 450         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009683501 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.79       |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.52488262910798\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -96.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 456         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009473467 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.75       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.9962962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -93.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 461         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010130088 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.71       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.995       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.44018264840183\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -90.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 468         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009150347 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.69       |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.722       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.88738738738739\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -87.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 473         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010402583 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.65       |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.868       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.30488888888888\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -84.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 479         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009794946 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.61       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.73684210526316\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -82         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 485         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009935563 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.56       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.648       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.15930735930736\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -79.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 77         |\n",
      "|    time_elapsed         | 491        |\n",
      "|    total_timesteps      | 78848      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01008001 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.54      |\n",
      "|    explained_variance   | 0.739      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.02       |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0526    |\n",
      "|    value_loss           | 2.42       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.56581196581196\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -77         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 498         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008679768 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.51       |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.921       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.98481012658227\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -74.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009243981 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.48       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.39166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -72         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 510         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008718977 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.47       |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.78024691358024\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -69.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 515         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009318902 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.44       |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.9         |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.16829268292683\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -67.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 521         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009526225 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.42       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.773       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.53815261044177\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -66         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 527         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009699364 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.92222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -64.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 535         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010483341 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.878       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.30039215686274\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -62.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 541         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008578647 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.38       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.67596899224806\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -59.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 548         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009707576 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.36       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.927       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.0360153256705\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -58.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 555         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009198273 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.34       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.706       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.37424242424242\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -56.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 561         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010331588 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.35       |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.759       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.71086142322098\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -55.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 568         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010163752 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.35       |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.06666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -53.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 574         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008773553 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.32       |\n",
      "|    explained_variance   | 0.769       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.89        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.40512820512821\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -52.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 581          |\n",
      "|    total_timesteps      | 93184        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091463365 |\n",
      "|    clip_fraction        | 0.188        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.32        |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.666        |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0505      |\n",
      "|    value_loss           | 2.52         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.7195652173913\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -51.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 587         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009081883 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0486     |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.03870967741935\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -50.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 594         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009425866 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.27       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.28        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.34042553191489\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -49.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 600         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010128276 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.63157894736842\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -48         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 607         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009235151 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.28       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.752       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.90069444444444\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -47.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 96         |\n",
      "|    time_elapsed         | 613        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00940039 |\n",
      "|    clip_fraction        | 0.197      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.27      |\n",
      "|    explained_variance   | 0.805      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.22       |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.0509    |\n",
      "|    value_loss           | 2.77       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.16426116838488\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -47         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 619         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010555448 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.25       |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.843       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.42517006802721\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -46.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 625         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009856146 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.6969696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -45.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 632         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009098783 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.23       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1           |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.95466666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -45.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 638        |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00910859 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.21      |\n",
      "|    explained_variance   | 0.822      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.03       |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.0472    |\n",
      "|    value_loss           | 2.72       |\n",
      "----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 33.0\n",
      "Overall Average Successful Assignments: 63.91530589478316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Testing Summary ----\n",
      "Mean Reward: 58.0\n",
      "Standard Deviation of Reward: 0.0\n",
      "Average Successful Assignments in Testing: 129.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n",
    "\n",
    "\n",
    "# Load new task dataset for testing\n",
    "new_tasks_df = pd.read_csv('RandomTasks200Test.csv')\n",
    "new_tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "test_env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, new_tasks_df), n_envs=1)\n",
    "\n",
    "# Evaluate the model on the new test environment\n",
    "mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=10)\n",
    "# Extract the successful assignments history from the test environment\n",
    "successful_assignments = test_env.envs[0].env.get_average_success()\n",
    "\n",
    "print(\"---- Testing Summary ----\")\n",
    "print(f\"Mean Reward: {mean_reward}\")\n",
    "print(f\"Standard Deviation of Reward: {std_reward}\")\n",
    "print(f\"Average Successful Assignments in Testing: {successful_assignments}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ced09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
