{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a7979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.09766677979988178\n",
      "RMSE: 0.12209370048784808\n",
      "R-squared: 0.9998991762339906\n",
      "RAE: 0.010200341696916998\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(columns=['Eligible'])\n",
    "    y = data['Eligible']\n",
    "    return X, y\n",
    "\n",
    "def train_ridge_model(X_train, y_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    ridge_model = Ridge()\n",
    "    ridge_model.fit(X_train_scaled, y_train)\n",
    "    return ridge_model, scaler\n",
    "\n",
    "# Load and train on VehicleTrainingDataset.csv\n",
    "X_train, y_train = load_and_preprocess_data('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "ridge_model, scaler = train_ridge_model(X_train, y_train)\n",
    "\n",
    "# Predict eligibility scores on 1000VehicleDataset.csv\n",
    "vehicles_df = pd.read_csv('1000VehicleDataset_Noisy_0.01.csv')\n",
    "X_test = vehicles_df.drop(columns=['Eligible'])\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "predicted_scores = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Assuming you have access to actual scores, replace this line with the actual score loading logic if available\n",
    "y_actual = vehicles_df['Eligible']  # This would be prior to overwriting with predictions if you run this block again\n",
    "\n",
    "# Replace actual scores with predicted ones\n",
    "vehicles_df['Eligible'] = predicted_scores  \n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_actual, predicted_scores)\n",
    "rmse = np.sqrt(mean_squared_error(y_actual, predicted_scores))\n",
    "r_squared = r2_score(y_actual, predicted_scores)\n",
    "rae = np.sum(np.abs(y_actual - predicted_scores)) / np.sum(np.abs(y_actual - np.mean(y_actual)))\n",
    "\n",
    "# Output the results\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"RAE: {rae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c64ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -382.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 9.666666666666666\n",
      "All assignments history: [14, 12, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -374     |\n",
      "| time/              |          |\n",
      "|    fps             | 292      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -328.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.916666666666668\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -374         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 282          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066980603 |\n",
      "|    clip_fraction        | 0.0584       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.84        |\n",
      "|    explained_variance   | -0.191       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.65         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0373      |\n",
      "|    value_loss           | 17.8         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -224.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.083333333333336\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -374         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 281          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073305797 |\n",
      "|    clip_fraction        | 0.0922       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.84        |\n",
      "|    explained_variance   | -0.0214      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.75         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0379      |\n",
      "|    value_loss           | 12.4         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -278.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.458333333333336\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -376         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 279          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068257516 |\n",
      "|    clip_fraction        | 0.0775       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.83        |\n",
      "|    explained_variance   | -0.0344      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.01         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0364      |\n",
      "|    value_loss           | 10.4         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -338.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.71666666666667\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -373         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 278          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 5120         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069599105 |\n",
      "|    clip_fraction        | 0.0834       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.83        |\n",
      "|    explained_variance   | -0.0488      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.54         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0346      |\n",
      "|    value_loss           | 9.68         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -352.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.93055555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007960781 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.82       |\n",
      "|    explained_variance   | -0.0295     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.2         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0397     |\n",
      "|    value_loss           | 7.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -286.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.88095238095238\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00828037 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.81      |\n",
      "|    explained_variance   | -0.00328   |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.41       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0395    |\n",
      "|    value_loss           | 6.64       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -266.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.416666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 275         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008244543 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.81       |\n",
      "|    explained_variance   | -0.00254    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0369     |\n",
      "|    value_loss           | 5.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -250.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.129629629629626\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008596898 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.8        |\n",
      "|    explained_variance   | 0.00382     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.642       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    value_loss           | 4.57        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.333333333333336\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -367       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 271        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 37         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00949824 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.8       |\n",
      "|    explained_variance   | 0.0118     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.85       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.042     |\n",
      "|    value_loss           | 4.02       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.78787878787879\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010395857 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.79       |\n",
      "|    explained_variance   | 0.00865     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.388       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    value_loss           | 3.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.11111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011340096 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.78       |\n",
      "|    explained_variance   | 0.00324     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.92        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 3.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.1025641025641\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012974315 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.77       |\n",
      "|    explained_variance   | 0.0117      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0624     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 3.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.33928571428571\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 270         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012194339 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.76       |\n",
      "|    explained_variance   | -0.00261    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.36111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 270         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013775828 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.75       |\n",
      "|    explained_variance   | 0.00932     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.473       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.3125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 270         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010244111 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.75       |\n",
      "|    explained_variance   | 0.0479      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.529       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 102.97549019607843\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010218525 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.74       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.04166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009482549 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.72       |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.716       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.30701754385964\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009017378 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.71       |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.551       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.3375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010310914 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.68       |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.645       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.08333333333333\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -358      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 272       |\n",
      "|    iterations           | 21        |\n",
      "|    time_elapsed         | 78        |\n",
      "|    total_timesteps      | 21504     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0082011 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -5.66     |\n",
      "|    explained_variance   | 0.239     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.693     |\n",
      "|    n_updates            | 200       |\n",
      "|    policy_gradient_loss | -0.0427   |\n",
      "|    value_loss           | 2.85      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.8409090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -357        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007561461 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.63       |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.505       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.7355072463768\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -355        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 273         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007329423 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.6        |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.45        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.52430555555554\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -354        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 273         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007558624 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.56       |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    value_loss           | 3.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.53333333333333\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -352         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 274          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 25600        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075772638 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.53        |\n",
      "|    explained_variance   | 0.3          |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 3.3          |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0399      |\n",
      "|    value_loss           | 2.94         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.36858974358975\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -351        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008936953 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.49       |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.783       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0411     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.01543209876544\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -348        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008893389 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.44       |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.78        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    value_loss           | 3.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.24107142857142\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -346        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009358095 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.39       |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 147.6005747126437\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -344        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008975208 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.578       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 149.73611111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -342        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007651109 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.3        |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.88        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 151.76075268817203\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -340        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008892441 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.26       |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 153.58072916666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -338        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008371705 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.18       |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0456     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 155.44444444444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -335        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008430756 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.11       |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.704       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 157.3578431372549\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -333         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 274          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075104292 |\n",
      "|    clip_fraction        | 0.149        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.04        |\n",
      "|    explained_variance   | 0.323        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.58         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.0435      |\n",
      "|    value_loss           | 3.22         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 159.33333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -330        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008306196 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.97       |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.7         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 161.09027777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -328        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008684683 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.297       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 162.78603603603602\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -325        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008475275 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.85       |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0477     |\n",
      "|    value_loss           | 3.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 164.5482456140351\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -322         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 274          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 141          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081583895 |\n",
      "|    clip_fraction        | 0.178        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.79        |\n",
      "|    explained_variance   | 0.285        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.39         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0487      |\n",
      "|    value_loss           | 3.38         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 166.35256410256412\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -319      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 274       |\n",
      "|    iterations           | 39        |\n",
      "|    time_elapsed         | 145       |\n",
      "|    total_timesteps      | 39936     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0092698 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -4.7      |\n",
      "|    explained_variance   | 0.314     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.28      |\n",
      "|    n_updates            | 380       |\n",
      "|    policy_gradient_loss | -0.0509   |\n",
      "|    value_loss           | 3.47      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 168.05\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -316        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007854958 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.63       |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 169.71951219512195\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -313       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 274        |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 153        |\n",
      "|    total_timesteps      | 41984      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00770837 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.57      |\n",
      "|    explained_variance   | 0.408      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.753      |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    value_loss           | 3.26       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 171.35119047619048\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -310        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007861137 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.5        |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 172.8953488372093\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -306        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007819926 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.46       |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 174.36742424242425\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -303        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007419966 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 175.75185185185185\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -299         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 274          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 167          |\n",
      "|    total_timesteps      | 46080        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075279037 |\n",
      "|    clip_fraction        | 0.136        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.34        |\n",
      "|    explained_variance   | 0.37         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.74         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0456      |\n",
      "|    value_loss           | 2.93         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 177.17753623188406\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -296         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 275          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 171          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074914284 |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.26        |\n",
      "|    explained_variance   | 0.399        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.61         |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.0469      |\n",
      "|    value_loss           | 2.93         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 178.56737588652481\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -293        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 275         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008250333 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.21       |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.918       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 179.88194444444446\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -290         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 275          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 178          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077059017 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.17        |\n",
      "|    explained_variance   | 0.426        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.984        |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.0412      |\n",
      "|    value_loss           | 2.87         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 181.2295918367347\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -286         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 275          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 50176        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083662635 |\n",
      "|    clip_fraction        | 0.139        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.1         |\n",
      "|    explained_variance   | 0.471        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.931        |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0447      |\n",
      "|    value_loss           | 2.87         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 182.55833333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -283        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 275         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006166068 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.01       |\n",
      "|    explained_variance   | 0.442       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    value_loss           | 3.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 136.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 183.84640522875816\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -277         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 275          |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 189          |\n",
      "|    total_timesteps      | 52224        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070713228 |\n",
      "|    clip_fraction        | 0.146        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.91        |\n",
      "|    explained_variance   | 0.382        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.53         |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0426      |\n",
      "|    value_loss           | 3.43         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 185.1522435897436\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -272        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008122338 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.9        |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 186.45911949685535\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -266        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008022053 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.79       |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.927       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0458     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 187.73302469135803\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -260        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 200         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008640386 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.74       |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.793       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 189.02424242424243\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -254        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007459161 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.7        |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.956       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0409     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 190.29761904761904\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -248         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 276          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 207          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076720817 |\n",
      "|    clip_fraction        | 0.146        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.62        |\n",
      "|    explained_variance   | 0.546        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.33         |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.044       |\n",
      "|    value_loss           | 3.36         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 191.51461988304092\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -242         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 276          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 210          |\n",
      "|    total_timesteps      | 58368        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074372273 |\n",
      "|    clip_fraction        | 0.134        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.54        |\n",
      "|    explained_variance   | 0.501        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.2          |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0423      |\n",
      "|    value_loss           | 3.37         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 162.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 192.73994252873564\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -236         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 277          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 214          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063396185 |\n",
      "|    clip_fraction        | 0.131        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.5         |\n",
      "|    explained_variance   | 0.414        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.33         |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.0393      |\n",
      "|    value_loss           | 3.42         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 193.90395480225988\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -230         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 277          |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 217          |\n",
      "|    total_timesteps      | 60416        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083723245 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.45        |\n",
      "|    explained_variance   | 0.438        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.915        |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0453      |\n",
      "|    value_loss           | 2.64         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 162.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 195.04305555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -225        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 221         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008389051 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.45       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 166.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 196.18032786885246\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -218        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006986065 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.42       |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 197.34139784946237\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -212         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 277          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 228          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072972407 |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.42        |\n",
      "|    explained_variance   | 0.302        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.31         |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.0394      |\n",
      "|    value_loss           | 3.2          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 198.47354497354496\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -205         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 278          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 232          |\n",
      "|    total_timesteps      | 64512        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068393867 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.4         |\n",
      "|    explained_variance   | 0.356        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.08         |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0403      |\n",
      "|    value_loss           | 2.79         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 168.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 199.56119791666666\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -199         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 278          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 235          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068358975 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.35        |\n",
      "|    explained_variance   | 0.4          |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.37         |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.0389      |\n",
      "|    value_loss           | 3.07         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 200.60897435897436\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -192         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 278          |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 238          |\n",
      "|    total_timesteps      | 66560        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073574823 |\n",
      "|    clip_fraction        | 0.146        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.32        |\n",
      "|    explained_variance   | 0.457        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.09         |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0395      |\n",
      "|    value_loss           | 2.66         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 201.6338383838384\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006533955 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.3        |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.619       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0391     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 202.70646766169153\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 279         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 245         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007584409 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.28       |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0395     |\n",
      "|    value_loss           | 3.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 203.74632352941177\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -173         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 279          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 249          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071650064 |\n",
      "|    clip_fraction        | 0.142        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.25        |\n",
      "|    explained_variance   | 0.343        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.937        |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.0413      |\n",
      "|    value_loss           | 2.77         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 204.76932367149757\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -167         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 279          |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 252          |\n",
      "|    total_timesteps      | 70656        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066230656 |\n",
      "|    clip_fraction        | 0.141        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.21        |\n",
      "|    explained_variance   | 0.198        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.24         |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0411      |\n",
      "|    value_loss           | 2.78         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 205.74642857142857\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -160       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 255        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00836363 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -3.17      |\n",
      "|    explained_variance   | 0.297      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.737      |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.0401    |\n",
      "|    value_loss           | 2.63       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 206.69248826291079\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -153        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007273462 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.13       |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0411     |\n",
      "|    value_loss           | 3.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 207.61342592592592\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -147       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 262        |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00725782 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -3.08      |\n",
      "|    explained_variance   | 0.334      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.43       |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.0408    |\n",
      "|    value_loss           | 3.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 208.513698630137\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -140        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 266         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006808665 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.02       |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 209.37612612612614\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -134        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007929503 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3          |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0391     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 210.23222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -128        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 273         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006566962 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3          |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 211.05811403508773\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -122         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 281          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 276          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062335143 |\n",
      "|    clip_fraction        | 0.113        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.96        |\n",
      "|    explained_variance   | 0.294        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.31         |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.0377      |\n",
      "|    value_loss           | 3.12         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 211.8961038961039\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -116         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 281          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 280          |\n",
      "|    total_timesteps      | 78848        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056656906 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.94        |\n",
      "|    explained_variance   | 0.329        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.7          |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0332      |\n",
      "|    value_loss           | 3.49         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 212.69230769230768\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -110        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 283         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006069711 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | 0.334       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0348     |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 213.48523206751054\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -104         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 281          |\n",
      "|    iterations           | 79           |\n",
      "|    time_elapsed         | 287          |\n",
      "|    total_timesteps      | 80896        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065662106 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.96        |\n",
      "|    explained_variance   | 0.353        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.23         |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.0341      |\n",
      "|    value_loss           | 3.1          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 214.25\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -98.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 281          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 290          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055490425 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.4          |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.01         |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.0352      |\n",
      "|    value_loss           | 2.97         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 215.00205761316872\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -92.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 281          |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 294          |\n",
      "|    total_timesteps      | 82944        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058063446 |\n",
      "|    clip_fraction        | 0.0977       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | 0.421        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.41         |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0334      |\n",
      "|    value_loss           | 3.36         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 215.72256097560975\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -87.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 297         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005849838 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 216.48694779116465\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -81.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 282          |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 301          |\n",
      "|    total_timesteps      | 84992        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058016414 |\n",
      "|    clip_fraction        | 0.12         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.52         |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0365      |\n",
      "|    value_loss           | 3.71         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 217.2093253968254\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -75.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005109457 |\n",
      "|    clip_fraction        | 0.0907      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 3.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 217.9078431372549\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -70.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 308         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005423911 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 3.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 218.5746124031008\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -64.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 311         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004867961 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.83        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    value_loss           | 3.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 219.27586206896552\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -59.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 315         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005173794 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 3.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 219.90530303030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -54.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 318         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004555722 |\n",
      "|    clip_fraction        | 0.0772      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    value_loss           | 3.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 220.56273408239701\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -50         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004960942 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    value_loss           | 3.23        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 184.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 221.20185185185184\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -45.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 282          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 325          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067237834 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.443        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.12         |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    value_loss           | 3.32         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 221.8562271062271\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -41.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 282          |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 329          |\n",
      "|    total_timesteps      | 93184        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059109936 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | 0.481        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.43         |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0343      |\n",
      "|    value_loss           | 3.24         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 222.4746376811594\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -37.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 282          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 332          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048716427 |\n",
      "|    clip_fraction        | 0.1          |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.82         |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    value_loss           | 3.71         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 223.09767025089604\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -33.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 336         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006433025 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    value_loss           | 3.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 223.7154255319149\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -30.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 283          |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 340          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063446807 |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.491        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.21         |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.0328      |\n",
      "|    value_loss           | 3.07         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 224.32280701754385\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -26.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005560897 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.993       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 3.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 224.91232638888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -22.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 347         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005825658 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.522       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.45        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0331     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 225.4905498281787\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -18.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004978801 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.516       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    value_loss           | 3.17        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 226.04166666666666\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -15.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 283          |\n",
      "|    iterations           | 98           |\n",
      "|    time_elapsed         | 354          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063877166 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.503        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.24         |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.0344      |\n",
      "|    value_loss           | 3.34         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 226.58670033670035\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -12.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 283          |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 357          |\n",
      "|    total_timesteps      | 101376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065066773 |\n",
      "|    clip_fraction        | 0.136        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.566        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.34         |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0356      |\n",
      "|    value_loss           | 2.99         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 227.12666666666667\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -9.46        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 283          |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 361          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063388357 |\n",
      "|    clip_fraction        | 0.129        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | 0.566        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.24         |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.0336      |\n",
      "|    value_loss           | 3.05         |\n",
      "------------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 94.32\n",
      "Overall Average Successful Assignments: 164.57457979473907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Testing Summary ----\n",
      "Mean Reward: 174.0\n",
      "Standard Deviation of Reward: 0.0\n",
      "Average Successful Assignments in Testing: 287.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n",
    "\n",
    "\n",
    "# Load new task dataset for testing\n",
    "new_tasks_df = pd.read_csv('RandomTasks400Test.csv')\n",
    "new_tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "test_env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, new_tasks_df), n_envs=1)\n",
    "\n",
    "# Evaluate the model on the new test environment\n",
    "mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=10)\n",
    "# Extract the successful assignments history from the test environment\n",
    "successful_assignments = test_env.envs[0].env.get_average_success()\n",
    "\n",
    "print(\"---- Testing Summary ----\")\n",
    "print(f\"Mean Reward: {mean_reward}\")\n",
    "print(f\"Standard Deviation of Reward: {std_reward}\")\n",
    "print(f\"Average Successful Assignments in Testing: {successful_assignments}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b1138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
