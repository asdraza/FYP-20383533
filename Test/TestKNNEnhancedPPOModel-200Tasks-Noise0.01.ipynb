{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f973fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best parameters: {'metric': 'manhattan', 'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "MAE: 1.7258836094021666\n",
      "RMSE: 2.159748576425794\n",
      "R-squared: 0.9684512036561819\n",
      "RAE: 0.18025169439477964\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(columns=['Eligible'])\n",
    "    y = data['Eligible']\n",
    "    return X, y\n",
    "\n",
    "def optimize_knn_model(X_train, y_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    knn_model = KNeighborsRegressor()\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 10, 20, 30, 40],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan'],\n",
    "        'p': [1, 2]  # 1: Manhattan distance, 2: Euclidean distance\n",
    "    }\n",
    "    grid_search = GridSearchCV(knn_model, param_grid, cv=5, verbose=1, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    best_knn = grid_search.best_estimator_\n",
    "    return best_knn, scaler\n",
    "\n",
    "# Load and train on VehicleTrainingDataset.csv\n",
    "X_train, y_train = load_and_preprocess_data('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "knn_model, scaler = optimize_knn_model(X_train, y_train)\n",
    "\n",
    "# Predict eligibility scores on 1000VehicleDataset.csv\n",
    "vehicles_df = pd.read_csv('1000VehicleDataset_Noisy_0.01.csv')\n",
    "X_test = vehicles_df.drop(columns=['Eligible'])\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "predicted_scores = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# Assuming you have access to actual scores, replace this line with the actual score loading logic if available\n",
    "y_actual = vehicles_df['Eligible']\n",
    "\n",
    "# Replace actual scores with predicted ones\n",
    "vehicles_df['Eligible'] = predicted_scores  \n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_actual, predicted_scores)\n",
    "rmse = np.sqrt(mean_squared_error(y_actual, predicted_scores))\n",
    "r_squared = r2_score(y_actual, predicted_scores)\n",
    "rae = np.sum(np.abs(y_actual - predicted_scores)) / np.sum(np.abs(y_actual - np.mean(y_actual)))\n",
    "\n",
    "# Output the results\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"RAE: {rae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1aa4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -194.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 3.3333333333333335\n",
      "All assignments history: [2, 7, 6, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -192     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 7.266666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008548127 |\n",
      "|    clip_fraction        | 0.0742      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.57       |\n",
      "|    explained_variance   | -0.249      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.62        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    value_loss           | 17.6        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 8.4\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -187         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093712285 |\n",
      "|    clip_fraction        | 0.138        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.57        |\n",
      "|    explained_variance   | -0.0583      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.77         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0468      |\n",
      "|    value_loss           | 11.8         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 9.2\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009679996 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.56       |\n",
      "|    explained_variance   | 0.0135      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.04        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    value_loss           | 9.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.093333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008944577 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.56       |\n",
      "|    explained_variance   | -0.00399    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.99        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    value_loss           | 9.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.722222222222221\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 138          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087847905 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.55        |\n",
      "|    explained_variance   | 0.0191       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.25         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0404      |\n",
      "|    value_loss           | 8.41         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.980952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010641255 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.55       |\n",
      "|    explained_variance   | 0.0181      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.38        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.045      |\n",
      "|    value_loss           | 7.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.25\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 137          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102208285 |\n",
      "|    clip_fraction        | 0.168        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.54        |\n",
      "|    explained_variance   | 0.0203       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.45         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0423      |\n",
      "|    value_loss           | 6.92         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.94814814814815\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010588519 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.54       |\n",
      "|    explained_variance   | 0.026       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.04        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    value_loss           | 5.96        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.773333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011910824 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.53       |\n",
      "|    explained_variance   | 0.0363      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0448     |\n",
      "|    value_loss           | 5.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.169696969696968\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 11264        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0126366485 |\n",
      "|    clip_fraction        | 0.234        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.53        |\n",
      "|    explained_variance   | 0.0428       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.478        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0465      |\n",
      "|    value_loss           | 5.37         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.711111111111112\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014283869 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.52       |\n",
      "|    explained_variance   | 0.0849      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.02        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 4.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.3025641025641\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014952454 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.52       |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 4.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.628571428571426\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014363075 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.52       |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.775       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 4.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.75555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015260673 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.52       |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.34        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 4.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.5125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014573863 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.52       |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.62        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 4.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.219607843137254\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -185         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 138          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 125          |\n",
      "|    total_timesteps      | 17408        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0121046575 |\n",
      "|    clip_fraction        | 0.219        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.52        |\n",
      "|    explained_variance   | 0.39         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.29         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0494      |\n",
      "|    value_loss           | 4.44         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.7\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012299187 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.13        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 4.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.20701754385965\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010842262 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.862       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 4.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.666666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011765614 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 4.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.8984126984127\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 144         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010678585 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.514       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.16        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.06666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010479949 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.758       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.2927536231884\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011844791 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 4.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.44166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 148         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011078343 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.585       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.55733333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 149         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008811767 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.62        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 3.83        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.55641025641026\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 149         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009372862 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 4.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.51111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 182         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009756237 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 3.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.36666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 189         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010778347 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 3.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.19310344827586\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 195         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010280866 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.83        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 3.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.04\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 200         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010292594 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.795       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 3.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.78709677419355\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 206         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009213103 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.999       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 3.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.55\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009382872 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.655       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 3.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.4020202020202\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011712916 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.38       |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 3.11        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.1764705882353\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010823073 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.36       |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.926       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.904761904761905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 231         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010610027 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.34       |\n",
      "|    explained_variance   | 0.704       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 3.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.62592592592593\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010111909 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.31       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.913       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 3.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.32432432432432\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 156        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 242        |\n",
      "|    total_timesteps      | 37888      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01024227 |\n",
      "|    clip_fraction        | 0.188      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.28      |\n",
      "|    explained_variance   | 0.704      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.08       |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0554    |\n",
      "|    value_loss           | 3.34       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.04912280701755\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -173       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 156        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 248        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00993631 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.25      |\n",
      "|    explained_variance   | 0.73       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.03       |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0549    |\n",
      "|    value_loss           | 3.1        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.69059829059829\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009389946 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.23       |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 3.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 262         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010765173 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.2        |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.394       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.988617886178865\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 270         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010081588 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.15       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.815       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0574     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.65396825396825\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010297618 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.11       |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.659       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0573     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.291472868217056\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 282         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009588674 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.08       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.62        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 3.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.915151515151514\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -166         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 155          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 289          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094184745 |\n",
      "|    clip_fraction        | 0.179        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.04        |\n",
      "|    explained_variance   | 0.704        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.732        |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.055       |\n",
      "|    value_loss           | 3.03         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.50222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010051629 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.99       |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.647       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 3.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.13623188405798\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -163        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 302         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009139005 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.95       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.7         |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.7404255319149\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -160         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 308          |\n",
      "|    total_timesteps      | 48128        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093711605 |\n",
      "|    clip_fraction        | 0.182        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.89        |\n",
      "|    explained_variance   | 0.647        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.755        |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0556      |\n",
      "|    value_loss           | 3.08         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.27222222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -158        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009264205 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.85       |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.82176870748299\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -156        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 320         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010589483 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.79       |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.955       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.38933333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -154        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 327         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009895926 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.72       |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.816       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.90588235294118\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -152        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 333         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009944534 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.69       |\n",
      "|    explained_variance   | 0.594       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.884       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.42179487179487\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -150        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010414282 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.68       |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.611       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.9245283018868\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -147        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 345         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010154936 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.63       |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.879       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.44197530864197\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -145        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 352         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011036804 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.6        |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.378       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.94424242424242\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 358         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010177376 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.53       |\n",
      "|    explained_variance   | 0.603       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.806       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.43214285714286\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -140        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 364         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010227347 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.47       |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.927       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.92748538011696\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -137        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 369         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010887857 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.46       |\n",
      "|    explained_variance   | 0.568       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.443       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.40114942528736\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -135        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010092893 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.42       |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.977       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.91186440677966\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -132        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 379         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010693876 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.4        |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.674       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.40222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -130        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009903289 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.778       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.89180327868853\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -127       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 388        |\n",
      "|    total_timesteps      | 62464      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00870637 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.29      |\n",
      "|    explained_variance   | 0.578      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.69       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0549    |\n",
      "|    value_loss           | 2.84       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.38924731182796\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -124        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 392         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009873398 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.25       |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.918       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.84232804232805\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -121        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 395         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010920224 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.22       |\n",
      "|    explained_variance   | 0.517       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.703       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0577     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.30833333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -119        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 399         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009798505 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.17       |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.893       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.75692307692307\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -116        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 403         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010341158 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.17       |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.615       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.21313131313131\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -114        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 406         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010872055 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.12       |\n",
      "|    explained_variance   | 0.594       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.543       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0587     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.65273631840796\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -111        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 410         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009980112 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.07       |\n",
      "|    explained_variance   | 0.597       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.718       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.08235294117647\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -109        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 414         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008446241 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.854       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.50434782608696\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -107        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 417         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009961006 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.99       |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.749       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.92666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -105        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 421         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008510763 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.94       |\n",
      "|    explained_variance   | 0.594       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.86        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.33990610328638\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -102        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 425         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010187877 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.514       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.7462962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -99.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 428         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008499812 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.85       |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.607       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.15799086757991\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -96.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 172         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 432         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008991471 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.8        |\n",
      "|    explained_variance   | 0.634       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.575       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.58018018018018\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -94.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 435        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00814953 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.76      |\n",
      "|    explained_variance   | 0.607      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.92       |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0497    |\n",
      "|    value_loss           | 2.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.99822222222222\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -92.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 438        |\n",
      "|    total_timesteps      | 76800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00915525 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.68      |\n",
      "|    explained_variance   | 0.612      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.79       |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0569    |\n",
      "|    value_loss           | 2.05       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.42368421052632\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -89.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 176         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 441         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009410044 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.59       |\n",
      "|    explained_variance   | 0.602       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.636       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.83116883116882\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -87         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 177         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 445         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009149041 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.55       |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.607       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.22820512820513\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -84.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 178         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 448         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008852944 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.5        |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.753       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.62869198312237\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -81.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 179         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 451         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008058596 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.47       |\n",
      "|    explained_variance   | 0.621       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.589       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.02\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -79.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 180          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 454          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075835455 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.41        |\n",
      "|    explained_variance   | 0.609        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.599        |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.0505      |\n",
      "|    value_loss           | 2.28         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.4156378600823\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -77.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 458         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008500718 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.39       |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.546       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0498     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.80569105691058\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -75.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 461         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008307787 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.37       |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.47        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 1.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.20240963855422\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -73         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 464         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007907003 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.33       |\n",
      "|    explained_variance   | 0.624       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.825       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.5904761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -70.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 468         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008606215 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.33       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.537       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.97647058823529\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -68.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008154729 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.29       |\n",
      "|    explained_variance   | 0.629       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.772       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0487     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.36201550387597\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -65.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 185          |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 474          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078767445 |\n",
      "|    clip_fraction        | 0.183        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.2         |\n",
      "|    explained_variance   | 0.65         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.582        |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.0527      |\n",
      "|    value_loss           | 2.12         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.7448275862069\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -63.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 186        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 478        |\n",
      "|    total_timesteps      | 89088      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00649745 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.12      |\n",
      "|    explained_variance   | 0.627      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.03       |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0469    |\n",
      "|    value_loss           | 2.67       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.11742424242425\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -60.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 481         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008710836 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.09       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.461       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0493     |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.47790262172285\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -58.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008437378 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.08       |\n",
      "|    explained_variance   | 0.579       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.822       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0481     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.83185185185185\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -55.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 487        |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00801372 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.01      |\n",
      "|    explained_variance   | 0.538      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.873      |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.0493    |\n",
      "|    value_loss           | 2.51       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.19560439560439\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -53.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 491         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007978572 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.98       |\n",
      "|    explained_variance   | 0.568       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.829       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.54057971014493\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -51         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 494         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009473438 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.759       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.90322580645162\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -49.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 497         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007937465 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.98       |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.719       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.26524822695035\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -47         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 501         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008487275 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.96       |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.632       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0481     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.6238596491228\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -44.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007819256 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.93       |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.693       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.98472222222222\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -43.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 193          |\n",
      "|    iterations           | 96           |\n",
      "|    time_elapsed         | 508          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076608947 |\n",
      "|    clip_fraction        | 0.168        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.91        |\n",
      "|    explained_variance   | 0.632        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.665        |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.0482      |\n",
      "|    value_loss           | 2.06         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.3340206185567\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -41.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 194          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 511          |\n",
      "|    total_timesteps      | 99328        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071700662 |\n",
      "|    clip_fraction        | 0.171        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.86        |\n",
      "|    explained_variance   | 0.58         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.669        |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0483      |\n",
      "|    value_loss           | 2.27         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.68299319727892\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -39.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 514         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006628211 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.86       |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.789       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0411     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.02154882154882\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -37.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 195          |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 517          |\n",
      "|    total_timesteps      | 101376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073615285 |\n",
      "|    clip_fraction        | 0.159        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.8         |\n",
      "|    explained_variance   | 0.592        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.733        |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0481      |\n",
      "|    value_loss           | 2.63         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.37466666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -35.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 521         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007104956 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.76       |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.897       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 25.14\n",
      "Overall Average Successful Assignments: 61.56048820288813\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298b3d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Testing Summary ----\n",
      "Mean Reward: 66.0\n",
      "Standard Deviation of Reward: 0.0\n",
      "Average Successful Assignments in Testing: 133.0\n"
     ]
    }
   ],
   "source": [
    "# Load new task dataset for testing\n",
    "new_tasks_df = pd.read_csv('RandomTasks200Test.csv')\n",
    "new_tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "test_env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, new_tasks_df), n_envs=1)\n",
    "\n",
    "# Evaluate the model on the new test environment\n",
    "mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=10)\n",
    "# Extract the successful assignments history from the test environment\n",
    "successful_assignments = test_env.envs[0].env.get_average_success()\n",
    "\n",
    "print(\"---- Testing Summary ----\")\n",
    "print(f\"Mean Reward: {mean_reward}\")\n",
    "print(f\"Standard Deviation of Reward: {std_reward}\")\n",
    "print(f\"Average Successful Assignments in Testing: {successful_assignments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897df48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
