{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43607ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 6.533333333333333\n",
      "All assignments history: [8, 6, 6, 7, 11, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -185     |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -166.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.333333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 301         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009608667 |\n",
      "|    clip_fraction        | 0.076       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.155      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.67        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 16.8        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.155555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 285         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012512729 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0503     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.733333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 259         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013792111 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.00738     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 9.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.08\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014980599 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.001       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 9.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -152.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.755555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015736707 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0414      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 7.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.438095238095238\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 241        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01584245 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0379     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.32       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0578    |\n",
      "|    value_loss           | 7.5        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.516666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016896749 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0541      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 6.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.133333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018831387 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0638      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.08        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 5.64        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.206666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019303534 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0864      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0616     |\n",
      "|    value_loss           | 5.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.67878787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020227399 |\n",
      "|    clip_fraction        | 0.44        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.672       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 4.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.088888888888889\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 238        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 51         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02232533 |\n",
      "|    clip_fraction        | 0.488      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.177      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.519      |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0639    |\n",
      "|    value_loss           | 4.54       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.671794871794871\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024630224 |\n",
      "|    clip_fraction        | 0.535       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.453       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0672     |\n",
      "|    value_loss           | 4.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.7\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026623417 |\n",
      "|    clip_fraction        | 0.587       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.275       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0713     |\n",
      "|    value_loss           | 4.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.47111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023715412 |\n",
      "|    clip_fraction        | 0.511       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.316       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0667     |\n",
      "|    value_loss           | 4.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.104166666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025829472 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.296       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 4.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.227450980392156\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024594039 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.462       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0706     |\n",
      "|    value_loss           | 3.95        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.677777777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023387525 |\n",
      "|    clip_fraction        | 0.496       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.326315789473686\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022911524 |\n",
      "|    clip_fraction        | 0.471       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.522       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.80666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023206195 |\n",
      "|    clip_fraction        | 0.503       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.356       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0722     |\n",
      "|    value_loss           | 3.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.676190476190477\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022999201 |\n",
      "|    clip_fraction        | 0.489       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.61        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 3.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.53030303030303\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 238        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 94         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02102389 |\n",
      "|    clip_fraction        | 0.441      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.61       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.46       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0673    |\n",
      "|    value_loss           | 3.77       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.527536231884056\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021571524 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.86        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 3.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.66111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022041582 |\n",
      "|    clip_fraction        | 0.47        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.33        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 3.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.21066666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 237        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 107        |\n",
      "|    total_timesteps      | 25600      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02163418 |\n",
      "|    clip_fraction        | 0.434      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.68       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.04       |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0695    |\n",
      "|    value_loss           | 3.49       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.82820512820513\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 235        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 113        |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02062812 |\n",
      "|    clip_fraction        | 0.44       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.684      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.163      |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0699    |\n",
      "|    value_loss           | 3.45       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.64938271604938\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021015897 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.283       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 3.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.25952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019579537 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.43        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0699     |\n",
      "|    value_loss           | 3.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.714942528735634\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019474823 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.908       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 3.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.07555555555555\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 134        |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02046341 |\n",
      "|    clip_fraction        | 0.433      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.703      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.91       |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.0735    |\n",
      "|    value_loss           | 3.47       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.21075268817204\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018693378 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0681      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0659     |\n",
      "|    value_loss           | 3.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.40208333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 220         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020247238 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.743       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 3.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.34949494949495\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 218         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016106589 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.596       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 3.26        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.17058823529412\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016575957 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 3.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.693333333333335\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -182         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 167          |\n",
      "|    total_timesteps      | 35840        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0154034365 |\n",
      "|    clip_fraction        | 0.291        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.26        |\n",
      "|    explained_variance   | 0.757        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.249        |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.0639      |\n",
      "|    value_loss           | 3.02         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.477777777777774\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016989537 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.226       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.0036036036036\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015617693 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.61        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.766666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015306884 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.424       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 3.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.07692307692308\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 204        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 195        |\n",
      "|    total_timesteps      | 39936      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01606419 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.777      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.358      |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0664    |\n",
      "|    value_loss           | 3          |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.41833333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 202         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016665034 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.71        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.923577235772356\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 210         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014731061 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.957       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.287301587301585\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 197        |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 217        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01573599 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.795      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.03       |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | -0.0662    |\n",
      "|    value_loss           | 2.82       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.76589147286822\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015937706 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.275757575757574\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 192        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 233        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01612452 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.818      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.541      |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.0661    |\n",
      "|    value_loss           | 2.5        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.828148148148145\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016094068 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.549       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.37101449275362\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 189        |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 248        |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01680895 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.812      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.489      |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.0686    |\n",
      "|    value_loss           | 2.61       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.06524822695035\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 256        |\n",
      "|    total_timesteps      | 48128      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01746249 |\n",
      "|    clip_fraction        | 0.34       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.808      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.499      |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0695    |\n",
      "|    value_loss           | 2.74       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.56944444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 264         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017313834 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.813       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.04761904761905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 271         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017025512 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.324       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.462666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 279         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017636675 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.81        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.88366013071895\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 286        |\n",
      "|    total_timesteps      | 52224      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01651843 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.819      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.058      |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0647    |\n",
      "|    value_loss           | 2.61       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.312820512820515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 180         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 294         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017198054 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.255       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.72578616352201\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 179         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 302         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017146202 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.818       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.22962962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 178         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018535648 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.69        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0725     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.69939393939394\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 177         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 317         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017493669 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.151      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.26904761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 176         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 325         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017875928 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.186       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.78947368421053\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 175         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 333         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017502086 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.288       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.34022988505747\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 173        |\n",
      "|    iterations           | 58         |\n",
      "|    time_elapsed         | 341        |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01876794 |\n",
      "|    clip_fraction        | 0.393      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.834      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.389      |\n",
      "|    n_updates            | 570        |\n",
      "|    policy_gradient_loss | -0.0717    |\n",
      "|    value_loss           | 2.4        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.78531073446328\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 172         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 349         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017155875 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.271       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.21\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 357         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018696107 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.749       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0743     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.69508196721311\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 365         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018961482 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.536       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.07096774193548\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 372         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017968282 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.498       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.4\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 381         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019126814 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.196      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.07       |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.728125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 388         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019486967 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.623       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.0974358974359\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 396        |\n",
      "|    total_timesteps      | 66560      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01884222 |\n",
      "|    clip_fraction        | 0.386      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.833      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.425      |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0704    |\n",
      "|    value_loss           | 2.52       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.491919191919195\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 404         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018844906 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.626       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.91442786069652\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -177       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 67         |\n",
      "|    time_elapsed         | 412        |\n",
      "|    total_timesteps      | 68608      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02030826 |\n",
      "|    clip_fraction        | 0.404      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.82       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.04       |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0726    |\n",
      "|    value_loss           | 2.4        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.3343137254902\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -177       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 68         |\n",
      "|    time_elapsed         | 420        |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01852722 |\n",
      "|    clip_fraction        | 0.401      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.823      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.672      |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | -0.0741    |\n",
      "|    value_loss           | 2.44       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.75072463768116\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 428         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019085264 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.446       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.163809523809526\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018209549 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.54835680751174\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 443         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020199358 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.918       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.95\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 451         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017849047 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.335159817351595\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 459         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018257339 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.423       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.69009009009009\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 467         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017072093 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.449       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.013333333333335\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 476        |\n",
      "|    total_timesteps      | 76800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01985868 |\n",
      "|    clip_fraction        | 0.402      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.18      |\n",
      "|    explained_variance   | 0.846      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.13       |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0697    |\n",
      "|    value_loss           | 2.2        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.29824561403509\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019149583 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.375       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.5991341991342\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 493         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020071201 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.399       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.87264957264957\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -174       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 501        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01927502 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.148      |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.0723    |\n",
      "|    value_loss           | 2.33       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.13417721518987\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -174       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 79         |\n",
      "|    time_elapsed         | 509        |\n",
      "|    total_timesteps      | 80896      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02195805 |\n",
      "|    clip_fraction        | 0.443      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.837      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.208      |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0739    |\n",
      "|    value_loss           | 2.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.325833333333335\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -174       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 517        |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02027855 |\n",
      "|    clip_fraction        | 0.401      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.804      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.842      |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | -0.0754    |\n",
      "|    value_loss           | 2.64       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.538271604938274\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 525         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021925796 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.311      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.798373983739836\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -173       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 533        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02146716 |\n",
      "|    clip_fraction        | 0.446      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.15      |\n",
      "|    explained_variance   | 0.836      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.112      |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.0739    |\n",
      "|    value_loss           | 2.13       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.02971887550201\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 540         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022359552 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.224       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0773     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.29047619047618\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 549         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020824661 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.328       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.53882352941176\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 557         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018617854 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.397       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0737     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.7922480620155\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 566         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018655302 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0522      |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.02681992337165\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -172       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 154        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 575        |\n",
      "|    total_timesteps      | 89088      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02042925 |\n",
      "|    clip_fraction        | 0.42       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.1       |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.472      |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0748    |\n",
      "|    value_loss           | 2.28       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.23787878787878\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 583         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020595798 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.224      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0729     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.47191011235955\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 592         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020178862 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.264       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.66740740740741\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -171       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 153        |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 600        |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02005762 |\n",
      "|    clip_fraction        | 0.416      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.08      |\n",
      "|    explained_variance   | 0.827      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.13       |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.0725    |\n",
      "|    value_loss           | 2.26       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.8923076923077\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -171       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 152        |\n",
      "|    iterations           | 91         |\n",
      "|    time_elapsed         | 609        |\n",
      "|    total_timesteps      | 93184      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02108547 |\n",
      "|    clip_fraction        | 0.462      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.07      |\n",
      "|    explained_variance   | 0.82       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.221     |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0782    |\n",
      "|    value_loss           | 2.27       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.12028985507246\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 617         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019825919 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.233       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0744     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.32688172043011\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 626         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021930343 |\n",
      "|    clip_fraction        | 0.461       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.03       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.862       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.077      |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.52127659574468\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 634         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019783208 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.447       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0763     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.71438596491228\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 642         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020809725 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8          |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.694       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0746     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.91944444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 650         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018773323 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.98       |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.706       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0751     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.11546391752577\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 659         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019919734 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.95       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.225       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0766     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.32993197278911\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 666         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020322692 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.95       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.109      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0764     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.53468013468013\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 674         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019685317 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.93       |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.374       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0736     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.726\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 682         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020951916 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.92       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0745     |\n",
      "|    value_loss           | 2.16        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -8.42\n",
      "Overall Average Successful Assignments: 46.11190205400554\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e66cc86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 5.4\n",
      "All assignments history: [4, 9, 9, 8, 11, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -184     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 149         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009752127 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.118      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.3         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 16.5        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.2\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011865662 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0325     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.55\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012201357 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0837      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0573     |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.493333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014325835 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0673      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.91        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 8.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.488888888888889\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 133        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 46         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01591589 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | 0.08       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.48       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.062     |\n",
      "|    value_loss           | 7.53       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.00952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017243572 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 6.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.333333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017610725 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 6.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.348148148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018715829 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.9         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0629     |\n",
      "|    value_loss           | 5.68        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.5\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019670688 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.453       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 5.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.96969696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021748424 |\n",
      "|    clip_fraction        | 0.484       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.49        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 4.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.822222222222223\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 123        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 99         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02352583 |\n",
      "|    clip_fraction        | 0.53       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.196      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.853      |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0667    |\n",
      "|    value_loss           | 4.34       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023957362 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.919       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0658     |\n",
      "|    value_loss           | 4.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.2\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024003752 |\n",
      "|    clip_fraction        | 0.52        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.17        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 4.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.44\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025723226 |\n",
      "|    clip_fraction        | 0.544       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.49        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 4.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.65\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023376822 |\n",
      "|    clip_fraction        | 0.518       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.156       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 4.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.07843137254902\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023119241 |\n",
      "|    clip_fraction        | 0.496       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.142       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 4.31        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -122.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.02962962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023381446 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 4.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -122.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.96842105263158\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021595461 |\n",
      "|    clip_fraction        | 0.478       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.52        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.94        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0665     |\n",
      "|    value_loss           | 3.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.016666666666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 121        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 169        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01992846 |\n",
      "|    clip_fraction        | 0.408      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.532      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.48       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0652    |\n",
      "|    value_loss           | 4.1        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.06031746031746\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 121        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 177        |\n",
      "|    total_timesteps      | 21504      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02108692 |\n",
      "|    clip_fraction        | 0.437      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.561      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.32       |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0656    |\n",
      "|    value_loss           | 3.88       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.97878787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020347066 |\n",
      "|    clip_fraction        | 0.43        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 3.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.240579710144928\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019439813 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.62        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 3.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.019444444444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 201         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018612053 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.241       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 3.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.738666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 210         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016320134 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.716       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 3.5         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.64102564102564\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018109586 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 3.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.59753086419753\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019709203 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.533       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 3.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.45\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018060686 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.657       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 3.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.197701149425285\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020168874 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.469       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.96222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018773362 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.62        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 3.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.825806451612905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 260         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019331763 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.727       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 3.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.702083333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 122        |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 268        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01804276 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.744      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.484      |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0686    |\n",
      "|    value_loss           | 3.23       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.541414141414144\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017171245 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.769       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.35686274509804\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 122        |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 284        |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01924657 |\n",
      "|    clip_fraction        | 0.382      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.779      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.994      |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.0671    |\n",
      "|    value_loss           | 2.84       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.205714285714286\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 292         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019392032 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.474074074074075\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 300         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018473737 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.584       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.61981981981982\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 122        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 309        |\n",
      "|    total_timesteps      | 37888      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01892596 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.791      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.22       |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0676    |\n",
      "|    value_loss           | 2.68       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.72105263157895\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 317         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018434513 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.773       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.068      |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.654700854700856\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 325         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017712558 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.217       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.068      |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.333333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 333         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018236367 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.297560975609755\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 342         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017236417 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.69        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.13968253968254\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 349         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019152729 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.157       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.04341085271318\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 358         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019182082 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.595       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0729     |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.89242424242424\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 368         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018947173 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.156       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.74666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 377         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017471474 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0143     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.48695652173913\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 122        |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 386        |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01850825 |\n",
      "|    clip_fraction        | 0.389      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.848      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.00019   |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.0733    |\n",
      "|    value_loss           | 2.17       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.17304964539007\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 394         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018911198 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.569       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.84166666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 122        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 402        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01776027 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.862      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.316      |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.07      |\n",
      "|    value_loss           | 2.09       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.50884353741497\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 122        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 410        |\n",
      "|    total_timesteps      | 50176      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01856164 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.871      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.156      |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0705    |\n",
      "|    value_loss           | 1.88       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.08133333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 122        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 418        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01865163 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.856      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.158      |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.073     |\n",
      "|    value_loss           | 2.07       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.74771241830065\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 426         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018967912 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0734     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.39871794871795\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017960096 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.855       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.00125786163522\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 443         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016998053 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.388       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.58765432098765\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 451         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017085476 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0628     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 1.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.15151515151515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 460         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018029714 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.889       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.66904761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 468         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020215675 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0274     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0739     |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.1812865497076\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 478         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017953757 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.228       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0706     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.644827586206894\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 486         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019321296 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.21        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0758     |\n",
      "|    value_loss           | 1.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.10960451977401\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 496         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020162761 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0401      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0759     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.54888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020385958 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.218      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0765     |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.93005464480874\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 514         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020622067 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0762     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.31612903225806\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 525         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018530458 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.6994708994709\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 535         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020170087 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.226       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0765     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.030208333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 120        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 545        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01817667 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0254    |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0734    |\n",
      "|    value_loss           | 1.5        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.40512820512821\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 556         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022119157 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0417      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0762     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.74747474747475\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 566         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019250277 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0171     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0751     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.07860696517413\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 576         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020775119 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0184     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0759     |\n",
      "|    value_loss           | 1.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.36960784313725\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 585         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020498399 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0983     |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0781     |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.656038647342996\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 594         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022427807 |\n",
      "|    clip_fraction        | 0.455       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00112     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0813     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.955238095238094\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 118        |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 605        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02019516 |\n",
      "|    clip_fraction        | 0.436      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.916      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.00737   |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.079     |\n",
      "|    value_loss           | 1.4        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.26103286384976\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 614         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019104144 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0104      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0776     |\n",
      "|    value_loss           | 1.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.57592592592592\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 623         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018840244 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0799     |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0766     |\n",
      "|    value_loss           | 1.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.86027397260274\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 633         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019801851 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.173       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0814     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.15675675675676\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 643         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021273522 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0772      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0791     |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.446222222222225\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 652         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019012485 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0776     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0775     |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.72543859649123\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 661         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019063972 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.206       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0809     |\n",
      "|    value_loss           | 1.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.9974025974026\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 671         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020451877 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0115      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0819     |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.25128205128205\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 681         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022355966 |\n",
      "|    clip_fraction        | 0.468       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.18       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.084      |\n",
      "|    value_loss           | 1.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.54008438818565\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 691         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018990774 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.285       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0803     |\n",
      "|    value_loss           | 1.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.83583333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 701         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022289433 |\n",
      "|    clip_fraction        | 0.455       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0619      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0833     |\n",
      "|    value_loss           | 1.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.13168724279835\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 116        |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 710        |\n",
      "|    total_timesteps      | 82944      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02184752 |\n",
      "|    clip_fraction        | 0.466      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.915      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.101     |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0855    |\n",
      "|    value_loss           | 1.32       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.4130081300813\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 720         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020487696 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0376      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0819     |\n",
      "|    value_loss           | 1.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.69236947791164\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 730         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020782799 |\n",
      "|    clip_fraction        | 0.457       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0128      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0838     |\n",
      "|    value_loss           | 1.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.97936507936508\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 739         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021177271 |\n",
      "|    clip_fraction        | 0.434       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.172      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.082      |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.26039215686275\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 748         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018105254 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.158      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0816     |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.51007751937985\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 758         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020053629 |\n",
      "|    clip_fraction        | 0.434       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0783     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0835     |\n",
      "|    value_loss           | 1.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.75785440613026\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 767         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019742008 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.131      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0817     |\n",
      "|    value_loss           | 1.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.98787878787878\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 776         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020395435 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.213      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0825     |\n",
      "|    value_loss           | 1.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.21423220973783\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -172       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 115        |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 786        |\n",
      "|    total_timesteps      | 91136      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01930872 |\n",
      "|    clip_fraction        | 0.41       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.14      |\n",
      "|    explained_variance   | 0.921      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.148     |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0835    |\n",
      "|    value_loss           | 1.28       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.44\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 795         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022436278 |\n",
      "|    clip_fraction        | 0.475       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.126      |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0876     |\n",
      "|    value_loss           | 0.957       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.68205128205128\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -171       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 115        |\n",
      "|    iterations           | 91         |\n",
      "|    time_elapsed         | 805        |\n",
      "|    total_timesteps      | 93184      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02046289 |\n",
      "|    clip_fraction        | 0.427      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.13      |\n",
      "|    explained_variance   | 0.93       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0781    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0846    |\n",
      "|    value_loss           | 1.13       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.92608695652174\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 814         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021163054 |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.24       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.086      |\n",
      "|    value_loss           | 1.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.17275985663082\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 822         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019706875 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.206      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0857     |\n",
      "|    value_loss           | 1.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.40851063829787\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 831         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019008122 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0325     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0848     |\n",
      "|    value_loss           | 1.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.65333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 841         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020659482 |\n",
      "|    clip_fraction        | 0.434       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.221      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0867     |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.90069444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 850         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018527094 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.254      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0843     |\n",
      "|    value_loss           | 1.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.13676975945017\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 860         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020092797 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.128      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0866     |\n",
      "|    value_loss           | 1.19        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.37278911564626\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 870         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025265118 |\n",
      "|    clip_fraction        | 0.5         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0397     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0878     |\n",
      "|    value_loss           | 1.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.5973063973064\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 879         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017136388 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.06       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0272      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0818     |\n",
      "|    value_loss           | 1.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.83333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 890         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020979319 |\n",
      "|    clip_fraction        | 0.445       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.193      |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0848     |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -1.68\n",
      "Overall Average Successful Assignments: 48.49513584562872\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ff3fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -166.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.4\n",
      "All assignments history: [8, 9, 8, 13, 8, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    fps             | 152      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -156.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.966666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009628169 |\n",
      "|    clip_fraction        | 0.0854      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.123       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.25        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 16.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.177777777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012662994 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0564      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.3\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 115        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 35         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01282469 |\n",
      "|    clip_fraction        | 0.221      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | 0.0594     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.09       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0582    |\n",
      "|    value_loss           | 10.1       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.946666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015241128 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 7.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.644444444444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016594881 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0926      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.31        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 7.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.323809523809523\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 65         |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01711344 |\n",
      "|    clip_fraction        | 0.368      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.128      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.98       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0643    |\n",
      "|    value_loss           | 6.28       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017298304 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.24        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 6.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.37037037037037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018737046 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 5.52        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.606666666666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 94         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01989601 |\n",
      "|    clip_fraction        | 0.436      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.151      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.68       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0603    |\n",
      "|    value_loss           | 5.22       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -194.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.593939393939394\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021781994 |\n",
      "|    clip_fraction        | 0.485       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 4.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.32777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022471908 |\n",
      "|    clip_fraction        | 0.497       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.192       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 4.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.446153846153845\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023657037 |\n",
      "|    clip_fraction        | 0.532       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 4.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.276190476190475\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023633085 |\n",
      "|    clip_fraction        | 0.513       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 4.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.586666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024760865 |\n",
      "|    clip_fraction        | 0.551       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.327       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 4.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -190.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.591666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022266896 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 4.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.098039215686274\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -183      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 108       |\n",
      "|    iterations           | 17        |\n",
      "|    time_elapsed         | 160       |\n",
      "|    total_timesteps      | 17408     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0226195 |\n",
      "|    clip_fraction        | 0.498     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.27     |\n",
      "|    explained_variance   | 0.414     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.551     |\n",
      "|    n_updates            | 160       |\n",
      "|    policy_gradient_loss | -0.0657   |\n",
      "|    value_loss           | 4.54      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -190.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.344444444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023919597 |\n",
      "|    clip_fraction        | 0.521       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 4.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.7859649122807\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021873612 |\n",
      "|    clip_fraction        | 0.472       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.53        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 4.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.88\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 188        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02310418 |\n",
      "|    clip_fraction        | 0.491      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.484      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.94       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0692    |\n",
      "|    value_loss           | 4.45       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.984126984126984\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024247065 |\n",
      "|    clip_fraction        | 0.508       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 4.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.996969696969696\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 207         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021336125 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.262       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 4.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.78840579710145\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 217        |\n",
      "|    total_timesteps      | 23552      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01980906 |\n",
      "|    clip_fraction        | 0.422      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.54       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.95       |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0689    |\n",
      "|    value_loss           | 4.25       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.81111111111111\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 226        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02037903 |\n",
      "|    clip_fraction        | 0.425      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.569      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 3.57       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.067     |\n",
      "|    value_loss           | 4.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.682666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 236         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018825375 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.578       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 4.28        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.81282051282051\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 245         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017142924 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.603       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.308       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 4.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.950617283950617\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 253         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017396487 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.3         |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 4.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.485714285714284\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 263         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018269189 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.323       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 4.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.91264367816092\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 273         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017260771 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.623       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.1         |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 3.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.59111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 282         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017672319 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 4.77        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 4           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.50537634408602\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 291         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016716285 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 3.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.6375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019106084 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.363       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 3.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.078787878787878\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 310         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017699251 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.9         |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 3.94        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.33921568627451\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 319         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018358361 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 3.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.333333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 329         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016106801 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.362       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 3.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.16111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016539807 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.535       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0628     |\n",
      "|    value_loss           | 3.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.96936936936937\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 350        |\n",
      "|    total_timesteps      | 37888      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01930331 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.665      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.52       |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0687    |\n",
      "|    value_loss           | 3.71       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.943859649122807\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 360         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019576773 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 3.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.823931623931625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 368         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015870813 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.05        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 3.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.578333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 376         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017037448 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.327       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 3.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.115447154471546\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 385         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017834777 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0667     |\n",
      "|    value_loss           | 3.55        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.84761904761905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 394         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017131938 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.736434108527135\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 403        |\n",
      "|    total_timesteps      | 44032      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01685347 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.71       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.38       |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0679    |\n",
      "|    value_loss           | 3.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.42575757575757\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 411         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016768735 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.42        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 3.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.00296296296296\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 420         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017804697 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.426       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 3.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.74782608695652\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 430         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017991569 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0879      |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.51631205673759\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 440         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018182911 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.179       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.334722222222226\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 449         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017987253 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0343     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0667     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.12517006802721\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 459         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016326047 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00288    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.73733333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 469         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016877586 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 3.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.179084967320264\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 478         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016808761 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.86794871794872\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 488         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017304745 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.241       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.544654088050315\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 499         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016622957 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.179       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.306172839506175\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 509        |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01641149 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.796      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.14       |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.0671    |\n",
      "|    value_loss           | 2.77       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.04606060606061\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 519         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017996259 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.541       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.77023809523809\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 529        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01767844 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.798      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.891      |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0694    |\n",
      "|    value_loss           | 2.73       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.50292397660819\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 538         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018869983 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.932       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.225287356321836\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -180      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 108       |\n",
      "|    iterations           | 58        |\n",
      "|    time_elapsed         | 548       |\n",
      "|    total_timesteps      | 59392     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0182902 |\n",
      "|    clip_fraction        | 0.378     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.24     |\n",
      "|    explained_variance   | 0.814     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.107     |\n",
      "|    n_updates            | 570       |\n",
      "|    policy_gradient_loss | -0.0687   |\n",
      "|    value_loss           | 2.56      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.833898305084745\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 557        |\n",
      "|    total_timesteps      | 60416      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02101956 |\n",
      "|    clip_fraction        | 0.417      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.807      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0141    |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0723    |\n",
      "|    value_loss           | 2.51       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.397777777777776\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 566         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018406581 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.953005464480874\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 576         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018770251 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0638     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.5258064516129\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 585         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019297188 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.823       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.1015873015873\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 595         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018733133 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0716     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.628125\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 605        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01752892 |\n",
      "|    clip_fraction        | 0.329      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0918    |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0693    |\n",
      "|    value_loss           | 2.49       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.12\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 614         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017906334 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.673       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.63939393939394\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 623         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016941588 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.238       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.09452736318408\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 633         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019523766 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0279      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.52843137254902\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 643         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017693825 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.746       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.978743961352656\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 652         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018381309 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.27        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.43238095238095\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 661         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019514348 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.795       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0739     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.87230046948357\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 671         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017631035 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.402       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.330555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 680         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018888483 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.117      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.801826484018264\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 690         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018609526 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.564       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.27117117117117\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 700         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016967721 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.69511111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 709         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018331716 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.562       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.10438596491228\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 719         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020121796 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00435    |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.55930735930736\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 728         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018123422 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.28        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.89572649572649\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 738         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018135171 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.609       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.26413502109705\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 747         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018210366 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.169      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.645833333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 757         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019260954 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.631       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.008230452674894\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 766         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017483097 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.529       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.31544715447154\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 776         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018649027 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.969       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.71887550200803\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 785         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017667023 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.504       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.07222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 794         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016597804 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.38117647058824\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 804         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018966662 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.379       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0699     |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.67364341085271\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -177      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 108       |\n",
      "|    iterations           | 86        |\n",
      "|    time_elapsed         | 814       |\n",
      "|    total_timesteps      | 88064     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0196651 |\n",
      "|    clip_fraction        | 0.404     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.19     |\n",
      "|    explained_variance   | 0.873     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | -0.0719   |\n",
      "|    n_updates            | 850       |\n",
      "|    policy_gradient_loss | -0.0725   |\n",
      "|    value_loss           | 1.97      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.01685823754789\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 823         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017739663 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00241    |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.45378787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 833         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018469222 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.225       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.8876404494382\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 842         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018773226 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.441       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0722     |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.27037037037037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 851         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017704576 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.881       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.6981684981685\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 860         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018199682 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.596       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 1.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.04492753623188\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 870         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018610384 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.132       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0738     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.36200716845878\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -174       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 880        |\n",
      "|    total_timesteps      | 95232      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01870362 |\n",
      "|    clip_fraction        | 0.384      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.16      |\n",
      "|    explained_variance   | 0.888      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.133      |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.07      |\n",
      "|    value_loss           | 1.69       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.68014184397163\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 888         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015897414 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.284       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.08771929824562\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 898         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017581105 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.67        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 1.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.46458333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 907         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018045919 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.557       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0736     |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.84192439862543\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 917         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017861761 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.579       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.073      |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.24081632653061\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 926         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019859372 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.362       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0754     |\n",
      "|    value_loss           | 1.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.635690235690234\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 935         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020184504 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0665     |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0741     |\n",
      "|    value_loss           | 1.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.026\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 945         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018334618 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.103      |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0743     |\n",
      "|    value_loss           | 1.79        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -24.66\n",
      "Overall Average Successful Assignments: 38.740154678802995\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9909454b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -190.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 6.4\n",
      "All assignments history: [8, 15, 6, 6, 11, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.166666666666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 130        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2048       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00893743 |\n",
      "|    clip_fraction        | 0.0597     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | -0.189     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.17       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0542    |\n",
      "|    value_loss           | 16.1       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.733333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011990678 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.108      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -166.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.583333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012559922 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0456     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.82        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0593     |\n",
      "|    value_loss           | 9.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.08\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015064031 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0373      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.15        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 8.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.788888888888888\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014946807 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0397      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 7.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.133333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016769685 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0644      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 6.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.783333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017241975 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0632      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.49        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 6.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.133333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018501934 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0725      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.74        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0607     |\n",
      "|    value_loss           | 5.72        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.673333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020267874 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0783      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 5.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.315151515151516\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022357762 |\n",
      "|    clip_fraction        | 0.503       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 4.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.655555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023375286 |\n",
      "|    clip_fraction        | 0.525       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.968       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0665     |\n",
      "|    value_loss           | 4.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.03076923076923\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022850458 |\n",
      "|    clip_fraction        | 0.503       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.163       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 4.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.076190476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024514932 |\n",
      "|    clip_fraction        | 0.53        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0245     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 4.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.786666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025206788 |\n",
      "|    clip_fraction        | 0.554       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.277       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 4.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.916666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025931526 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.997       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 3.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.411764705882355\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 159        |\n",
      "|    total_timesteps      | 17408      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02423223 |\n",
      "|    clip_fraction        | 0.529      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.512      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.395      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0713    |\n",
      "|    value_loss           | 3.87       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.27407407407407\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021207131 |\n",
      "|    clip_fraction        | 0.437       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 3.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024178835 |\n",
      "|    clip_fraction        | 0.511       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.26        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0729     |\n",
      "|    value_loss           | 3.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.883333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022185577 |\n",
      "|    clip_fraction        | 0.442       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.47        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 3.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.82857142857143\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021578867 |\n",
      "|    clip_fraction        | 0.449       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.205       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 3.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.006060606060608\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 207         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020750351 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 3.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.04927536231884\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 216         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020389091 |\n",
      "|    clip_fraction        | 0.436       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 3.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -152.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.616666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 226        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01963406 |\n",
      "|    clip_fraction        | 0.391      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.653      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.353      |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0667    |\n",
      "|    value_loss           | 3.75       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.754666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019746467 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 3.85        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.638461538461538\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 246         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020342538 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.94        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0679     |\n",
      "|    value_loss           | 3.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.45432098765432\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 257        |\n",
      "|    total_timesteps      | 27648      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02043442 |\n",
      "|    clip_fraction        | 0.403      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.678      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.968      |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.07      |\n",
      "|    value_loss           | 3.57       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.738095238095237\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 266         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019422114 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.831       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 3.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.64367816091954\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 275         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022616167 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0724     |\n",
      "|    value_loss           | 3.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.66\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 285         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021216795 |\n",
      "|    clip_fraction        | 0.442       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0732     |\n",
      "|    value_loss           | 3.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.3505376344086\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 294         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022918722 |\n",
      "|    clip_fraction        | 0.476       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.66        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0751     |\n",
      "|    value_loss           | 3.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.31666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024659758 |\n",
      "|    clip_fraction        | 0.48        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0755     |\n",
      "|    value_loss           | 3.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.183838383838385\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 313         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023083547 |\n",
      "|    clip_fraction        | 0.48        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.162       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0758     |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.07058823529412\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 324         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022093652 |\n",
      "|    clip_fraction        | 0.445       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0746     |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.86857142857143\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 334        |\n",
      "|    total_timesteps      | 35840      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02069776 |\n",
      "|    clip_fraction        | 0.41       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.748      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.347      |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0717    |\n",
      "|    value_loss           | 3.16       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.62777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 344         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020697368 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.667       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 2.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.1009009009009\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 353        |\n",
      "|    total_timesteps      | 37888      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02336815 |\n",
      "|    clip_fraction        | 0.47       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.765      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.667      |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.074     |\n",
      "|    value_loss           | 3          |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.228070175438596\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 362         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021584373 |\n",
      "|    clip_fraction        | 0.453       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.49        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0754     |\n",
      "|    value_loss           | 3.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.32136752136752\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 372         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021733036 |\n",
      "|    clip_fraction        | 0.442       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0716     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.50333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 381         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021253634 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.716       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.68943089430894\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 391         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021771133 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.664       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0748     |\n",
      "|    value_loss           | 3.01        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.993650793650794\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 400         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021930521 |\n",
      "|    clip_fraction        | 0.444       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0732     |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.372093023255815\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 409        |\n",
      "|    total_timesteps      | 44032      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02080753 |\n",
      "|    clip_fraction        | 0.421      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.778      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.2        |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0726    |\n",
      "|    value_loss           | 3.17       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.733333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 418         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021327838 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.689       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0742     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.08\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 428         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023794431 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0743     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.44202898550725\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 437         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020394392 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.659       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.680851063829785\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 447         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021609519 |\n",
      "|    clip_fraction        | 0.434       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0738     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.897222222222226\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 456        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02132921 |\n",
      "|    clip_fraction        | 0.424      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.817      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.185      |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.074     |\n",
      "|    value_loss           | 2.64       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.089795918367344\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 466         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020680517 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0724     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.422666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 475         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021130478 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.672       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0739     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.79738562091503\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021751191 |\n",
      "|    clip_fraction        | 0.436       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.73        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0758     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.17051282051282\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 493         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021053728 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.117       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0718     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.46415094339623\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 503         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020868838 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.246       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.720987654320986\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 512         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019329768 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.337       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.00242424242424\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 522         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021214101 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.17        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0732     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.44047619047619\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 531        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02031173 |\n",
      "|    clip_fraction        | 0.407      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.833      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.302      |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0737    |\n",
      "|    value_loss           | 2.46       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.72631578947368\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 540         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019200835 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.108      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.216091954022986\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 550         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020306878 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.828       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0754     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.742372881355934\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 558         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017434243 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.331       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.25222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 566         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018438872 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.123       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.692896174863385\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 575         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018626703 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0739     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.20645161290322\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 583         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019135987 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.463       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.60634920634921\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 591         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019883689 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0573     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0751     |\n",
      "|    value_loss           | 1.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.984375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 599         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017759535 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.322051282051284\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 607         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018835189 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.139       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0713     |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.6989898989899\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 615         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016984537 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.401       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.05273631840796\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 623         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017161716 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0799      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0738     |\n",
      "|    value_loss           | 1.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.37843137254902\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 631         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016797157 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.409       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.073      |\n",
      "|    value_loss           | 1.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.70531400966183\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -180      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 110       |\n",
      "|    iterations           | 69        |\n",
      "|    time_elapsed         | 640       |\n",
      "|    total_timesteps      | 70656     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0165846 |\n",
      "|    clip_fraction        | 0.315     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.23     |\n",
      "|    explained_variance   | 0.889     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.191     |\n",
      "|    n_updates            | 680       |\n",
      "|    policy_gradient_loss | -0.0724   |\n",
      "|    value_loss           | 1.76      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.03904761904762\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 647        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01861624 |\n",
      "|    clip_fraction        | 0.364      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.898      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.0755    |\n",
      "|    value_loss           | 1.57       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.36619718309859\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 656         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017883852 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.173       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.077      |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.70648148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 664         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018567167 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0898      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0782     |\n",
      "|    value_loss           | 1.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.0\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 671         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017157912 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0909      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0737     |\n",
      "|    value_loss           | 1.68        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.32612612612613\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 679         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017764144 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0778     |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0761     |\n",
      "|    value_loss           | 1.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.60977777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 687         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018470116 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.079      |\n",
      "|    value_loss           | 1.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.89561403508772\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 694         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020571616 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.176       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0808     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.16969696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 702         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019538883 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.189      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0798     |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.47435897435897\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 709         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019092258 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.031       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0799     |\n",
      "|    value_loss           | 1.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.76033755274262\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 717         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018531881 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.167       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0789     |\n",
      "|    value_loss           | 1.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.056666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 724         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018773668 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0816     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.28230452674897\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 732        |\n",
      "|    total_timesteps      | 82944      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02056228 |\n",
      "|    clip_fraction        | 0.407      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.911      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0503    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0817    |\n",
      "|    value_loss           | 1.49       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.5130081300813\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 740        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01979295 |\n",
      "|    clip_fraction        | 0.4        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0487    |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.0809    |\n",
      "|    value_loss           | 1.48       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.744578313253015\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 747         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021482341 |\n",
      "|    clip_fraction        | 0.433       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0623     |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0846     |\n",
      "|    value_loss           | 1.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.91746031746032\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 755         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018494084 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.163       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0784     |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.142745098039214\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 762         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019754648 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0792     |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.372093023255815\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 770         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021127928 |\n",
      "|    clip_fraction        | 0.444       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0856     |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.59463601532567\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 778         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020404693 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0829     |\n",
      "|    value_loss           | 1.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.80530303030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 785         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019827627 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.076      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.083      |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.02996254681648\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 793         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019693147 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00585    |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0813     |\n",
      "|    value_loss           | 1.49        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.26888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 801         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018620599 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0812     |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.4996336996337\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 808         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020915229 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0841     |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.72608695652174\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 816         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021355812 |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.104      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0869     |\n",
      "|    value_loss           | 1.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.96989247311828\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 823         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018755771 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0736     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0805     |\n",
      "|    value_loss           | 1.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.20496453900709\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 830         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020474711 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0853     |\n",
      "|    value_loss           | 1.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.41614035087719\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 838         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021189094 |\n",
      "|    clip_fraction        | 0.444       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0319     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0855     |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.63194444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 845         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022809036 |\n",
      "|    clip_fraction        | 0.451       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0803     |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0848     |\n",
      "|    value_loss           | 1.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.858419243986255\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -174       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 116        |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 852        |\n",
      "|    total_timesteps      | 99328      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02007607 |\n",
      "|    clip_fraction        | 0.422      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.15      |\n",
      "|    explained_variance   | 0.917      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.047     |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0851    |\n",
      "|    value_loss           | 1.44       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.08163265306123\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 859         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021216623 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.102      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0839     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.31649831649832\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 866         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022469893 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.209      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0872     |\n",
      "|    value_loss           | 1.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.534\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -173       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 117        |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 874        |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01987443 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.13      |\n",
      "|    explained_variance   | 0.928      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.134     |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.0852    |\n",
      "|    value_loss           | 1.26       |\n",
      "----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -40.64\n",
      "Overall Average Successful Assignments: 40.952832815423015\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3fb4618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 7.133333333333334\n",
      "All assignments history: [6, 10, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -185     |\n",
      "| time/              |          |\n",
      "|    fps             | 188      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -152.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.766666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008743267 |\n",
      "|    clip_fraction        | 0.0656      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.00796    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.13        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 16.1        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -194.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.022222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 149         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011926854 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0759     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0589     |\n",
      "|    value_loss           | 11.8        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 9.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 148         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012908822 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0405      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.18        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 9.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.573333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014923271 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.00404     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.88        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 8.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.511111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016081885 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.047       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 7.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.8\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016540486 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.034       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 6.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -162.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.391666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016828056 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0612      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.45        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 6.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.34074074074074\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 141        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 65         |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01905178 |\n",
      "|    clip_fraction        | 0.423      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0632     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.81       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0628    |\n",
      "|    value_loss           | 5.44       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.113333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020099685 |\n",
      "|    clip_fraction        | 0.436       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0818      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.801       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 5.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.4\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 137        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 81         |\n",
      "|    total_timesteps      | 11264      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02130391 |\n",
      "|    clip_fraction        | 0.468      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.102      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.82       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0607    |\n",
      "|    value_loss           | 5.1        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.43888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022830114 |\n",
      "|    clip_fraction        | 0.519       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.371       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 4.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.77948717948718\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023857106 |\n",
      "|    clip_fraction        | 0.5         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.517       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0607     |\n",
      "|    value_loss           | 4.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.833333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027802028 |\n",
      "|    clip_fraction        | 0.59        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.197       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 4.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.346666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029355884 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 4.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -184.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.754166666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027199952 |\n",
      "|    clip_fraction        | 0.565       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.898       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 4.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.647058823529413\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026277158 |\n",
      "|    clip_fraction        | 0.557       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.607       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 4.6         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.92222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025716977 |\n",
      "|    clip_fraction        | 0.559       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.357       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 4.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.63859649122807\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023497796 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0657     |\n",
      "|    value_loss           | 4.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.303333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023207098 |\n",
      "|    clip_fraction        | 0.488       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 4.26        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 4.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.97142857142857\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021922134 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.522       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 4.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.636363636363637\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021925442 |\n",
      "|    clip_fraction        | 0.441       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.74        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.065      |\n",
      "|    value_loss           | 4.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.095652173913045\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023253508 |\n",
      "|    clip_fraction        | 0.455       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.43        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 4.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.2\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020227386 |\n",
      "|    clip_fraction        | 0.432       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.29        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 4.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.434666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022456106 |\n",
      "|    clip_fraction        | 0.447       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.896       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0667     |\n",
      "|    value_loss           | 4.36        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.68717948717949\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 193         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020427722 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.603       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.133       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0646     |\n",
      "|    value_loss           | 4.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.990123456790123\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 200         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021190548 |\n",
      "|    clip_fraction        | 0.442       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.331       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 4.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.55952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 208         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022954067 |\n",
      "|    clip_fraction        | 0.459       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.604       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.95        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 3.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.048275862068966\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 137        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 215        |\n",
      "|    total_timesteps      | 29696      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02160437 |\n",
      "|    clip_fraction        | 0.438      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.632      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.2        |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0674    |\n",
      "|    value_loss           | 3.91       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.377777777777776\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021338565 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.51        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 3.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.7247311827957\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 231         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020613771 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.253       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0658     |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.433333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 238         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021694724 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 3.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.591919191919192\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 245         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021912452 |\n",
      "|    clip_fraction        | 0.447       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.554       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 3.78        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.735294117647058\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 253         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020704381 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 3.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.504761904761907\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 261         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020125749 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.25        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0682     |\n",
      "|    value_loss           | 3.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.66111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019740533 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.988       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 3.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.713513513513515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021376688 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1           |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0718     |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.63859649122807\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 137        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 283        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01916138 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.695      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.35       |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0677    |\n",
      "|    value_loss           | 3.81       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.44615384615385\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 291         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018644001 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.71        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.191       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 3.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.16\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 299        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01883196 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.726      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.03       |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0675    |\n",
      "|    value_loss           | 3.3        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.6520325203252\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018637322 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.613       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 3.5         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.217460317460315\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017253172 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 3.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.82325581395349\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017205406 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.759       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 3.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.5\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 329         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018367678 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.532       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.026666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 336         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016612973 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.52        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 3.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.30869565217391\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 344         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018375231 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 3.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.63120567375886\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 351         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016577344 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.788       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.93888888888889\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 359        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01738817 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.753      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.13       |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0664    |\n",
      "|    value_loss           | 3.2        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.35918367346939\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 366         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018101837 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.662       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 3.17        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.792\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017351896 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.09        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.17516339869281\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -181      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 136       |\n",
      "|    iterations           | 51        |\n",
      "|    time_elapsed         | 381       |\n",
      "|    total_timesteps      | 52224     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0161051 |\n",
      "|    clip_fraction        | 0.3       |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.25     |\n",
      "|    explained_variance   | 0.769     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.71      |\n",
      "|    n_updates            | 500       |\n",
      "|    policy_gradient_loss | -0.0645   |\n",
      "|    value_loss           | 2.99      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.68974358974359\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -181      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 136       |\n",
      "|    iterations           | 52        |\n",
      "|    time_elapsed         | 389       |\n",
      "|    total_timesteps      | 53248     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0170705 |\n",
      "|    clip_fraction        | 0.329     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.25     |\n",
      "|    explained_variance   | 0.761     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.27      |\n",
      "|    n_updates            | 510       |\n",
      "|    policy_gradient_loss | -0.068    |\n",
      "|    value_loss           | 3.17      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.19622641509434\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 53         |\n",
      "|    time_elapsed         | 397        |\n",
      "|    total_timesteps      | 54272      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01738732 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.759      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.711      |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0666    |\n",
      "|    value_loss           | 3.04       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.714814814814815\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 404         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017751448 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.748       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0699     |\n",
      "|    value_loss           | 3.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.18424242424243\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 412         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017380735 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.61        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.70595238095238\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 419        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01792475 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.777      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.26       |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0697    |\n",
      "|    value_loss           | 2.92       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.1953216374269\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 426         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016226467 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.598       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.65172413793103\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 434         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015479231 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.070056497175145\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 442         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015131976 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.323       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.47\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 449         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017097592 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.78688524590164\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -180      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 136       |\n",
      "|    iterations           | 61        |\n",
      "|    time_elapsed         | 457       |\n",
      "|    total_timesteps      | 62464     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0153699 |\n",
      "|    clip_fraction        | 0.305     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.24     |\n",
      "|    explained_variance   | 0.797     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.907     |\n",
      "|    n_updates            | 600       |\n",
      "|    policy_gradient_loss | -0.0694   |\n",
      "|    value_loss           | 2.72      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.116129032258065\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 465        |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01611582 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.8        |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.242      |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.066     |\n",
      "|    value_loss           | 2.71       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.45079365079365\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 472         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015249561 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.48        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.753125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 480         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015124617 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.08615384615385\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 487         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013183322 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.872       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.382828282828285\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 495         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014000438 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.196       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.667661691542285\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 502         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014670832 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.275       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.93333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 510         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014746607 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.385       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.19903381642512\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 517         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014492391 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.898       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.47714285714286\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 525        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01523748 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.816      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.667      |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.0674    |\n",
      "|    value_loss           | 2.64       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.7868544600939\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 532        |\n",
      "|    total_timesteps      | 72704      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01572347 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.81       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.196      |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0684    |\n",
      "|    value_loss           | 2.66       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.05833333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 540         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015661722 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.641       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.298630136986304\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 548         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015783373 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.459       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.54504504504504\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 555         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017121674 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.148      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.80088888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015359839 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.804       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.121929824561406\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 76         |\n",
      "|    time_elapsed         | 570        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01507759 |\n",
      "|    clip_fraction        | 0.284      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.834      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.837      |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | -0.0663    |\n",
      "|    value_loss           | 2.41       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.43722943722944\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 578         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015803576 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.269       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.71367521367522\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 585         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015541859 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0501      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.00168776371308\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 79         |\n",
      "|    time_elapsed         | 593        |\n",
      "|    total_timesteps      | 80896      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01617521 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.845      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.801      |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0717    |\n",
      "|    value_loss           | 2.2        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.255\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 601         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017731773 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.287       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0739     |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.48559670781893\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 609         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017364994 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.71138211382114\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 616         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018478503 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0737     |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.973493975903615\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 624         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019868318 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.214       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0762     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.23730158730159\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 631         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018379107 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0737      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0766     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.4964705882353\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 639         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016385183 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.452       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0747     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.77286821705427\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -177       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 647        |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01870393 |\n",
      "|    clip_fraction        | 0.408      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.874      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.399      |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.0804    |\n",
      "|    value_loss           | 1.83       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.08199233716475\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 654         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017227469 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0126      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0768     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.38181818181818\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 662         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017835818 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.321       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.810486891385764\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 669         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020534486 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0779     |\n",
      "|    value_loss           | 1.65        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.22666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 677         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017932096 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.224       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0744     |\n",
      "|    value_loss           | 1.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.643956043956045\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 684         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019050462 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.216       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0797     |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.073188405797104\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 692         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018855356 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.231       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0775     |\n",
      "|    value_loss           | 1.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.510394265232975\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 699         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016798861 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.04        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0752     |\n",
      "|    value_loss           | 1.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.93262411347518\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 707        |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01917756 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.15      |\n",
      "|    explained_variance   | 0.887      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.108      |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.08      |\n",
      "|    value_loss           | 1.74       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.320701754385965\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -174      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 136       |\n",
      "|    iterations           | 95        |\n",
      "|    time_elapsed         | 714       |\n",
      "|    total_timesteps      | 97280     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0186564 |\n",
      "|    clip_fraction        | 0.394     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.15     |\n",
      "|    explained_variance   | 0.9       |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.0141    |\n",
      "|    n_updates            | 940       |\n",
      "|    policy_gradient_loss | -0.0797   |\n",
      "|    value_loss           | 1.52      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.672916666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 722         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019894864 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00109    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0779     |\n",
      "|    value_loss           | 1.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.011683848797254\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 729         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020276515 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.194       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0794     |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.3421768707483\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 737         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020221878 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0811      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0807     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.593265993265994\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 744         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021495584 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0831     |\n",
      "|    value_loss           | 1.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.876666666666665\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -173       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 752        |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02104874 |\n",
      "|    clip_fraction        | 0.441      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.13      |\n",
      "|    explained_variance   | 0.899      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.031     |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.0822    |\n",
      "|    value_loss           | 1.52       |\n",
      "----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -45.62\n",
      "Overall Average Successful Assignments: 34.73590547739111\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52edbb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 7.4\n",
      "All assignments history: [5, 8, 13, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -184     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.666666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008541726 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0107     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.66        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 17.9        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.844444444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 148         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011381008 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.00427    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 12.8        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -156.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.566666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 144         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012745911 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0342      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.82        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 9.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.706666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014505038 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0873      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 8.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.677777777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014727057 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0732      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.04        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 7.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.790476190476191\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017513797 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.069       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0657     |\n",
      "|    value_loss           | 6.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.916666666666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 139        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 58         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01810817 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0774     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.01       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0624    |\n",
      "|    value_loss           | 6.1        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.051851851851852\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019379288 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0789      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 5.53        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.586666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019648036 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0995      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.24        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0628     |\n",
      "|    value_loss           | 5.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.315151515151515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020630598 |\n",
      "|    clip_fraction        | 0.444       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.374       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 4.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.805555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020924091 |\n",
      "|    clip_fraction        | 0.464       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.99        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 4.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.794871794871796\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023923714 |\n",
      "|    clip_fraction        | 0.529       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.777       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 4.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -168.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.504761904761907\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023586161 |\n",
      "|    clip_fraction        | 0.515       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.068      |\n",
      "|    value_loss           | 4.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.137777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024261657 |\n",
      "|    clip_fraction        | 0.544       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.875       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 4.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.691666666666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 138        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 118        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02277179 |\n",
      "|    clip_fraction        | 0.504      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.453      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.32       |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0685    |\n",
      "|    value_loss           | 4.05       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.729411764705883\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021666085 |\n",
      "|    clip_fraction        | 0.458       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.129       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0646     |\n",
      "|    value_loss           | 4.1         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.703703703703702\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021483602 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.31        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.72280701754386\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021764606 |\n",
      "|    clip_fraction        | 0.444       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.53        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.41        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0659     |\n",
      "|    value_loss           | 3.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.523333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021124873 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.546       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.575       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 3.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.784126984126985\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019684236 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0629     |\n",
      "|    value_loss           | 4.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.206060606060607\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021303978 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 3.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.620289855072464\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018222207 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.02        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 3.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.51388888888889\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 139        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 176        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01865209 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.628      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.49       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0641    |\n",
      "|    value_loss           | 3.71       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.765333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 183         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018364165 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.654       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.221       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 3.45        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.615384615384617\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018831879 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.42        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 3.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.074074074074073\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016808318 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.687       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 3.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.919047619047618\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 206         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016893994 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.39        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 3.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.83448275862069\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016613424 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0809      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 3.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.73111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017654113 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.405       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 3.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.64516129032258\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017776957 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.943       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 3.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.41875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016065031 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.12727272727273\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016521538 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0629     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.86078431372549\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017728876 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0659     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.49142857142857\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017656459 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.083333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017440688 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.315       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.87207207207207\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017871471 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00658     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.5859649122807\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 284         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018462077 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.418       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0677     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.31111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 292         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017246671 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.848       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.13666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 300         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017317627 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.393       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.99674796747968\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 308         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017520843 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.109      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.665079365079364\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016096734 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.695       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.4031007751938\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 324         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015540108 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.694       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.25151515151515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 332         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016790587 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.669       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.12888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 340         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015502183 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.478       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.01449275362319\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 348         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015570665 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.479       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.8354609929078\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017960902 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.671       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.459722222222226\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 364         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017455745 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.589       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 1.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.06122448979592\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 373         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015126528 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.246       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.48133333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 381         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015510194 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.38        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.90196078431372\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 389         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016708508 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0808      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.30897435897436\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 396         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018033242 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 1.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.74339622641509\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 405         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016542912 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.197       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0725     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.17283950617284\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 412         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017893853 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00449    |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.075      |\n",
      "|    value_loss           | 1.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.5769696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 420         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017079648 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.226       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.073      |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.05\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 428         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017254481 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.432       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0757     |\n",
      "|    value_loss           | 1.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.51695906432749\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016306994 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.158       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.94137931034483\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 442         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017319627 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.188       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0749     |\n",
      "|    value_loss           | 1.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.41016949152542\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 449         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019027725 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0441      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0771     |\n",
      "|    value_loss           | 1.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.88\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 457         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016196597 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.33224043715847\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 464         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017575966 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0458      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.806451612903224\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 473        |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01798216 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.905      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.188      |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.0749    |\n",
      "|    value_loss           | 1.51       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.28994708994709\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 480         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017953385 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.112      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0752     |\n",
      "|    value_loss           | 1.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.764583333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 488         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015953682 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0233      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0733     |\n",
      "|    value_loss           | 1.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.246153846153845\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -180      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 133       |\n",
      "|    iterations           | 65        |\n",
      "|    time_elapsed         | 497       |\n",
      "|    total_timesteps      | 66560     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0161873 |\n",
      "|    clip_fraction        | 0.331     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.22     |\n",
      "|    explained_variance   | 0.909     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.0342    |\n",
      "|    n_updates            | 640       |\n",
      "|    policy_gradient_loss | -0.0751   |\n",
      "|    value_loss           | 1.46      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.73535353535353\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017077869 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0297      |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0764     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.20796019900497\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 512         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018023845 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00902    |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0787     |\n",
      "|    value_loss           | 1.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.68235294117647\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 520         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017366946 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.117      |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0741     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.153623188405795\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 528         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018667208 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.259      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0783     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.62952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 536         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017333385 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0358      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.076      |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.07699530516432\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 133        |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 544        |\n",
      "|    total_timesteps      | 72704      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01715117 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.903      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.215      |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.078     |\n",
      "|    value_loss           | 1.6        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.483333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 552         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018470198 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0983      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0798     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.83744292237443\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 561         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017219279 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.29        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.1981981981982\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 569         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017706364 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.158       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0787     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.58311111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 577         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017979428 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.287       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0769     |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.97719298245614\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 586         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016249688 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0765     |\n",
      "|    value_loss           | 1.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.356709956709956\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 594         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016654804 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0767     |\n",
      "|    value_loss           | 1.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.70940170940171\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 602         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016111031 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.383       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0754     |\n",
      "|    value_loss           | 1.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.07679324894515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 609         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016561108 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.133       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0764     |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.454166666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 618         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016706027 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.067       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0786     |\n",
      "|    value_loss           | 1.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.80493827160494\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 626         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016304173 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.202       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0797     |\n",
      "|    value_loss           | 1.59        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.1219512195122\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 634        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01673846 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.00128    |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.0784    |\n",
      "|    value_loss           | 1.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.442570281124496\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 642         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018472286 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.079      |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.756349206349206\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 84         |\n",
      "|    time_elapsed         | 650        |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01574254 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.912      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0411     |\n",
      "|    n_updates            | 830        |\n",
      "|    policy_gradient_loss | -0.0749    |\n",
      "|    value_loss           | 1.58       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.08\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 658         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016535915 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0303     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.078      |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.38449612403101\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 666         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015799778 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.25        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0757     |\n",
      "|    value_loss           | 1.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.682758620689654\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -174       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 674        |\n",
      "|    total_timesteps      | 89088      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01751092 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.15      |\n",
      "|    explained_variance   | 0.916      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0948    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0785    |\n",
      "|    value_loss           | 1.47       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.98257575757576\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 682         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017785976 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.132      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0804     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.24644194756554\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 690         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019969627 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.114       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0828     |\n",
      "|    value_loss           | 1.62        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.50592592592593\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -173       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 131        |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 698        |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01719652 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.13      |\n",
      "|    explained_variance   | 0.924      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.192     |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.0765    |\n",
      "|    value_loss           | 1.18       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.75604395604395\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 707         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018054944 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.156      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0802     |\n",
      "|    value_loss           | 1.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.00289855072464\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016877936 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0802     |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0789     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.25878136200717\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 723         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017714078 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.155       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0811     |\n",
      "|    value_loss           | 1.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.51418439716312\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 731         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015220237 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.167       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0763     |\n",
      "|    value_loss           | 1.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.75087719298246\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 739         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015335129 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0762     |\n",
      "|    value_loss           | 1.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.979166666666664\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -170       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 131        |\n",
      "|    iterations           | 96         |\n",
      "|    time_elapsed         | 747        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01583998 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.08      |\n",
      "|    explained_variance   | 0.927      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0635    |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.077     |\n",
      "|    value_loss           | 1.29       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.189690721649484\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 755         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017011134 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.06       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.164       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0779     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.410204081632656\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 763         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016970018 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0784     |\n",
      "|    value_loss           | 1.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.63636363636363\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -169       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 131        |\n",
      "|    iterations           | 99         |\n",
      "|    time_elapsed         | 771        |\n",
      "|    total_timesteps      | 101376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01724133 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.05      |\n",
      "|    explained_variance   | 0.924      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0814    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0801    |\n",
      "|    value_loss           | 1.28       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.852\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 779         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015506713 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.03       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.282       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0763     |\n",
      "|    value_loss           | 1.65        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -25.44\n",
      "Overall Average Successful Assignments: 40.174932372019065\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff74faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 7.733333333333333\n",
      "All assignments history: [9, 8, 6, 13, 10, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -122.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.066666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008524009 |\n",
      "|    clip_fraction        | 0.0557      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.231      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.83        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 16.5        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011879022 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.126      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0591     |\n",
      "|    value_loss           | 12.6        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.1\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -183         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135473125 |\n",
      "|    clip_fraction        | 0.238        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | -0.0595      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.48         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0606      |\n",
      "|    value_loss           | 9.5          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.573333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014420319 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.00746    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.62        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 8.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.988888888888887\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015299769 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.038       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.38        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0606     |\n",
      "|    value_loss           | 7.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.514285714285716\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016351841 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0376      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 6.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -184.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.916666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017833948 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0417      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 6.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.77777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018437546 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0606      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 5.77        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.673333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020035664 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0772      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.54        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 5.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.024242424242424\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020975811 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.22        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0628     |\n",
      "|    value_loss           | 4.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.283333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023026004 |\n",
      "|    clip_fraction        | 0.52        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.627       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 4.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.369230769230768\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023781281 |\n",
      "|    clip_fraction        | 0.537       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.256       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 4.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.947619047619046\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024761098 |\n",
      "|    clip_fraction        | 0.548       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 4.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.662222222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026880262 |\n",
      "|    clip_fraction        | 0.567       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.625       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 4.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.754166666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 127         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024460658 |\n",
      "|    clip_fraction        | 0.525       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.288       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.729411764705883\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 135        |\n",
      "|    total_timesteps      | 17408      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02463578 |\n",
      "|    clip_fraction        | 0.533      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.491      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.916      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0686    |\n",
      "|    value_loss           | 3.86       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.5\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 144        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02348449 |\n",
      "|    clip_fraction        | 0.503      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.517      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.5        |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.07      |\n",
      "|    value_loss           | 4.09       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.90877192982456\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 152         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023960873 |\n",
      "|    clip_fraction        | 0.515       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 3.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.26\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024543528 |\n",
      "|    clip_fraction        | 0.518       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.562       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.697       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.21904761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025275098 |\n",
      "|    clip_fraction        | 0.526       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.603       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.24        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 3.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.96969696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022409426 |\n",
      "|    clip_fraction        | 0.455       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.81        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0682     |\n",
      "|    value_loss           | 4.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.41159420289855\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023378978 |\n",
      "|    clip_fraction        | 0.46        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 3.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.92777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 193         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022638045 |\n",
      "|    clip_fraction        | 0.471       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.663       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.01        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 3.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.69866666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 201         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022684071 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.978       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0725     |\n",
      "|    value_loss           | 3.83        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.5\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 209         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023345754 |\n",
      "|    clip_fraction        | 0.477       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.136       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0722     |\n",
      "|    value_loss           | 3.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.08641975308642\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 217         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022733673 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.92        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 3.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.15238095238095\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 226         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023474546 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.347       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 3.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.14022988505747\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 234         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021918517 |\n",
      "|    clip_fraction        | 0.451       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.719       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0737     |\n",
      "|    value_loss           | 3.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.89111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023358926 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.28        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0718     |\n",
      "|    value_loss           | 3.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.89032258064516\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 250        |\n",
      "|    total_timesteps      | 31744      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02025018 |\n",
      "|    clip_fraction        | 0.414      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.716      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.499      |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0702    |\n",
      "|    value_loss           | 3.57       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.97291666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 258         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020762201 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.967       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 3.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.17979797979798\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 266         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019778013 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 3.35        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.35098039215686\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 275         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020295529 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.16        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 3.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.57333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 283         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019018231 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 3.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.79259259259259\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 291         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019622304 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.448       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 3.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.84144144144144\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 299         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017992774 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.03508771929825\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 307         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019535616 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.29401709401709\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018004566 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.769       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.444       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.62833333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 323         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020488553 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.08        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0706     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.0\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 331         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020340452 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.246       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.455555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019746177 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.223       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.948837209302326\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 347         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018243901 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.442424242424245\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 355         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018705191 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0714     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.877037037037034\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 362         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018887253 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.83        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.2695652173913\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 370         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018413953 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.523       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.64964539007092\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 379         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019235011 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.708       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0714     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.92777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 387         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019490935 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.321       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.13061224489796\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 395         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019254517 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.37        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.20933333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 403        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01919023 |\n",
      "|    clip_fraction        | 0.39       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.834      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.604      |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.0701    |\n",
      "|    value_loss           | 2.32       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.28888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 411         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018066721 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0614     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.42564102564103\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 420         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017689414 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.987       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0682     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.66415094339623\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 428         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017430592 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.347       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.85308641975308\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 437        |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01741201 |\n",
      "|    clip_fraction        | 0.33       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.852      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.156     |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.0656    |\n",
      "|    value_loss           | 2.12       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.11151515151515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 445         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018196642 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.249       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.32142857142857\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018029941 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.612       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.47017543859649\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017652504 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.411       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.54367816091954\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020223431 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.678       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.722033898305085\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 479         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019572675 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.97222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 486         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015364545 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.808       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.19234972677596\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 494         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016381012 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.248       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.43333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 502         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016536776 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.957       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.67089947089947\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 510         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017334383 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.147       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.87083333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 519         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017449498 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.463       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.114871794871796\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 527         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016956834 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.213       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.068      |\n",
      "|    value_loss           | 1.78        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.352525252525254\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 536         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014968449 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.216       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.559203980099504\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 543         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015236473 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.179       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0677     |\n",
      "|    value_loss           | 1.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.69705882352941\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 68         |\n",
      "|    time_elapsed         | 551        |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01458046 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.88       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0752    |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | -0.0674    |\n",
      "|    value_loss           | 1.84       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.792270531400966\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 558         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015296757 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.313       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.843809523809526\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 565         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015923958 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.284       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.89201877934272\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 572         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015076795 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.143       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.012962962962966\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 580         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015057256 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00112     |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.20091324200913\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 587         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015216831 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0823     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.3\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 594        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01665857 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.298      |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0691    |\n",
      "|    value_loss           | 1.78       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.34755555555556\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 602        |\n",
      "|    total_timesteps      | 76800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01724324 |\n",
      "|    clip_fraction        | 0.341      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.885      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.226      |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0732    |\n",
      "|    value_loss           | 1.76       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.44824561403509\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 609         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017869342 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.131       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0759     |\n",
      "|    value_loss           | 1.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.62510822510823\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 616         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019283693 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.278       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0751     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.75982905982906\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 623        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01800716 |\n",
      "|    clip_fraction        | 0.349      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.369      |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.074     |\n",
      "|    value_loss           | 1.81       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.856540084388186\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 631         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017550526 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0261      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0767     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.895833333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 638         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018278986 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.156       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0774     |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.02880658436214\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -177       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 645        |\n",
      "|    total_timesteps      | 82944      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01762855 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.897      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.555      |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.072     |\n",
      "|    value_loss           | 1.57       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.170731707317074\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 652         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018666634 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.235       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0778     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.267469879518075\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 660         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019447427 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0435     |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0741     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.41111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 666         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019548124 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0565      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0782     |\n",
      "|    value_loss           | 1.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.625882352941176\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 674         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017667169 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0134      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0776     |\n",
      "|    value_loss           | 1.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.85581395348837\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 682         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018092962 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.135      |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0748     |\n",
      "|    value_loss           | 1.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.09655172413793\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 129        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 689        |\n",
      "|    total_timesteps      | 89088      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01881089 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.902      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.298     |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0768    |\n",
      "|    value_loss           | 1.56       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.3280303030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 696         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018856352 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0507      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0785     |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.53932584269663\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 703         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019569123 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.103      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0755     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.78074074074074\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 711         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018676108 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0517      |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0762     |\n",
      "|    value_loss           | 1.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.000732600732604\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 718         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019789811 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.139      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0772     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.15652173913043\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 725         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020481225 |\n",
      "|    clip_fraction        | 0.441       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.52        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0825     |\n",
      "|    value_loss           | 1.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.293189964157705\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 732         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018452136 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.314       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0784     |\n",
      "|    value_loss           | 1.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.44609929078014\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 740         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018635297 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.169       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0772     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.58385964912281\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 130        |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 747        |\n",
      "|    total_timesteps      | 97280      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01774336 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0772    |\n",
      "|    value_loss           | 1.57       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.794444444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 754         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019917611 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0249     |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0822     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.049484536082474\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 761         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018003581 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.107      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0772     |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.33061224489796\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 768         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019594016 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0655      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0814     |\n",
      "|    value_loss           | 1.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.65319865319865\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 776         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019338477 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.199       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0779     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.96066666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 784         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019378511 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.308       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0798     |\n",
      "|    value_loss           | 1.54        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -39.58\n",
      "Overall Average Successful Assignments: 43.897954106206264\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dada3cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -190.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 6.0\n",
      "All assignments history: [9, 8, 8, 8, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -184     |\n",
      "| time/              |          |\n",
      "|    fps             | 165      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.0\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009393228 |\n",
      "|    clip_fraction        | 0.071       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.39       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 15.6        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -184.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 9.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013175437 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0488     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.33        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 11.1        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -198.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 8.3\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 131        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 31         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01447655 |\n",
      "|    clip_fraction        | 0.272      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | 0.098      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.98       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0627    |\n",
      "|    value_loss           | 9.79       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -190.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 7.88\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015038603 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0626      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.02        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0615     |\n",
      "|    value_loss           | 8.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.088888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016614815 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0771      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 7.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -198.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 9.095238095238095\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017012719 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0702      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 7.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -200.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 8.308333333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 64         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01787357 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0824     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.19       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0585    |\n",
      "|    value_loss           | 6.25       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 8.037037037037036\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018789262 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0884      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0629     |\n",
      "|    value_loss           | 5.54        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.74\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019930582 |\n",
      "|    clip_fraction        | 0.448       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 4.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.527272727272727\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021944597 |\n",
      "|    clip_fraction        | 0.477       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.438       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 4.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.68888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022458244 |\n",
      "|    clip_fraction        | 0.496       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0863      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 4.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.03076923076923\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023883123 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0404     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 4.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.771428571428572\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023427945 |\n",
      "|    clip_fraction        | 0.521       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.796       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 4.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.986666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022745293 |\n",
      "|    clip_fraction        | 0.496       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 3.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.87083333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021461608 |\n",
      "|    clip_fraction        | 0.461       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.529       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.333333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022233438 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.558       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1           |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0672     |\n",
      "|    value_loss           | 3.66        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.58888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023365173 |\n",
      "|    clip_fraction        | 0.491       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.604       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.83        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 3.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.61052631578947\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022644445 |\n",
      "|    clip_fraction        | 0.473       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0546      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 3.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.35333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025392823 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.825       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0716     |\n",
      "|    value_loss           | 3.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.974603174603175\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024370333 |\n",
      "|    clip_fraction        | 0.519       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.871       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 3.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.28484848484848\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023214158 |\n",
      "|    clip_fraction        | 0.489       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.907       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0716     |\n",
      "|    value_loss           | 3.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.45507246376812\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 188         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022073705 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.139       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 3.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.42777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022504956 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.615       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 3.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.45333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023419227 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.72        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 3.4         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.276923076923076\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024724415 |\n",
      "|    clip_fraction        | 0.509       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.832       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0733     |\n",
      "|    value_loss           | 3.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.05432098765432\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 222         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023010913 |\n",
      "|    clip_fraction        | 0.482       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 3.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.96190476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 229         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021195274 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.299       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.63448275862069\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 238         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021229189 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.806       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.135555555555555\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 124        |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 246        |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02003539 |\n",
      "|    clip_fraction        | 0.404      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.769      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.747      |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.0669    |\n",
      "|    value_loss           | 3          |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.27741935483871\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 124        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 254        |\n",
      "|    total_timesteps      | 31744      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02191459 |\n",
      "|    clip_fraction        | 0.445      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.76       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.652      |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0721    |\n",
      "|    value_loss           | 3.08       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.354166666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 262         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019324388 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0291     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0672     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.51111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 270         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020865194 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.85        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.62352941176471\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 278         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022702549 |\n",
      "|    clip_fraction        | 0.46        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.551       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.073      |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.58095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018997762 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.694       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.61296296296296\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019185465 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.881       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.538738738738736\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 303         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020224296 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.60526315789474\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 311         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021234263 |\n",
      "|    clip_fraction        | 0.433       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0749     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.71623931623932\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 319         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021138111 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.252       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.93333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 327         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019496147 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.265040650406505\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 334        |\n",
      "|    total_timesteps      | 41984      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01903893 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.827      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.954      |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0675    |\n",
      "|    value_loss           | 2.45       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.58412698412698\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017627269 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.542       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.74883720930232\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 351         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017626438 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.92272727272727\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 358        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02157876 |\n",
      "|    clip_fraction        | 0.411      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.852      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.551      |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.071     |\n",
      "|    value_loss           | 2.24       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.964444444444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 366         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020800527 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.22        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.073      |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.03913043478261\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018924613 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.332       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.224113475177305\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 382         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021564329 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.557       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.34444444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019347932 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.427       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.481632653061226\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018196233 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.221       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.588\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 406        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01972908 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.862      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.195      |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.0699    |\n",
      "|    value_loss           | 2.11       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.69150326797386\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 414         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021659467 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.571       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.741025641025644\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 422         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017354982 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0178     |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.838993710691824\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 429         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022297844 |\n",
      "|    clip_fraction        | 0.456       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.411       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.99753086419753\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 438         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021360245 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0175     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.09090909090909\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 446         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020098083 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.76        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0706     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.266666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 454         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020959124 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.217       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.471345029239764\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 462         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019071797 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.443       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 2           |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.70919540229885\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 470         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018904686 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.24        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.90621468926554\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 478         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018905694 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.732       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0706     |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.16\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 485         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019372609 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.36939890710382\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 493         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021516278 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0203      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 2           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.5989247311828\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 501         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017369408 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.148      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.85820105820106\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 509         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018424604 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.128125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 517         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020006083 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.344       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 1.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.33846153846154\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 525         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019783054 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.456       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0718     |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.537373737373734\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 533         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018341595 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0865      |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 1.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.78009950248756\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 541         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017823013 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.763       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.963725490196076\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 549         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018174542 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.538       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 1.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.19033816425121\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 558        |\n",
      "|    total_timesteps      | 70656      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01852081 |\n",
      "|    clip_fraction        | 0.364      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.895      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.617      |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0711    |\n",
      "|    value_loss           | 1.81       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.383809523809525\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 565         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017297834 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.65        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.57464788732394\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 573         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018861573 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.429       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 1.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.763888888888886\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 581         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016323315 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.652       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.990867579908674\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 589         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021051615 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0752     |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.24324324324324\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 597        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02044028 |\n",
      "|    clip_fraction        | 0.413      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.231      |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0734    |\n",
      "|    value_loss           | 1.98       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.461333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 605         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019127514 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.461       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0729     |\n",
      "|    value_loss           | 1.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.69561403508772\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 612         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023311784 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0737      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0756     |\n",
      "|    value_loss           | 1.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.954112554112555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 620         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023710929 |\n",
      "|    clip_fraction        | 0.477       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0229      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 1.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.207692307692305\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 629        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01843077 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.886      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.206     |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.0714    |\n",
      "|    value_loss           | 1.81       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.41603375527426\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 636         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021022655 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.149      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 1.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.61666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 645         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018030733 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.334       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 1.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.821399176954735\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 653        |\n",
      "|    total_timesteps      | 82944      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01851926 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.897      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.21       |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0713    |\n",
      "|    value_loss           | 1.64       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.01869918699187\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 661         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018772345 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.114       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0725     |\n",
      "|    value_loss           | 1.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.205622489959836\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 668         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021252474 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.172       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0729     |\n",
      "|    value_loss           | 1.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.3968253968254\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 677         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020479182 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.441       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 1.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.63137254901961\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 685         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022777185 |\n",
      "|    clip_fraction        | 0.458       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0611      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0757     |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.831782945736435\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 692         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018604746 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00713     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0736     |\n",
      "|    value_loss           | 1.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.030651340996165\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 700         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019042687 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.23        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0737     |\n",
      "|    value_loss           | 1.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.23030303030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 708         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016597189 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.169      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 1.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.429962546816476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016336143 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.413       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0679     |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.644444444444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 723         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019085556 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.336       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0718     |\n",
      "|    value_loss           | 1.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.82710622710623\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 732         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021576995 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.339       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0744     |\n",
      "|    value_loss           | 1.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.004347826086956\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 92         |\n",
      "|    time_elapsed         | 739        |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01957697 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.18      |\n",
      "|    explained_variance   | 0.902      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.795      |\n",
      "|    n_updates            | 910        |\n",
      "|    policy_gradient_loss | -0.0741    |\n",
      "|    value_loss           | 1.64       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.174193548387095\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 747        |\n",
      "|    total_timesteps      | 95232      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02263315 |\n",
      "|    clip_fraction        | 0.462      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.18      |\n",
      "|    explained_variance   | 0.892      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0386     |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0771    |\n",
      "|    value_loss           | 1.65       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.361702127659576\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 755         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019994766 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0901      |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0755     |\n",
      "|    value_loss           | 1.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.54105263157895\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 763         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018372077 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.313       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0756     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.72291666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 771         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018538913 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.219       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.075      |\n",
      "|    value_loss           | 1.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.90859106529209\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 779         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018837204 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.292      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 1.49        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.03809523809524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 786         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020152047 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.055      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0737     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.16632996632997\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 99         |\n",
      "|    time_elapsed         | 794        |\n",
      "|    total_timesteps      | 101376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01960918 |\n",
      "|    clip_fraction        | 0.4        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.903      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.252      |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0742    |\n",
      "|    value_loss           | 1.55       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.314\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 803         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022106227 |\n",
      "|    clip_fraction        | 0.452       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0643     |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0781     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -20.16\n",
      "Overall Average Successful Assignments: 48.43965149027618\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b0656b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.066666666666666\n",
      "All assignments history: [14, 8, 10, 8, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -180     |\n",
      "| time/              |          |\n",
      "|    fps             | 590      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.666666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008878497 |\n",
      "|    clip_fraction        | 0.06        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.147      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.71        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 16.4        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.22222222222222\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -184         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 451          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117793325 |\n",
      "|    clip_fraction        | 0.177        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | -0.0787      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.57         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0588      |\n",
      "|    value_loss           | 12.6         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -166.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.116666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 443         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012123372 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0396     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.99        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 10.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.946666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 431         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013714455 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0172      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.69        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 9.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.822222222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 426         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015187123 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0288      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.12        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 8.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -184.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.571428571428571\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 424        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01656055 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0554     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.47       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0608    |\n",
      "|    value_loss           | 7.13       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -198.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.091666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 421         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017738227 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0683      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.18        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 6.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.45925925925926\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 421        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01873235 |\n",
      "|    clip_fraction        | 0.413      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0829     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.36       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0625    |\n",
      "|    value_loss           | 5.74       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.04\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 420         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018513288 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.28        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 5.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.927272727272726\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 417         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021469843 |\n",
      "|    clip_fraction        | 0.492       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.065      |\n",
      "|    value_loss           | 4.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.43888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 417         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022221498 |\n",
      "|    clip_fraction        | 0.502       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.73        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 4.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -152.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.615384615384617\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 416         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023238659 |\n",
      "|    clip_fraction        | 0.515       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.154       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 4.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.133333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 414         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026477152 |\n",
      "|    clip_fraction        | 0.572       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.753       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 4.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.342222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 413         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024638021 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 4.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.241666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 413         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025286458 |\n",
      "|    clip_fraction        | 0.564       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 4.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.498039215686273\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 413        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 42         |\n",
      "|    total_timesteps      | 17408      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02383112 |\n",
      "|    clip_fraction        | 0.514      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.41       |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0664    |\n",
      "|    value_loss           | 3.95       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.92962962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 411         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023798667 |\n",
      "|    clip_fraction        | 0.519       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0682     |\n",
      "|    value_loss           | 4.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.62456140350877\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -183      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 410       |\n",
      "|    iterations           | 19        |\n",
      "|    time_elapsed         | 47        |\n",
      "|    total_timesteps      | 19456     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0221731 |\n",
      "|    clip_fraction        | 0.477     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.26     |\n",
      "|    explained_variance   | 0.53      |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 2.06      |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | -0.0668   |\n",
      "|    value_loss           | 4.01      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.903333333333336\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -183      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 409       |\n",
      "|    iterations           | 20        |\n",
      "|    time_elapsed         | 49        |\n",
      "|    total_timesteps      | 20480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0233475 |\n",
      "|    clip_fraction        | 0.496     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.26     |\n",
      "|    explained_variance   | 0.561     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.41      |\n",
      "|    n_updates            | 190       |\n",
      "|    policy_gradient_loss | -0.0677   |\n",
      "|    value_loss           | 3.83      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.63174603174603\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 409         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021931577 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.562       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.382       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 4.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.487878787878785\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 409         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022697717 |\n",
      "|    clip_fraction        | 0.487       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.28        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 3.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.53913043478261\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 409         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022466116 |\n",
      "|    clip_fraction        | 0.497       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.59        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 3.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.44722222222222\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 409        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 59         |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02114897 |\n",
      "|    clip_fraction        | 0.44       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.642      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.99       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0671    |\n",
      "|    value_loss           | 3.68       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.458666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 409         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022875518 |\n",
      "|    clip_fraction        | 0.49        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0699     |\n",
      "|    value_loss           | 3.58        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.55384615384615\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 409         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021033436 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.492       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0679     |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.4641975308642\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -182      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 408       |\n",
      "|    iterations           | 27        |\n",
      "|    time_elapsed         | 67        |\n",
      "|    total_timesteps      | 27648     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0218348 |\n",
      "|    clip_fraction        | 0.476     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.26     |\n",
      "|    explained_variance   | 0.69      |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.57      |\n",
      "|    n_updates            | 260       |\n",
      "|    policy_gradient_loss | -0.0713   |\n",
      "|    value_loss           | 3.44      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.15952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022308178 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0714     |\n",
      "|    value_loss           | 3.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.81609195402299\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 408        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 72         |\n",
      "|    total_timesteps      | 29696      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02236196 |\n",
      "|    clip_fraction        | 0.468      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.71       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.36       |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0699    |\n",
      "|    value_loss           | 3.26       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.388888888888886\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022735761 |\n",
      "|    clip_fraction        | 0.464       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 3.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.06021505376344\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021387225 |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 3.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.52708333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019473419 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.985       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.07       |\n",
      "|    value_loss           | 3.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.991919191919195\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019583218 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.836       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 3.08        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.39607843137255\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019495921 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.407       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.01714285714286\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020246185 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0703     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.49814814814815\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021258157 |\n",
      "|    clip_fraction        | 0.453       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.611       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.016216216216215\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 409         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020571517 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.217       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.073      |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.3140350877193\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 409         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022364777 |\n",
      "|    clip_fraction        | 0.444       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.371       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.753846153846155\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 409         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019637099 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.11666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 409         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019625867 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0944      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.66178861788618\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 409        |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 102        |\n",
      "|    total_timesteps      | 41984      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02135562 |\n",
      "|    clip_fraction        | 0.432      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.808      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.292      |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0717    |\n",
      "|    value_loss           | 2.53       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.352380952380955\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021200445 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.294       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.06821705426356\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017655432 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.558       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.695454545454545\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018404134 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.726       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.284444444444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018780842 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00273    |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.83768115942029\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017198162 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.255       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.46241134751773\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 407         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019067872 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.338       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0703     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.06805555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 406         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019006452 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0673     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.68299319727891\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 405         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019171592 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.291       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.228\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 404         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020357605 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.213       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0739     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.83398692810457\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 404         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018396301 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.293       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.30897435897436\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 402         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018822195 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.504       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.716981132075475\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 402         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019396175 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.281       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0736     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.046913580246915\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 401         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017265338 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.011       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.481212121212124\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -180      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 400       |\n",
      "|    iterations           | 55        |\n",
      "|    time_elapsed         | 140       |\n",
      "|    total_timesteps      | 56320     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0196387 |\n",
      "|    clip_fraction        | 0.383     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.24     |\n",
      "|    explained_variance   | 0.866     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.592     |\n",
      "|    n_updates            | 540       |\n",
      "|    policy_gradient_loss | -0.0729   |\n",
      "|    value_loss           | 1.84      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.85\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 400         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019030947 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.565       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.19415204678363\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 399         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016056444 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.718       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 2           |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.641379310344824\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 398         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016375605 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00775    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 1.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.09152542372881\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 398         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017747449 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0119     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0742     |\n",
      "|    value_loss           | 1.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.532222222222224\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 154        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01572527 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.881      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.267      |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.069     |\n",
      "|    value_loss           | 1.83       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.93224043715847\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 157        |\n",
      "|    total_timesteps      | 62464      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01692008 |\n",
      "|    clip_fraction        | 0.335      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.881      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0395     |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0719    |\n",
      "|    value_loss           | 1.81       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.33440860215054\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 397         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018005699 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.227       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 1.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.74391534391535\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018647013 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 1.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.159375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018177893 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0767     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 1.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.54871794871795\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017089609 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.168       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 1.86        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.9040404040404\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 394         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015241884 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0124      |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.25771144278607\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 394         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 173         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015476901 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.165      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 1.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.59901960784314\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 394         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 176         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018718757 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.361       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0751     |\n",
      "|    value_loss           | 1.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.91400966183575\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 179         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017107807 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.175       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0741     |\n",
      "|    value_loss           | 1.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.21523809523809\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 182         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015607217 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0729     |\n",
      "|    value_loss           | 1.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.50798122065728\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 185        |\n",
      "|    total_timesteps      | 72704      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02097249 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.907      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0173     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0781    |\n",
      "|    value_loss           | 1.4        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.763888888888886\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015705898 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0267      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 1.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.04109589041096\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 391         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018692201 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0294      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0763     |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.26126126126127\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 391         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 193         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016410183 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.359       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0748     |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.45244444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017531984 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.322       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.078      |\n",
      "|    value_loss           | 1.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.66929824561403\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 199         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017517798 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0732     |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0763     |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.86320346320346\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 202         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019441213 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0762     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0798     |\n",
      "|    value_loss           | 1.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.11880341880342\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 204         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020169329 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0346     |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0768     |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.35189873417721\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -177       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 79         |\n",
      "|    time_elapsed         | 207        |\n",
      "|    total_timesteps      | 80896      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01845163 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.18      |\n",
      "|    explained_variance   | 0.917      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0128     |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0785    |\n",
      "|    value_loss           | 1.35       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.58416666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 210         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018016398 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.297       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.077      |\n",
      "|    value_loss           | 1.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.81399176954733\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017819447 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.183       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0783     |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.04634146341463\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 216         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019745536 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.088      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0823     |\n",
      "|    value_loss           | 1.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.29317269076306\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018980391 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0831     |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.54523809523809\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 84         |\n",
      "|    time_elapsed         | 221        |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01915833 |\n",
      "|    clip_fraction        | 0.389      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.15      |\n",
      "|    explained_variance   | 0.914      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0945     |\n",
      "|    n_updates            | 830        |\n",
      "|    policy_gradient_loss | -0.0805    |\n",
      "|    value_loss           | 1.37       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.80235294117647\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 85         |\n",
      "|    time_elapsed         | 224        |\n",
      "|    total_timesteps      | 87040      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01657571 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.14      |\n",
      "|    explained_variance   | 0.917      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.00957    |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0758    |\n",
      "|    value_loss           | 1.34       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.04961240310078\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019495007 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00335    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0805     |\n",
      "|    value_loss           | 1.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.28888888888889\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -174       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 230        |\n",
      "|    total_timesteps      | 89088      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01903437 |\n",
      "|    clip_fraction        | 0.418      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.12      |\n",
      "|    explained_variance   | 0.926      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.237     |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0837    |\n",
      "|    value_loss           | 1.17       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.525\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -174      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 386       |\n",
      "|    iterations           | 88        |\n",
      "|    time_elapsed         | 233       |\n",
      "|    total_timesteps      | 90112     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0181599 |\n",
      "|    clip_fraction        | 0.379     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.11     |\n",
      "|    explained_variance   | 0.917     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | -0.014    |\n",
      "|    n_updates            | 870       |\n",
      "|    policy_gradient_loss | -0.0799   |\n",
      "|    value_loss           | 1.32      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.78052434456929\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019562405 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0881     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0814     |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.01629629629629\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 238         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018953372 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0212      |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0848     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.24981684981685\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018935334 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0824      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.084      |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.49347826086957\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -171       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 92         |\n",
      "|    time_elapsed         | 244        |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01882533 |\n",
      "|    clip_fraction        | 0.397      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.06      |\n",
      "|    explained_variance   | 0.912      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0917    |\n",
      "|    n_updates            | 910        |\n",
      "|    policy_gradient_loss | -0.0834    |\n",
      "|    value_loss           | 1.28       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.73333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -170       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 247        |\n",
      "|    total_timesteps      | 95232      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01780168 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.04      |\n",
      "|    explained_variance   | 0.9        |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0429     |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.079     |\n",
      "|    value_loss           | 1.58       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.95106382978723\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 250         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018210698 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0635      |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0822     |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.16\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018104333 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.99       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.024       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0851     |\n",
      "|    value_loss           | 1.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.38472222222222\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -168       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 96         |\n",
      "|    time_elapsed         | 255        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01903376 |\n",
      "|    clip_fraction        | 0.416      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.97      |\n",
      "|    explained_variance   | 0.918      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0267    |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.0843    |\n",
      "|    value_loss           | 1.11       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.60412371134021\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 258         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017034253 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0803     |\n",
      "|    value_loss           | 1.43        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.84965986394558\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 261         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020444762 |\n",
      "|    clip_fraction        | 0.442       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.91       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.143      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0872     |\n",
      "|    value_loss           | 1.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.05656565656565\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 263         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016854867 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.87       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0908      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0821     |\n",
      "|    value_loss           | 1.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.28666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 266         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018247548 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.83       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.024      |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0855     |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -0.68\n",
      "Overall Average Successful Assignments: 50.254886833282114\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4efcfd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 8.466666666666667\n",
      "All assignments history: [8, 9, 9, 3, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -185     |\n",
      "| time/              |          |\n",
      "|    fps             | 524      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -134.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.5\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 433         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009080526 |\n",
      "|    clip_fraction        | 0.0736      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0991     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.81        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 16.9        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -184.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.533333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012229961 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0121     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.616666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013676528 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.00244     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0598     |\n",
      "|    value_loss           | 9.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -162.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.293333333333333\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 379          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 5120         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0141828805 |\n",
      "|    clip_fraction        | 0.243        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | 0.0868       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.44         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0562      |\n",
      "|    value_loss           | 9.18         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.355555555555554\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 375         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015118014 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0904      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.2         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0577     |\n",
      "|    value_loss           | 7.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.866666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -186       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 19         |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01631908 |\n",
      "|    clip_fraction        | 0.349      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0859     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.33       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0597    |\n",
      "|    value_loss           | 6.79       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.833333333333332\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -186       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 22         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01770918 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0931     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.61       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0612    |\n",
      "|    value_loss           | 6.07       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.266666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 370         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019221619 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 5.69        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.153333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019856341 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.27        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0607     |\n",
      "|    value_loss           | 4.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.38181818181818\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021896116 |\n",
      "|    clip_fraction        | 0.483       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.89        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 4.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.40555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 370         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022463432 |\n",
      "|    clip_fraction        | 0.497       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 4.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.93846153846154\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 35         |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02407866 |\n",
      "|    clip_fraction        | 0.527      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.298      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.57       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0664    |\n",
      "|    value_loss           | 4.3        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.82380952380952\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02498608 |\n",
      "|    clip_fraction        | 0.554      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.368      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.77       |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0684    |\n",
      "|    value_loss           | 4.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.11555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024541426 |\n",
      "|    clip_fraction        | 0.543       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.398       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 3.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.94166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025275378 |\n",
      "|    clip_fraction        | 0.558       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.501       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.412       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0714     |\n",
      "|    value_loss           | 3.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.47450980392157\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 375         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023195717 |\n",
      "|    clip_fraction        | 0.493       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.36        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 3.95        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.03703703703704\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 375         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025367308 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.911       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 3.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.61052631578947\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022639103 |\n",
      "|    clip_fraction        | 0.484       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.585       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.526       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 3.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.556666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 372         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023115909 |\n",
      "|    clip_fraction        | 0.496       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.49        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 3.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.38095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 371         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021536853 |\n",
      "|    clip_fraction        | 0.447       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.623       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 3.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.89090909090909\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 370         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021927591 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.331       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 3.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.60579710144928\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 370         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020557053 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0796      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 3.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.49722222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022138163 |\n",
      "|    clip_fraction        | 0.441       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.747       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 3.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.104\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020080745 |\n",
      "|    clip_fraction        | 0.432       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.52        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 3.19        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.45641025641026\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018552922 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.406       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.888888888888886\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 74         |\n",
      "|    total_timesteps      | 27648      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01788849 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.742      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.09       |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0655    |\n",
      "|    value_loss           | 3.08       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.25476190476191\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019709831 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0979      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.42988505747127\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 80         |\n",
      "|    total_timesteps      | 29696      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01876561 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.757      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.97       |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0685    |\n",
      "|    value_loss           | 2.98       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.63111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018714603 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.068      |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.991397849462366\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017438931 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.427083333333336\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 88         |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01757164 |\n",
      "|    clip_fraction        | 0.34       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.778      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.71       |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0692    |\n",
      "|    value_loss           | 2.85       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.898989898989896\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 91         |\n",
      "|    total_timesteps      | 33792      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01760743 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.802      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.59       |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0675    |\n",
      "|    value_loss           | 2.53       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.286274509803924\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018577185 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0706     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.68571428571428\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018590093 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.655       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.987037037037034\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017979383 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.381       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.07       |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.30990990990991\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018496374 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.703       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.62280701754386\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017767727 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.515       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.88034188034188\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018355235 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.138       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.225\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018057512 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.54308943089431\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018933747 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 1.89        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.91111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019859191 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.189       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0771     |\n",
      "|    value_loss           | 1.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.265116279069765\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 119        |\n",
      "|    total_timesteps      | 44032      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02116367 |\n",
      "|    clip_fraction        | 0.428      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.888      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.212     |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0775    |\n",
      "|    value_loss           | 1.68       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.57727272727273\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018550955 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.147       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0763     |\n",
      "|    value_loss           | 1.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.92296296296296\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017163813 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.31        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0735     |\n",
      "|    value_loss           | 1.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.18695652173913\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018089991 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00542    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0751     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.45248226950355\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017342918 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0764     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0725     |\n",
      "|    value_loss           | 1.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.8125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018089764 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0615      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0739     |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.12789115646258\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 136         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019684404 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.193      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0755     |\n",
      "|    value_loss           | 1.4         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.510666666666665\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 138        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01868824 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.92       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.147      |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.0733    |\n",
      "|    value_loss           | 1.31       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.84313725490196\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018372914 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.154       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0761     |\n",
      "|    value_loss           | 1.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.20384615384615\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 370         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020426802 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0694     |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0739     |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.60754716981132\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 371         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018977761 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0747     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0742     |\n",
      "|    value_loss           | 1.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.86666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 372         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017456084 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0425      |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0762     |\n",
      "|    value_loss           | 1.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.089696969696966\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 151        |\n",
      "|    total_timesteps      | 56320      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02016976 |\n",
      "|    clip_fraction        | 0.396      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0447    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0776    |\n",
      "|    value_loss           | 1.35       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.39047619047619\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018385444 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.14        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0788     |\n",
      "|    value_loss           | 1.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.642105263157895\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -180         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 373          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 156          |\n",
      "|    total_timesteps      | 58368        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0153574245 |\n",
      "|    clip_fraction        | 0.292        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.24        |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | -0.0114      |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0726      |\n",
      "|    value_loss           | 1.42         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.90574712643678\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 373        |\n",
      "|    iterations           | 58         |\n",
      "|    time_elapsed         | 158        |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01792594 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.929      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.097     |\n",
      "|    n_updates            | 570        |\n",
      "|    policy_gradient_loss | -0.0751    |\n",
      "|    value_loss           | 1.28       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.2\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020032197 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0142     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0783     |\n",
      "|    value_loss           | 1.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.513333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021235514 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.219      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0841     |\n",
      "|    value_loss           | 1.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.895081967213116\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 375         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 166         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020056637 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.213      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0774     |\n",
      "|    value_loss           | 1.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.22150537634408\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 375         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020981777 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.193      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0784     |\n",
      "|    value_loss           | 1.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.51111111111111\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 376        |\n",
      "|    iterations           | 63         |\n",
      "|    time_elapsed         | 171        |\n",
      "|    total_timesteps      | 64512      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01882232 |\n",
      "|    clip_fraction        | 0.39       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.937      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.146     |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0788    |\n",
      "|    value_loss           | 1.06       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.87708333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 376         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017467711 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.179      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0778     |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.30564102564102\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 376         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 176         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019074135 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.157      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0783     |\n",
      "|    value_loss           | 1.17        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.64343434343434\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 377         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 179         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017008308 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.214      |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0788     |\n",
      "|    value_loss           | 1.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.94427860696518\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 67         |\n",
      "|    time_elapsed         | 181        |\n",
      "|    total_timesteps      | 68608      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02082661 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.945      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.205     |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.081     |\n",
      "|    value_loss           | 0.986      |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.17941176470588\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 377         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 184         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018869992 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0806     |\n",
      "|    value_loss           | 1.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.45507246376812\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 378         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017342795 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.177      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0783     |\n",
      "|    value_loss           | 1           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.703809523809525\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 378         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 189         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019104684 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.105      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0828     |\n",
      "|    value_loss           | 1.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.97840375586854\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 378         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 191         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020492855 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0382     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0825     |\n",
      "|    value_loss           | 1.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.31574074074074\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 379         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018822111 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.144      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.082      |\n",
      "|    value_loss           | 1.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.64657534246575\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 379         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016787045 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.204      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0747     |\n",
      "|    value_loss           | 1.03        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.04144144144144\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 199        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01901958 |\n",
      "|    clip_fraction        | 0.398      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.29      |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0807    |\n",
      "|    value_loss           | 1          |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.42311111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 202         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018761039 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0867     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.082      |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.77894736842106\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 204         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020313157 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.254      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0833     |\n",
      "|    value_loss           | 0.903       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.08744588744588\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 207         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019408248 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.19       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0832     |\n",
      "|    value_loss           | 1           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.42649572649573\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 209        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01902688 |\n",
      "|    clip_fraction        | 0.402      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.184     |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.0831    |\n",
      "|    value_loss           | 0.971      |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.7746835443038\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 381         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018643998 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.104      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0811     |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.14\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 214         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019005904 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.188      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0839     |\n",
      "|    value_loss           | 1.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.440329218107\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 216         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019495878 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.266      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0824     |\n",
      "|    value_loss           | 1.01        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.72764227642277\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020683229 |\n",
      "|    clip_fraction        | 0.439       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.211      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0873     |\n",
      "|    value_loss           | 0.922       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.01526104417671\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 221         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020984467 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.287      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0854     |\n",
      "|    value_loss           | 1.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.27698412698413\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 224         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019030858 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.178      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.084      |\n",
      "|    value_loss           | 0.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.55294117647058\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 226         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020879388 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.304      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0846     |\n",
      "|    value_loss           | 0.818       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.80542635658915\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 229         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019500084 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.243      |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0823     |\n",
      "|    value_loss           | 1.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.07739463601533\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 231         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020148128 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.238      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0855     |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.32954545454545\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 234         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020086855 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.188      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0848     |\n",
      "|    value_loss           | 0.893       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.57378277153558\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 236         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018252779 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.254      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0833     |\n",
      "|    value_loss           | 0.964       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.80888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 239         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020105628 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.249      |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0866     |\n",
      "|    value_loss           | 0.914       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.03296703296704\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018377896 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0641     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0835     |\n",
      "|    value_loss           | 1.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.2427536231884\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -172       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 92         |\n",
      "|    time_elapsed         | 244        |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01887268 |\n",
      "|    clip_fraction        | 0.414      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.13      |\n",
      "|    explained_variance   | 0.93       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.143     |\n",
      "|    n_updates            | 910        |\n",
      "|    policy_gradient_loss | -0.0865    |\n",
      "|    value_loss           | 1.12       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.46164874551971\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 247         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016995817 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0364     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0842     |\n",
      "|    value_loss           | 1.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.68581560283688\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -171       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 249        |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01888634 |\n",
      "|    clip_fraction        | 0.409      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.1       |\n",
      "|    explained_variance   | 0.933      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.2       |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0859    |\n",
      "|    value_loss           | 1.07       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.88912280701754\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -171      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 386       |\n",
      "|    iterations           | 95        |\n",
      "|    time_elapsed         | 252       |\n",
      "|    total_timesteps      | 97280     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0201273 |\n",
      "|    clip_fraction        | 0.449     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.08     |\n",
      "|    explained_variance   | 0.938     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | -0.285    |\n",
      "|    n_updates            | 940       |\n",
      "|    policy_gradient_loss | -0.0883   |\n",
      "|    value_loss           | 1.01      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.10625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 254         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019342178 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.167      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0868     |\n",
      "|    value_loss           | 1.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.34776632302406\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 257         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021122195 |\n",
      "|    clip_fraction        | 0.471       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.147      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0898     |\n",
      "|    value_loss           | 1.01        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.55578231292517\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017661896 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.04       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.139      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.084      |\n",
      "|    value_loss           | 1.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.77979797979798\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 261         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021271834 |\n",
      "|    clip_fraction        | 0.471       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.175      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0901     |\n",
      "|    value_loss           | 0.944       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.99733333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 264         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020249132 |\n",
      "|    clip_fraction        | 0.451       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.99       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.118      |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0887     |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 2.0\n",
      "Overall Average Successful Assignments: 53.79254969536124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa48ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
