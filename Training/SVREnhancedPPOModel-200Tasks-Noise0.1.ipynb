{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1eabb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.403494699070916\n",
      "RMSE: 1.9135086312664382\n",
      "R-squared: 0.9752350615552381\n",
      "RAE: 0.14658131997050208\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(columns=['Eligible'])\n",
    "    y = data['Eligible']\n",
    "    return X, y\n",
    "\n",
    "def train_svr_model(X_train, y_train):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    svr_model = SVR()\n",
    "    svr_model.fit(X_train_scaled, y_train)\n",
    "    return svr_model, scaler\n",
    "\n",
    "# Load and train on VehicleTrainingDataset.csv\n",
    "X_train, y_train = load_and_preprocess_data('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "svr_model, scaler = train_svr_model(X_train, y_train)\n",
    "\n",
    "# Predict eligibility scores on 1000VehicleDataset.csv\n",
    "vehicles_df = pd.read_csv('1000VehicleDataset_Noisy_0.1.csv')\n",
    "X_test = vehicles_df.drop(columns=['Eligible'])\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "predicted_scores = svr_model.predict(X_test_scaled)\n",
    "\n",
    "# Assuming you have access to actual scores, replace this line with the actual score loading logic if available\n",
    "y_actual = vehicles_df['Eligible']  # This would be prior to overwriting with predictions if you run this block again\n",
    "\n",
    "# Replace actual scores with predicted ones\n",
    "vehicles_df['Eligible'] = predicted_scores  \n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_actual, predicted_scores)\n",
    "rmse = np.sqrt(mean_squared_error(y_actual, predicted_scores))\n",
    "r_squared = r2_score(y_actual, predicted_scores)\n",
    "rae = np.sum(np.abs(y_actual - predicted_scores)) / np.sum(np.abs(y_actual - np.mean(y_actual)))\n",
    "\n",
    "# Output the results\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"RAE: {rae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e4c77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 6.8\n",
      "All assignments history: [5, 4, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -187     |\n",
      "| time/              |          |\n",
      "|    fps             | 94       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 9.666666666666666\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 88           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075806063 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | -0.259       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 3.02         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0427      |\n",
      "|    value_loss           | 17.8         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.066666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008815343 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.181      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.32        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    value_loss           | 13.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.166666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008535936 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | -0.0388     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.75        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    value_loss           | 11.5        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.213333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008700279 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0424      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.59        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 9.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.522222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009107465 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0709      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.89        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    value_loss           | 8.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.476190476190474\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009576436 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.92        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    value_loss           | 7.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.475\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0103143975 |\n",
      "|    clip_fraction        | 0.177        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.48        |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.34         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0452      |\n",
      "|    value_loss           | 6.58         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.585185185185185\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010936546 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    value_loss           | 5.56        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.986666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011018159 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 5.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.412121212121214\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011528362 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    value_loss           | 4.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.172222222222224\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 143          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0131497905 |\n",
      "|    clip_fraction        | 0.259        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.46        |\n",
      "|    explained_variance   | 0.192        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.388        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0505      |\n",
      "|    value_loss           | 4.8          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.4\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014856984 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.051      |\n",
      "|    value_loss           | 4.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.1\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013968229 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.82        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    value_loss           | 4.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.76888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013089066 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.17        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 4.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.3375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012841977 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 4.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.75686274509804\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012885142 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.98518518518519\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 217         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012975413 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.486       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 4.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.10877192982456\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 229         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012771354 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.514       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.03        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 4.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.67\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011301473 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.846       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 4.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.0\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 254         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010802502 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 4.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.32727272727273\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -183      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 84        |\n",
      "|    iterations           | 22        |\n",
      "|    time_elapsed         | 265       |\n",
      "|    total_timesteps      | 22528     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0104833 |\n",
      "|    clip_fraction        | 0.205     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -6.43     |\n",
      "|    explained_variance   | 0.581     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.77      |\n",
      "|    n_updates            | 210       |\n",
      "|    policy_gradient_loss | -0.0543   |\n",
      "|    value_loss           | 3.97      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.492753623188406\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 279         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010114897 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.923       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 3.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.580555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 293         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009364196 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 3.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.42133333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009659827 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.617       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 3.83        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.31794871794872\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 319         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010122813 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 3.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.10864197530864\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 331         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009401144 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.934       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0505     |\n",
      "|    value_loss           | 3.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.864285714285714\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008856461 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.38       |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.77        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0492     |\n",
      "|    value_loss           | 3.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010085406 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.28        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 3.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.31111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 370         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009895235 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.764       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 3.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.00430107526882\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009862373 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.33       |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 3.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.64791666666667\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -177      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 82        |\n",
      "|    iterations           | 32        |\n",
      "|    time_elapsed         | 396       |\n",
      "|    total_timesteps      | 32768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0098195 |\n",
      "|    clip_fraction        | 0.189     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -6.32     |\n",
      "|    explained_variance   | 0.702     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.42      |\n",
      "|    n_updates            | 310       |\n",
      "|    policy_gradient_loss | -0.0534   |\n",
      "|    value_loss           | 3.13      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.14343434343434\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 409         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008898609 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.3        |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 422         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010491681 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.29       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.825       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 3.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.09904761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008805275 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.26       |\n",
      "|    explained_variance   | 0.719       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.629629629629626\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 448         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008078156 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.24       |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.948       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 3.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.17297297297297\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 460         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008607922 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.21       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.937       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 3.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.675438596491226\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 473         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008373661 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.2        |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.863       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.19145299145299\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 485         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009217894 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.16       |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.757       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 3.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.693333333333335\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -169         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 497          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092611555 |\n",
      "|    clip_fraction        | 0.18         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.12        |\n",
      "|    explained_variance   | 0.709        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.38         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0543      |\n",
      "|    value_loss           | 2.94         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.21951219512195\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 509         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010151729 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.08       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.75555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 521         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009988343 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.05       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.894       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.29767441860465\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 533         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010058506 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.852       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.803030303030305\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -163        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 545         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010022853 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.96       |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.978       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.33777777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 556         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009976393 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.9        |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.703       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.79710144927536\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -158       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 570        |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00996012 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.86      |\n",
      "|    explained_variance   | 0.679      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.952      |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.0577    |\n",
      "|    value_loss           | 2.79       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.25248226950355\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -156        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 582         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009735206 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.83       |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.82        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.66805555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -154        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 596         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008838725 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.78       |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.0952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -152        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 609         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010727423 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.73       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.7         |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.52666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -150        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 623         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011162288 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.69       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.96078431372548\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -148        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 636         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011528604 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.63       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.989       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.44615384615385\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -145        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 650         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011103917 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.59       |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.551       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0624     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.90817610062894\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -143        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 662         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008313858 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.53       |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.877       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 3.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.29753086419753\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -140        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 675         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010084554 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.49       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.927       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.70424242424242\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -137        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 686         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009927537 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.44       |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.881       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.1595238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -135        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 699         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010520201 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.38       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.383       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.60584795321637\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -132        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 711         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009979783 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.33       |\n",
      "|    explained_variance   | 0.654       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.821       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.02528735632184\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -129        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 725         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010409404 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.3        |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.595       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.45310734463277\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -126        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 737         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010837339 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.26       |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.57        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.9088888888889\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -123       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 81         |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 751        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01023375 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.2       |\n",
      "|    explained_variance   | 0.633      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.501      |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0576    |\n",
      "|    value_loss           | 2.22       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.3584699453552\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -120       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 81         |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 763        |\n",
      "|    total_timesteps      | 62464      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00878145 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.14      |\n",
      "|    explained_variance   | 0.594      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.731      |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0545    |\n",
      "|    value_loss           | 2.63       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.8021505376344\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -118        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 775         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009381564 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.14       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.791       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.22857142857143\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -115        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 788         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009082289 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.11       |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.742       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.71041666666666\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -112         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 801          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0105486605 |\n",
      "|    clip_fraction        | 0.217        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.08        |\n",
      "|    explained_variance   | 0.603        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.53         |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.0577      |\n",
      "|    value_loss           | 2.05         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.18564102564102\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -110        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 813         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008632135 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.832       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.63333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -107        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 826         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009249622 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.915       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.07661691542289\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -104        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 839         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009206748 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.86       |\n",
      "|    explained_variance   | 0.615       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.715       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.54509803921569\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -101        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 852         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009846162 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.82       |\n",
      "|    explained_variance   | 0.527       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.863       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0567     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.98937198067632\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -97.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 863         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009761697 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.79       |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.914       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.47142857142858\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -95         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 878         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010271485 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.81       |\n",
      "|    explained_variance   | 0.546       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.439       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.9511737089202\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -91.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 890         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009480011 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.77       |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.42592592592592\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -89.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 903         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009299913 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.74       |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.579       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.88127853881278\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -87.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 916         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009774324 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.71       |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.697       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.36576576576577\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -84.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 929         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009679403 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.63       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.761       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.86044444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -82         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 941         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008849334 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.55       |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.692       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.33508771929824\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -79.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 955         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009728125 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.49       |\n",
      "|    explained_variance   | 0.514       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.649       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.79220779220779\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -77         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 967         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009204584 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.46       |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.733       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.24615384615385\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -74.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 980         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009995343 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.44       |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.909       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.68776371308017\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -72       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 81        |\n",
      "|    iterations           | 79        |\n",
      "|    time_elapsed         | 994       |\n",
      "|    total_timesteps      | 80896     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0104933 |\n",
      "|    clip_fraction        | 0.229     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -4.42     |\n",
      "|    explained_variance   | 0.469     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.722     |\n",
      "|    n_updates            | 780       |\n",
      "|    policy_gradient_loss | -0.0563   |\n",
      "|    value_loss           | 2.44      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.105\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -69.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1008        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009847023 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.937       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.52510288065844\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -67.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 1024        |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009830016 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.35       |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.739       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.9430894308943\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -64.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 1039        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009191746 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.34       |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.859       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.3437751004016\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -62.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 1054        |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008483369 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.32       |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0505     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.76349206349207\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -60.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 1069        |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009287317 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.28       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.896       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0573     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.16156862745098\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -58.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 1083        |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009444724 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.757       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.58527131782945\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -55.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 1096        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009794275 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.16       |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.825       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.97701149425288\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -53.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1109        |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009941536 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.12       |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.733       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.36363636363636\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -52.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1123        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010069596 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.12       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.586       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 1.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.75880149812734\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -50.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 80         |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 1136       |\n",
      "|    total_timesteps      | 91136      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00923833 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.06      |\n",
      "|    explained_variance   | 0.403      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.915      |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0543    |\n",
      "|    value_loss           | 2.22       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.14888888888889\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -48.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 80         |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 1151       |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00840562 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.02      |\n",
      "|    explained_variance   | 0.42       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.16       |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.049     |\n",
      "|    value_loss           | 2.41       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.52234432234432\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -46.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1164        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008883466 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.929       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0474     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.89492753623189\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -44.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1179        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008818036 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.01       |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.903       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.26379928315413\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -42.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1192        |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008526537 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4          |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.799       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.63262411347517\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -40.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 79           |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 1206         |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0107051935 |\n",
      "|    clip_fraction        | 0.214        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.01        |\n",
      "|    explained_variance   | 0.417        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.977        |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.0524      |\n",
      "|    value_loss           | 2.21         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.98245614035088\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -40         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1221        |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009172532 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4          |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.796       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.31597222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -38.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1235        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008740136 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.97       |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.734       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.6618556701031\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -37.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 79           |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 1248         |\n",
      "|    total_timesteps      | 99328        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097339265 |\n",
      "|    clip_fraction        | 0.196        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.97        |\n",
      "|    explained_variance   | 0.351        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.746        |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0489      |\n",
      "|    value_loss           | 2.44         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.9938775510204\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -36.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1262        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009274652 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.01       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.31380471380471\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -35.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 79         |\n",
      "|    iterations           | 99         |\n",
      "|    time_elapsed         | 1276       |\n",
      "|    total_timesteps      | 101376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00792297 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -3.99      |\n",
      "|    explained_variance   | 0.406      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.84       |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0495    |\n",
      "|    value_loss           | 2.5        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.646\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -33.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 79         |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 1290       |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01010428 |\n",
      "|    clip_fraction        | 0.214      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -3.97      |\n",
      "|    explained_variance   | 0.432      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.955      |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.0534    |\n",
      "|    value_loss           | 2.36       |\n",
      "----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 20.9\n",
      "Overall Average Successful Assignments: 62.47224416357992\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31bac7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -196.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 3.6666666666666665\n",
      "All assignments history: [9, 6, 8, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.466666666666667\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -187         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088465605 |\n",
      "|    clip_fraction        | 0.0872       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | -0.207       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 3.27         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0473      |\n",
      "|    value_loss           | 17.8         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.866666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008414641 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.137      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.27        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    value_loss           | 13.6        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.216666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008626715 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0122      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.34        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    value_loss           | 10.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008880226 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | -0.00724    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.26        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    value_loss           | 9.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -96.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.855555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010016854 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0339      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.18        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    value_loss           | 8.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009962738 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0475      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.63        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    value_loss           | 7.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.983333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010705983 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.044       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    value_loss           | 6.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.42222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012133466 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.0623      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    value_loss           | 5.72        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.593333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011883541 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.0821      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.17        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    value_loss           | 5.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.87272727272727\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 77         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 145        |\n",
      "|    total_timesteps      | 11264      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01224545 |\n",
      "|    clip_fraction        | 0.24       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.46      |\n",
      "|    explained_variance   | 0.117      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.47       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0473    |\n",
      "|    value_loss           | 5.02       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.92222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012916225 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.615       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 4.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.97948717948718\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013777593 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.217       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0487     |\n",
      "|    value_loss           | 4.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.60952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014949903 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 4.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.68888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 199         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013510488 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.411       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 4.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.4625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013352081 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.65        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 3.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.28235294117647\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 226         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013720058 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.52        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 3.83        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.425925925925924\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -184         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 76           |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 240          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0120023955 |\n",
      "|    clip_fraction        | 0.255        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.44        |\n",
      "|    explained_variance   | 0.517        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.67         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0535      |\n",
      "|    value_loss           | 3.91         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.11578947368421\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 253         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013035524 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 3.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.593333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012605021 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.72        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 3.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.888888888888886\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -183         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 76           |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 280          |\n",
      "|    total_timesteps      | 21504        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0122367125 |\n",
      "|    clip_fraction        | 0.246        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.42        |\n",
      "|    explained_variance   | 0.629        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.26         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0551      |\n",
      "|    value_loss           | 3.66         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.04545454545455\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012745331 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.638       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0569     |\n",
      "|    value_loss           | 3.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.37101449275362\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 308         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010641377 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 3.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.65555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011323338 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 3.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.89333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 336         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011841756 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 3.65        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.11282051282051\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 349         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012271218 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 3.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.318518518518516\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011590334 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.38        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0569     |\n",
      "|    value_loss           | 3.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.397619047619045\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 377         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009473494 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.38       |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0498     |\n",
      "|    value_loss           | 3.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.47126436781609\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 391         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009714167 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 3.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.42\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 404         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009511916 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.36       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 3.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.30752688172043\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 418         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011668541 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.776       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 3.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.24375\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -177       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 76         |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 430        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01137423 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.33      |\n",
      "|    explained_variance   | 0.721      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.712      |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0556    |\n",
      "|    value_loss           | 3.07       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.147474747474746\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 445         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010673581 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.31       |\n",
      "|    explained_variance   | 0.71        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.844       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 3.27        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.970588235294116\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 458         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010087987 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.29       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 3.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.72571428571428\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009571666 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.27       |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 3.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.483333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010674925 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.25       |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0567     |\n",
      "|    value_loss           | 3.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.203603603603604\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 496         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008930529 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.22       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.76        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 3.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.86491228070175\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -172       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 76         |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 509        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00974101 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.2       |\n",
      "|    explained_variance   | 0.718      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.879      |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0511    |\n",
      "|    value_loss           | 3.19       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.51111111111111\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -171         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 76           |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 522          |\n",
      "|    total_timesteps      | 39936        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104060965 |\n",
      "|    clip_fraction        | 0.187        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.19        |\n",
      "|    explained_variance   | 0.705        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.518        |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0538      |\n",
      "|    value_loss           | 3.25         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.151666666666664\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -170         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 76           |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 535          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0118598845 |\n",
      "|    clip_fraction        | 0.244        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.17        |\n",
      "|    explained_variance   | 0.704        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.07         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0615      |\n",
      "|    value_loss           | 3.1          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.78536585365854\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 545         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010532053 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.14       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.876       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.407936507936505\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 558         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010914806 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.11       |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.666       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 2.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.074418604651164\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 571         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009509491 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.07       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 3.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.67272727272727\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 584         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008871317 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.03       |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.758       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 3.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.25185185185185\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -163        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 597         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009507099 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.98       |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.727       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.84347826086956\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -162        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 611         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009519894 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.93       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.42836879432625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 623         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010506613 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.86       |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.94        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 3.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.97222222222223\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -158       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 77         |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 636        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01085851 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.83      |\n",
      "|    explained_variance   | 0.656      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.972      |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0555    |\n",
      "|    value_loss           | 2.9        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.48571428571428\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -157        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 649         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010734354 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.8        |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.04933333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -155        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 660         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010388592 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.8        |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.677       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.58562091503268\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -152        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 674         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010131257 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.73       |\n",
      "|    explained_variance   | 0.622       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.653       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 3.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.08717948717948\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -150        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 686         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009752592 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.66       |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.678       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.6062893081761\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -148        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 698         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010344049 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.61       |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.868       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.08148148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -146        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 711         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010083495 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.53       |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.772       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.58545454545454\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -144         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 77           |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 723          |\n",
      "|    total_timesteps      | 56320        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101583265 |\n",
      "|    clip_fraction        | 0.211        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.49        |\n",
      "|    explained_variance   | 0.584        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.706        |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0574      |\n",
      "|    value_loss           | 3.05         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.07261904761904\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -141        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 736         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008940306 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.43       |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.869       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.54385964912281\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -138        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 748         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008824208 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.36       |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 3.4         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.01264367816091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -135        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 760         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008288601 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.28       |\n",
      "|    explained_variance   | 0.6         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.72        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.48248587570622\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -132        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 773         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009555824 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.25       |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.838       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.96222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -129        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 785         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008567849 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.22       |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.549       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.42622950819673\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -127        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 798         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009963023 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.19       |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.781       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.91397849462365\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -124        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 810         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010212505 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.16       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.37566137566138\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -121        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 822         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009564282 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.12       |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.798       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.865625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -118        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 834         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009895663 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.07       |\n",
      "|    explained_variance   | 0.522       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.788       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.41025641025641\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -115         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 78           |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 846          |\n",
      "|    total_timesteps      | 66560        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0109001035 |\n",
      "|    clip_fraction        | 0.219        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.06        |\n",
      "|    explained_variance   | 0.517        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.905        |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0544      |\n",
      "|    value_loss           | 2.58         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.91515151515152\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -112        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 858         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009211564 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.963       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.40398009950249\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -110      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 78        |\n",
      "|    iterations           | 67        |\n",
      "|    time_elapsed         | 870       |\n",
      "|    total_timesteps      | 68608     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0102464 |\n",
      "|    clip_fraction        | 0.201     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -4.97     |\n",
      "|    explained_variance   | 0.531     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.688     |\n",
      "|    n_updates            | 660       |\n",
      "|    policy_gradient_loss | -0.0551   |\n",
      "|    value_loss           | 2.55      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.9186274509804\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -106        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 882         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009470321 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.94       |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.4135265700483\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -103       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 78         |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 894        |\n",
      "|    total_timesteps      | 70656      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00891065 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.88      |\n",
      "|    explained_variance   | 0.514      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.1        |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0529    |\n",
      "|    value_loss           | 2.73       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.87904761904763\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -100        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 906         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009339949 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.84       |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.34741784037558\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -97.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 918         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008934767 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.81       |\n",
      "|    explained_variance   | 0.524       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.883       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.7925925925926\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 931         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008962211 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.75       |\n",
      "|    explained_variance   | 0.462       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.675       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.25662100456621\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -92.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 79         |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 943        |\n",
      "|    total_timesteps      | 74752      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01052562 |\n",
      "|    clip_fraction        | 0.197      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.69      |\n",
      "|    explained_variance   | 0.467      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.692      |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0565    |\n",
      "|    value_loss           | 2.6        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.73423423423424\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -88.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 955         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009238392 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.65       |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.18044444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -85.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 968         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009383222 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.59       |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.774       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.62982456140351\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -82.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 980         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010157428 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.56       |\n",
      "|    explained_variance   | 0.517       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.862       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.07359307359307\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -80         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 993         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008998977 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.51       |\n",
      "|    explained_variance   | 0.516       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.747       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.5068376068376\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -77.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 1005        |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007843637 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.45       |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.92573839662447\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -74.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 1017        |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010165906 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.45       |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.34166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -72.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1029        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009250004 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.36       |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.902       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.75884773662551\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -69.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 1041        |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009763187 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.35       |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.845       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.20569105691057\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -66.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 1053        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009441146 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.94        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.62489959839357\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -63.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 1064        |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008363562 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.23       |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.934       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.03571428571429\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -61.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 79         |\n",
      "|    iterations           | 84         |\n",
      "|    time_elapsed         | 1078       |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00853619 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.22      |\n",
      "|    explained_variance   | 0.425      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.894      |\n",
      "|    n_updates            | 830        |\n",
      "|    policy_gradient_loss | -0.0486    |\n",
      "|    value_loss           | 2.84       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.44\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -59.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 1090        |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009521639 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.18       |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.84651162790698\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -56.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 79         |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 1102       |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00877763 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.16      |\n",
      "|    explained_variance   | 0.495      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.829      |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.0508    |\n",
      "|    value_loss           | 2.52       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.24750957854405\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -54         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1113        |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008444849 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.1        |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.875       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.62878787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -52.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1125        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009638956 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.08       |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.927       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.02771535580524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -49.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1137        |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008487179 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.04       |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.41037037037037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -46.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1149        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008990357 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.04       |\n",
      "|    explained_variance   | 0.522       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.79413919413919\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -44.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1161        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007847169 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4          |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.636       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.16884057971015\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -41.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1174        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009005098 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.98       |\n",
      "|    explained_variance   | 0.486       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.902       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.53189964157706\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -39.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1186        |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010678331 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.99       |\n",
      "|    explained_variance   | 0.522       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.88368794326242\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -38.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1198        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008958378 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.97       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.787       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.22526315789473\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -36.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 80         |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 1210       |\n",
      "|    total_timesteps      | 97280      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00850328 |\n",
      "|    clip_fraction        | 0.197      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -3.96      |\n",
      "|    explained_variance   | 0.55       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.903      |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0511    |\n",
      "|    value_loss           | 2.71       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.57569444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -35         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1222        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010011577 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.94       |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.984       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.92646048109965\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -33.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1234        |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009062486 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.96       |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.753       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.26054421768707\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -32.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1246        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008937873 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.96       |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.6026936026936\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -30.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1259        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008603857 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.95       |\n",
      "|    explained_variance   | 0.602       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.678       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 2.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.94266666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -28.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1270        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008233056 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.91       |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 25.26\n",
      "Overall Average Successful Assignments: 61.1018161098028\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "662bcb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 4.4\n",
      "All assignments history: [5, 8, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -190     |\n",
      "| time/              |          |\n",
      "|    fps             | 98       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.9\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -189         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 91           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073889927 |\n",
      "|    clip_fraction        | 0.0609       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | -0.254       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.46         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0441      |\n",
      "|    value_loss           | 18           |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.911111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008987464 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.0278     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    value_loss           | 11.8        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.016666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -188       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 43         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00904919 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.51      |\n",
      "|    explained_variance   | -0.0538    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.64       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0463    |\n",
      "|    value_loss           | 9.5        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.0\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008308233 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.00283     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.85        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 9.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.033333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009057334 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.019       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.09        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    value_loss           | 8.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.419047619047618\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -187         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 7168         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102845635 |\n",
      "|    clip_fraction        | 0.174        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.49        |\n",
      "|    explained_variance   | 0.0343       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.45         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.046       |\n",
      "|    value_loss           | 6.65         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009780019 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0373      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.041      |\n",
      "|    value_loss           | 6.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.31111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010652423 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.0454      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.64        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0423     |\n",
      "|    value_loss           | 5.74        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.89333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 117         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011818386 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.0491      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.26        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    value_loss           | 5.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.07272727272727\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012474754 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.0704      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.923       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0464     |\n",
      "|    value_loss           | 5.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.977777777777774\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013345109 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.36        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    value_loss           | 5.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.87692307692308\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -186       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 153        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01444204 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.46      |\n",
      "|    explained_variance   | 0.158      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 4.26       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0517    |\n",
      "|    value_loss           | 4.65       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.67619047619048\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 164        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01379572 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.46      |\n",
      "|    explained_variance   | 0.238      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.66       |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0505    |\n",
      "|    value_loss           | 4.63       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.33777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 176         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014194834 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.82        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 4.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.725\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012348734 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.468       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 4.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.14901960784314\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -184         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 199          |\n",
      "|    total_timesteps      | 17408        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0124711245 |\n",
      "|    clip_fraction        | 0.236        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.46        |\n",
      "|    explained_variance   | 0.388        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.38         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.051       |\n",
      "|    value_loss           | 4.49         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.2037037037037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012839267 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.346       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 4.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.23157894736842\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010149345 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.378       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 4.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.13\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 234         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011634881 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 4.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.91746031746032\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 245         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010671093 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 4.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.82424242424243\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 257         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011442593 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.506       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.77        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 4.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.4231884057971\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009857714 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.519       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.47        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 4.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.92777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010058079 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.556       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.364       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 4.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.61866666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 293         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010070211 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 4.16        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.261538461538464\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010964094 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.77        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.839506172839506\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 317         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011203475 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.6         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.72        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 3.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.352380952380955\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 329         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010629973 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.469       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 4.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.829885057471266\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -180         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 340          |\n",
      "|    total_timesteps      | 29696        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113003105 |\n",
      "|    clip_fraction        | 0.216        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.4         |\n",
      "|    explained_variance   | 0.603        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 3.03         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0542      |\n",
      "|    value_loss           | 4.09         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.315555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 352         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010938457 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.535       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 3.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.733333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010627763 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.38       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.492       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 4.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.12916666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011088316 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.36       |\n",
      "|    explained_variance   | 0.622       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 3.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.68080808080808\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 385         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010194348 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.33        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 3.54        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.11372549019608\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 397         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010938864 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.34       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 3.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.60190476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 408         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010717256 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.32       |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 3.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.03333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 419         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010587266 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.31       |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.836       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 3.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.40900900900901\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 430         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010017715 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.29       |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 3.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.819298245614036\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 441         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010681884 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.27       |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.282       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 3.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.24786324786325\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 453         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010817649 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.24       |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.501       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 3.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.62\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010640433 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.22       |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.156       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.09593495934959\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -170       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 474        |\n",
      "|    total_timesteps      | 41984      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00983799 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.2       |\n",
      "|    explained_variance   | 0.707      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.346      |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0543    |\n",
      "|    value_loss           | 2.89       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.57619047619048\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 485         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010083688 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.18       |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.04961240310078\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 496         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009813537 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.15       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.5\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 508         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008944683 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.12       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.45        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 3.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.94814814814815\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 518         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009769151 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.08       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.545       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.3840579710145\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 530         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009310808 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.04       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.672       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.77163120567376\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -162       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 540        |\n",
      "|    total_timesteps      | 48128      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01042782 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.98      |\n",
      "|    explained_variance   | 0.689      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.59       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0577    |\n",
      "|    value_loss           | 3          |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.22638888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 552         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009831484 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.95       |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.467       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.69795918367348\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -159       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 563        |\n",
      "|    total_timesteps      | 50176      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00972357 |\n",
      "|    clip_fraction        | 0.182      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.91      |\n",
      "|    explained_variance   | 0.646      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.418      |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.054     |\n",
      "|    value_loss           | 2.97       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.132\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -157        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 574         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010462181 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.87       |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.453       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.55294117647058\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -155        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 585         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010508969 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.84       |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.634       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.99615384615385\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -153        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 596         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009384749 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.8        |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.40251572327044\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -151        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 607         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008613002 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.76       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.976       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0505     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.83456790123456\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -149        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 618         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009070396 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.7        |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.465       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.24484848484849\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -147       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 629        |\n",
      "|    total_timesteps      | 56320      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00999934 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.65      |\n",
      "|    explained_variance   | 0.657      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.813      |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0589    |\n",
      "|    value_loss           | 2.49       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.70714285714286\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -144       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 640        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00861466 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.59      |\n",
      "|    explained_variance   | 0.614      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.23       |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0521    |\n",
      "|    value_loss           | 3.27       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.14152046783626\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 652         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009336969 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.51       |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.998       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.5816091954023\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -139         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 89           |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 662          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093024615 |\n",
      "|    clip_fraction        | 0.189        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.44        |\n",
      "|    explained_variance   | 0.638        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.24         |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.0571      |\n",
      "|    value_loss           | 2.81         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.02598870056497\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -137        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 674         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009071635 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.41       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.519       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.4088888888889\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -134       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 685        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01004259 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.37      |\n",
      "|    explained_variance   | 0.646      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.832      |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0547    |\n",
      "|    value_loss           | 2.51       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.79781420765028\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -132        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 696         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008427845 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.29       |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.767       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.21397849462366\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -129        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 707         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010187915 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.25       |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.726       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 2.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.62010582010582\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -126        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 718         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009394102 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.21       |\n",
      "|    explained_variance   | 0.594       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.00208333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -123        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 728         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008691635 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.17       |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.799       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.38666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -121        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 739         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009371539 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.09       |\n",
      "|    explained_variance   | 0.555       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.666       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.79494949494949\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -117        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 750         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009424635 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.03       |\n",
      "|    explained_variance   | 0.57        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.24378109452736\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -114        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 761         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008030979 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.98       |\n",
      "|    explained_variance   | 0.55        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.742       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.71960784313725\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -111       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 90         |\n",
      "|    iterations           | 68         |\n",
      "|    time_elapsed         | 772        |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00930679 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.96      |\n",
      "|    explained_variance   | 0.565      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.897      |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | -0.0521    |\n",
      "|    value_loss           | 2.26       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.21932367149759\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -108        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 783         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008289104 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.88       |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.489       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.69238095238096\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -104        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 794         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009199406 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.85       |\n",
      "|    explained_variance   | 0.551       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.16244131455399\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -101        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 804         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009159704 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.79       |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.822       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.61944444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -98.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 816         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009437898 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.76       |\n",
      "|    explained_variance   | 0.6         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.954       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0498     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.08949771689498\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -95.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 826         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009180401 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.72       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.819       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.55765765765766\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -92.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 837         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008594072 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.66       |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.668       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.0231111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -89.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 848         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009399252 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.62       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.673       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.48508771929825\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -86.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 859         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008979583 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.59       |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.587       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.93506493506493\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -83.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 870         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007962733 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.52       |\n",
      "|    explained_variance   | 0.582       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.891       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.049      |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.37777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -80.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 881         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009049883 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.46       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.939       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.84135021097046\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -77.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 892         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008887479 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.42       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.707       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.28083333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -74.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 903         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009288744 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.38       |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.547       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.71028806584363\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -71.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 914         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009802806 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.35       |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.766       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.14878048780488\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -68.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 925         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008480771 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.29       |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.701       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.57510040160642\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -65.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 936         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008252184 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.29       |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.532       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0462     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.0079365079365\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -62.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 946         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007811614 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.27       |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.637       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.42823529411764\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -59.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 957         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009117145 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.23       |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.54        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.84341085271318\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -57.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 968         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008783573 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.19       |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.2183908045977\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -55.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 979         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009111704 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.17       |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.874       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.64242424242424\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -52.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 990         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009070521 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.15       |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.456       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.02771535580524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -51.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1001        |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008830594 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.09       |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.675       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.42592592592592\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -49         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1012        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007608251 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.06       |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.643       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.82124542124542\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -46.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1024        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008407786 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.778       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.20579710144928\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -44.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1035        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008299815 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4          |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.514       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.60215053763442\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -42         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1047        |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008067278 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.475       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.96099290780141\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -40.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1059        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008145457 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.01       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.887       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.33052631578947\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -38.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 1070         |\n",
      "|    total_timesteps      | 97280        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0099801505 |\n",
      "|    clip_fraction        | 0.182        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.99        |\n",
      "|    explained_variance   | 0.766        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.983        |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0509      |\n",
      "|    value_loss           | 2.32         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.70694444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -36.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1082        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008906854 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.99       |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.693       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.06666666666666\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -35          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 1093         |\n",
      "|    total_timesteps      | 99328        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077782962 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.97        |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.996        |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0467      |\n",
      "|    value_loss           | 2.42         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.42925170068027\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -33.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1104        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008725298 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.94       |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.709       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.051      |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.7986531986532\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -31.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 1116         |\n",
      "|    total_timesteps      | 101376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078122355 |\n",
      "|    clip_fraction        | 0.158        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -3.92        |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.691        |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.047       |\n",
      "|    value_loss           | 2.53         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.152\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -30.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1126        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008042475 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.93       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.958       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 29.9\n",
      "Overall Average Successful Assignments: 65.5434077361968\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0bb182",
   "metadata": {},
   "source": [
    "Done Till Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48bc36f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -198.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 3.1333333333333333\n",
      "All assignments history: [12, 8, 6, 9, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -185     |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -168.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 8.3\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -184         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077499934 |\n",
      "|    clip_fraction        | 0.067        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | -0.091       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0459      |\n",
      "|    value_loss           | 16.2         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.2\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008893898 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.0297     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 11.1        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.4\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079826135 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | 0.0184       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.04         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0427      |\n",
      "|    value_loss           | 10.3         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.786666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008781508 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0337      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.28        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 8.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.4\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008865574 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0609      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.4         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    value_loss           | 8.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.495238095238093\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008669453 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0736      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    value_loss           | 7.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.341666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009999279 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.073       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 6.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -136.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.659259259259258\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -186       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 97         |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01152421 |\n",
      "|    clip_fraction        | 0.195      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.48      |\n",
      "|    explained_variance   | 0.0869     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.19       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0471    |\n",
      "|    value_loss           | 5.6        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.426666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010590844 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.11        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    value_loss           | 5.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.82424242424243\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 117         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012270787 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.06        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 5.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.166666666666664\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -186       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 95         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 128        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01333154 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.47      |\n",
      "|    explained_variance   | 0.166      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.18       |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0485    |\n",
      "|    value_loss           | 4.47       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.94358974358974\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014528118 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.179       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 4.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.352380952380955\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014501128 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.45        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 4.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.18222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015073056 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.583       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 4.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.641666666666666\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -185         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 172          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0134789385 |\n",
      "|    clip_fraction        | 0.264        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.46        |\n",
      "|    explained_variance   | 0.389        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.45         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0521      |\n",
      "|    value_loss           | 4.39         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.8\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 183         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014120971 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.48        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 4.25        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.34444444444444\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -185         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 194          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0134665435 |\n",
      "|    clip_fraction        | 0.25         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.46        |\n",
      "|    explained_variance   | 0.481        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 4.37         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.053       |\n",
      "|    value_loss           | 4.23         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.5859649122807\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012068635 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 4.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.71666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 216         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012790335 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.61        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.7047619047619\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012144076 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 4.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.38181818181818\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -184         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 238          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104788905 |\n",
      "|    clip_fraction        | 0.178        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.44        |\n",
      "|    explained_variance   | 0.562        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.53         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0511      |\n",
      "|    value_loss           | 4.25         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.12463768115942\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 249        |\n",
      "|    total_timesteps      | 23552      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01152036 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.43      |\n",
      "|    explained_variance   | 0.575      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.12       |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0556    |\n",
      "|    value_loss           | 4.13       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.922222222222224\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 261        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01119094 |\n",
      "|    clip_fraction        | 0.213      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.43      |\n",
      "|    explained_variance   | 0.592      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.671      |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0539    |\n",
      "|    value_loss           | 4.12       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.776\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011307299 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.603       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.9         |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 3.88        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.57179487179487\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 282         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010359272 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.599       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.513       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 4.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.303703703703704\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 293         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009164646 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.635       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 3.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.92619047619048\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 304        |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00959018 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.4       |\n",
      "|    explained_variance   | 0.647      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.02       |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0516    |\n",
      "|    value_loss           | 3.71       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.58390804597701\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -181         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 315          |\n",
      "|    total_timesteps      | 29696        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0107918065 |\n",
      "|    clip_fraction        | 0.187        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.39        |\n",
      "|    explained_variance   | 0.665        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.34         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0534      |\n",
      "|    value_loss           | 3.54         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.117777777777775\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 326         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011273403 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.38       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.935       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 3.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.686021505376345\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 337         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010399473 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 3.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.208333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 347         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010171594 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.45        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 3.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.62222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009826966 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.34       |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 3.55        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.04117647058823\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -177       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 366        |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01031031 |\n",
      "|    clip_fraction        | 0.201      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.32      |\n",
      "|    explained_variance   | 0.673      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.56       |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.0552    |\n",
      "|    value_loss           | 3.44       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.42285714285714\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 377         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010302389 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.3        |\n",
      "|    explained_variance   | 0.71        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.472       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 3.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.73148148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 389         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009889195 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.29       |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.589       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 3.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.04504504504504\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 399         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010258816 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.27       |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.925       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.324561403508774\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 411         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009522083 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.25       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 3.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.59145299145299\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 421         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010160051 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.22       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.08        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.92666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 432         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010021159 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.19       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0579     |\n",
      "|    value_loss           | 3.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.27967479674797\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 443         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010042389 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.17       |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.7015873015873\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 454         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009601312 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.12       |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.52        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 3.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.04341085271318\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -168       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 465        |\n",
      "|    total_timesteps      | 44032      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01078818 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.1       |\n",
      "|    explained_variance   | 0.693      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.98       |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0591    |\n",
      "|    value_loss           | 2.83       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.39848484848486\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 476         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009423891 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.08       |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.928       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.75555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 487         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010082404 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.05       |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.08260869565217\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -163        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 498         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008779699 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6          |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.828       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.48085106382979\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -162        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 510         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009284597 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.98       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.858       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.91111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 520         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008996874 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.94       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.35238095238095\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -158        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 533         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008903494 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.92       |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.898       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.76666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -156        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 544         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008503202 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.89       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.552       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.14640522875817\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -154        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 555         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007903669 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.84       |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.685       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.4948717948718\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -152        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 566         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007999456 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.79       |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.732       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.86037735849057\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -151        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 578         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008944827 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.75       |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.761       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.24444444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -148        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 589         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008395962 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.69       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.306       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.63878787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -146        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 600         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009755366 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.67       |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.448       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.08214285714286\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -144        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 611         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008632293 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.62       |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.636       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.52631578947368\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 623         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008246054 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.59       |\n",
      "|    explained_variance   | 0.719       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.677       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.97241379310344\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -140        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 634         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009046999 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.57       |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.634       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.41694915254237\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -137        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 645         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009797461 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.51       |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.767       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.87555555555555\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -135       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 93         |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 656        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00806137 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.42      |\n",
      "|    explained_variance   | 0.729      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.497      |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0555    |\n",
      "|    value_loss           | 2.06       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.37377049180328\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -133        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 668         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008246958 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.36       |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.646       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 1.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.83333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -131        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 679         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008684343 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.3        |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.662       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.29417989417989\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -128        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 690         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008607502 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.24       |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.195       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.051      |\n",
      "|    value_loss           | 1.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.73854166666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -125        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 701         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009770335 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.16       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.181       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.19589743589744\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -122        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 712         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008677866 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.11       |\n",
      "|    explained_variance   | 0.717       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.354       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.64848484848486\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -120       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 93         |\n",
      "|    iterations           | 66         |\n",
      "|    time_elapsed         | 723        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00914054 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.08      |\n",
      "|    explained_variance   | 0.687      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.562      |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | -0.0526    |\n",
      "|    value_loss           | 2.01       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.09054726368159\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -117         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 93           |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 735          |\n",
      "|    total_timesteps      | 68608        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072087008 |\n",
      "|    clip_fraction        | 0.148        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5           |\n",
      "|    explained_variance   | 0.721        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.495        |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0499      |\n",
      "|    value_loss           | 1.95         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.52843137254902\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -115        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 746         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009458262 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.95       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.386       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.96231884057971\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -112        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 757         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008600575 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.94       |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.384       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0493     |\n",
      "|    value_loss           | 1.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.41619047619048\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -109        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 768         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008449652 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.9        |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.405       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.85446009389672\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -106         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 93           |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 780          |\n",
      "|    total_timesteps      | 72704        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073276665 |\n",
      "|    clip_fraction        | 0.144        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.86        |\n",
      "|    explained_variance   | 0.739        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.454        |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.0479      |\n",
      "|    value_loss           | 1.93         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.28148148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -104        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 791         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008881403 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.82       |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.366       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 1.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.69771689497716\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -101        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 802         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008866811 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.77       |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.576       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.13873873873874\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -97.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 814         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008842912 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.69       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.459       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.55733333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -94.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 825         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008325029 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.64       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.574       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.98859649122807\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -91.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 92           |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 836          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080979755 |\n",
      "|    clip_fraction        | 0.168        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.59        |\n",
      "|    explained_variance   | 0.698        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.61         |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.0519      |\n",
      "|    value_loss           | 2.24         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.42597402597403\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -88.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 847         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007875336 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.54       |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.601       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.84017094017094\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -85.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 858         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008978985 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.49       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.59        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.2717299578059\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -82.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 870         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009806231 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.46       |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.675       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.685\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -79.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 92         |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 881        |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00922666 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.46      |\n",
      "|    explained_variance   | 0.709      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.654      |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | -0.0471    |\n",
      "|    value_loss           | 2.19       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.08641975308642\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -76.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 892         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008249752 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.44       |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.646       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.5\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -73.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 904         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008572213 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.38       |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.964       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.90361445783132\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -70.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 92           |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 915          |\n",
      "|    total_timesteps      | 84992        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088415705 |\n",
      "|    clip_fraction        | 0.176        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.32        |\n",
      "|    explained_variance   | 0.746        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.857        |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0501      |\n",
      "|    value_loss           | 2.06         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.30555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -67.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 926         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008936061 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.29       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.704       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0498     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.70117647058824\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -65.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 937         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009226026 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.21       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.478       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.09457364341085\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -62.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 92           |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 949          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074466276 |\n",
      "|    clip_fraction        | 0.178        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.15        |\n",
      "|    explained_variance   | 0.732        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.544        |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.0495      |\n",
      "|    value_loss           | 2.08         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.46130268199234\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -59.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 960         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009322535 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.12       |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.597       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 2.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.8340909090909\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -56.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 971         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009101036 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.13       |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.891       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.19850187265918\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -54.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 981         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009377874 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.14       |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.578       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.58074074074074\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -52.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 992         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009482184 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.12       |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.778       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.93846153846154\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -50.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1003        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009623077 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.09       |\n",
      "|    explained_variance   | 0.615       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.746       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.30144927536232\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -48.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1015        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009352824 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.03       |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.689       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.68243727598566\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -45.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1026        |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008020275 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.98       |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.61        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.05531914893616\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -43.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1037        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008277981 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.95       |\n",
      "|    explained_variance   | 0.645       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.762       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.42736842105263\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -41.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1047        |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008523449 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.92       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.699       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.775\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -39.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1059        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009446681 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.9        |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.14089347079037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -37.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1070        |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009534331 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.88       |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.922       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.50408163265305\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -35.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 92         |\n",
      "|    iterations           | 98         |\n",
      "|    time_elapsed         | 1081       |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00857609 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -3.84      |\n",
      "|    explained_variance   | 0.588      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.615      |\n",
      "|    n_updates            | 970        |\n",
      "|    policy_gradient_loss | -0.0515    |\n",
      "|    value_loss           | 2.1        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.85589225589226\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -34.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1093        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007889378 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.81       |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.899       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0477     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.20933333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -32.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1104        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008756659 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.78       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.698       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 21.92\n",
      "Overall Average Successful Assignments: 63.691230142312264\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0069fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -196.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 3.6\n",
      "All assignments history: [6, 13, 6, 4, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    fps             | 99       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -168.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 8.3\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 101          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079884585 |\n",
      "|    clip_fraction        | 0.0643       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | -0.131       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.84         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0472      |\n",
      "|    value_loss           | 18           |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.777777777777779\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009524617 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.00338    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.049      |\n",
      "|    value_loss           | 12.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.516666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008366559 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0177      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.17        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 10.6        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.506666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009217737 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0877      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.66        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    value_loss           | 9.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.4\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009301513 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.087       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.84        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 8.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.133333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009579119 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0987      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    value_loss           | 7.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.883333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010405677 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.0975      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.49        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 6.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.474074074074075\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011043394 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    value_loss           | 5.73        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.946666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011435069 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.65        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    value_loss           | 5.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.2060606060606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013317676 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.537       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 4.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.9\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013952729 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 4.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.42051282051282\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015145354 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.784       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 4.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.65714285714286\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014295759 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.265       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.05       |\n",
      "|    value_loss           | 4.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.675555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015473597 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.331       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0574     |\n",
      "|    value_loss           | 4.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.7\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014629279 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 4.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.8078431372549\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 184         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013651593 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.542       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 3.94        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.05555555555556\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -183         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 93           |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 196          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0124082025 |\n",
      "|    clip_fraction        | 0.243        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.44        |\n",
      "|    explained_variance   | 0.503        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0539      |\n",
      "|    value_loss           | 3.92         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.12982456140351\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 208         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010551814 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 4.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.193333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011423903 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.562       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.92        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 3.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.17460317460318\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011139838 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.6         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 3.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.93636363636364\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010277757 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.638       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 3.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.71884057971015\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009992877 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.903       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 3.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.516666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009366341 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 3.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.285333333333334\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -181         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 91           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 279          |\n",
      "|    total_timesteps      | 25600        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0109003885 |\n",
      "|    clip_fraction        | 0.208        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.4         |\n",
      "|    explained_variance   | 0.713        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.37         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0552      |\n",
      "|    value_loss           | 3.04         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.07435897435897\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 292         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010019692 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.38       |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.80493827160494\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010113638 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.783       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.44285714285714\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010627978 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 3.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.04137931034483\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -178         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 328          |\n",
      "|    total_timesteps      | 29696        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0100647025 |\n",
      "|    clip_fraction        | 0.188        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.33        |\n",
      "|    explained_variance   | 0.724        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.871        |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0549      |\n",
      "|    value_loss           | 3.27         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.766666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 340         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009930398 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.32       |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.817       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.3247311827957\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 352         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010329641 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.31       |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.565       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.9\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011336042 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.29       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.5         |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.36363636363637\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 375         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009841105 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.27       |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.727450980392156\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 386         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009717361 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.25       |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.906       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.163809523809526\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 397         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010292015 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.23       |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.535185185185185\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -173       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 90         |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 409        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00910534 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.2       |\n",
      "|    explained_variance   | 0.75       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.691      |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.0548    |\n",
      "|    value_loss           | 2.8        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.93153153153153\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -172       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 421        |\n",
      "|    total_timesteps      | 37888      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00944685 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.17      |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.773      |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0545    |\n",
      "|    value_loss           | 2.8        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.373684210526314\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 433         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009218896 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.13       |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.914       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.89059829059829\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009438947 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.09       |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.576       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.431666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 456         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008248443 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.04       |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.17        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.05       |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.952845528455285\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 468         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009871769 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.444       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.53015873015873\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 479         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010071914 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.95       |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.307       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.11937984496124\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -163         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 89           |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 492          |\n",
      "|    total_timesteps      | 44032        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092069935 |\n",
      "|    clip_fraction        | 0.181        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.9         |\n",
      "|    explained_variance   | 0.756        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.643        |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0555      |\n",
      "|    value_loss           | 2.65         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.666666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -161        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010084603 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.84       |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.679       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.19111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -159        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 516         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010810614 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.78       |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.499       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.698550724637684\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -158        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 528         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009573262 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.74       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.812       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.23971631205674\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -155        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 541         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010560505 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.68       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.46        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.77361111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -153        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 552         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008850982 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.64       |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.745       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.28163265306122\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -151        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 565         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010001951 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.56       |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.792\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -148       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 577        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00993019 |\n",
      "|    clip_fraction        | 0.198      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.48      |\n",
      "|    explained_variance   | 0.724      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.593      |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.0575    |\n",
      "|    value_loss           | 2.29       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.31372549019608\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -145        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 589         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010842081 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.41       |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.663       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.77564102564102\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 601         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009459091 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.38       |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.887       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.25408805031446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -139        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 613         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009483151 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.543       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.72592592592592\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -136        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 625         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009136006 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.26       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.852       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.18666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -133        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 637         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010205347 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.2        |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.424       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0569     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.63928571428572\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -130       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 649        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00969913 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.19      |\n",
      "|    explained_variance   | 0.71       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.893      |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0488    |\n",
      "|    value_loss           | 2.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.09239766081872\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -126        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 661         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009348234 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.15       |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.635       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.54022988505747\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -123        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 673         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008224975 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.09       |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.521       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.00677966101695\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -120        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 685         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009340921 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.03       |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.737       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.44666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -117        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 698         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008802635 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.98       |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.699       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.88196721311475\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -114        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 709         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009400158 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.97       |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.677       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.2989247311828\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -111        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 721         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010565924 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.96       |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.565       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 1.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.71322751322751\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -108        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 733         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009123746 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.94       |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.429       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.121875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -106        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 745         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010539582 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.94       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.382       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 1.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.55076923076923\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -103        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 758         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008892909 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.696       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.97373737373738\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -101        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 769         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009381261 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.452       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 1.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.38109452736319\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -98.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 782         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008714387 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.569       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.79117647058824\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -96         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 794         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008834173 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.89       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.554       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.16908212560386\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -93.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 806         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008553812 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.82       |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.613       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.56571428571428\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -91.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 818         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009717112 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.82       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.513       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.95774647887323\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -89.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 829         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008225359 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.8        |\n",
      "|    explained_variance   | 0.637       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.743       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 1.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.34722222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -87.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 841         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008214128 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.79       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.498       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0477     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.7351598173516\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -86.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 853         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009417402 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.76       |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.732       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 1.83        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.1108108108108\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -84.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 865         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008159501 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.72       |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.531       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.50577777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -83.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 877         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008555004 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.72       |\n",
      "|    explained_variance   | 0.621       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.871       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0505     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.87456140350878\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -81.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 76         |\n",
      "|    time_elapsed         | 889        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00958355 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.73      |\n",
      "|    explained_variance   | 0.659      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.69       |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | -0.0499    |\n",
      "|    value_loss           | 2.05       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.24415584415584\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -80.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 901         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008977175 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.68       |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.453       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.62393162393163\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -78.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 913         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008471049 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.63       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.762       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.0\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -77.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 926         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009545235 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.59       |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.615       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.37916666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -75.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 938         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009388385 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.56       |\n",
      "|    explained_variance   | 0.608       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.436       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.76954732510288\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -74.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 950          |\n",
      "|    total_timesteps      | 82944        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077734888 |\n",
      "|    clip_fraction        | 0.149        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.52        |\n",
      "|    explained_variance   | 0.57         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.87         |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0479      |\n",
      "|    value_loss           | 2.48         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.16829268292683\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -72.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 962         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009283336 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.51       |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.38        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 1.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.55100401606425\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -70.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 974         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009479363 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.47       |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.738       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.92380952380952\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -69.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 986         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009257231 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.46       |\n",
      "|    explained_variance   | 0.591       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.597       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.32627450980392\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -67.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 998          |\n",
      "|    total_timesteps      | 87040        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093419645 |\n",
      "|    clip_fraction        | 0.205        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.44        |\n",
      "|    explained_variance   | 0.572        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.675        |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0525      |\n",
      "|    value_loss           | 2.09         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.6906976744186\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -65.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 1010        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010068946 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.44       |\n",
      "|    explained_variance   | 0.578       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.593       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.04904214559387\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -64.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1022        |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009673719 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.43       |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.832       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.39242424242424\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -63.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1034        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009347366 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.39       |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.696       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.7378277153558\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -61.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1046        |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010262115 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.39       |\n",
      "|    explained_variance   | 0.581       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.766       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.0562962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -60.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1059        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010267656 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.39       |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.559       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.40586080586081\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -59.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1071        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010324229 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.37       |\n",
      "|    explained_variance   | 0.56        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.841       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.75579710144928\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -58         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1083        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009723443 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.33       |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.582       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.12043010752689\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -56.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 1095         |\n",
      "|    total_timesteps      | 95232        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096433135 |\n",
      "|    clip_fraction        | 0.219        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.29        |\n",
      "|    explained_variance   | 0.536        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.572        |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0543      |\n",
      "|    value_loss           | 1.98         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.4758865248227\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -54.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1107        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009991566 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.23       |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.596       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.83017543859648\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -52.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1119        |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009487497 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.2        |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.748       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.17361111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -50.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1131        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010095459 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.15       |\n",
      "|    explained_variance   | 0.504       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.435       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.5360824742268\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -48.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 1143         |\n",
      "|    total_timesteps      | 99328        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102526955 |\n",
      "|    clip_fraction        | 0.218        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.13        |\n",
      "|    explained_variance   | 0.462        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.775        |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.052       |\n",
      "|    value_loss           | 2.24         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.88367346938776\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -47.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1155        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009793919 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.11       |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.903       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.21818181818182\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -45.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1166        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010802301 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.11       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.498       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.56\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -43.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1179        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010915711 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.1        |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.402       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 15.42\n",
      "Overall Average Successful Assignments: 61.987667441653585\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ab579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 9.6\n",
      "All assignments history: [5, 10, 2, 5, 2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -190     |\n",
      "| time/              |          |\n",
      "|    fps             | 94       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.066666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -190        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007774767 |\n",
      "|    clip_fraction        | 0.0654      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.0301     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.54        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    value_loss           | 15.9        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.511111111111113\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -189        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008103218 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | 0.0533      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.94        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 12.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.566666666666666\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -189         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082905665 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | 0.0498       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.46         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0431      |\n",
      "|    value_loss           | 9.82         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.693333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -189        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008768195 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0753      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.46        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    value_loss           | 8.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.67777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -189        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008516345 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0794      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.22        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    value_loss           | 8.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.63809523809524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009659445 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0795      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 7.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.275\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -188         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 88           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0100038685 |\n",
      "|    clip_fraction        | 0.169        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.48        |\n",
      "|    explained_variance   | 0.0853       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.19         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0431      |\n",
      "|    value_loss           | 6.26         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.34814814814815\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010938751 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.0976      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.82        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0464     |\n",
      "|    value_loss           | 5.69        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.093333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011822281 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.444       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    value_loss           | 5.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.696969696969695\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012216582 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    value_loss           | 5.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.0\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013916206 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.776       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 4.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.53846153846154\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014417123 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 4.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.86666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013836635 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.298       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 4.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.86222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011542445 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.99        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    value_loss           | 4.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.99583333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 182         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012410591 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.26        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 4.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.082352941176474\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013159435 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.915       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 3.68        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.17407407407408\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 206         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012698948 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.585       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.861       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 3.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.045614035087716\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011575447 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 3.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.71\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 230         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012212235 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.338       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 3.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.27301587301587\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011225129 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 3.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.084848484848486\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 254        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01161992 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.43      |\n",
      "|    explained_variance   | 0.691      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.21       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0569    |\n",
      "|    value_loss           | 3.29       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.643478260869564\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -183      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 88        |\n",
      "|    iterations           | 23        |\n",
      "|    time_elapsed         | 266       |\n",
      "|    total_timesteps      | 23552     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0107929 |\n",
      "|    clip_fraction        | 0.216     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -6.42     |\n",
      "|    explained_variance   | 0.694     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.945     |\n",
      "|    n_updates            | 220       |\n",
      "|    policy_gradient_loss | -0.0549   |\n",
      "|    value_loss           | 3.23      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.15555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 278         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009667739 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.051      |\n",
      "|    value_loss           | 3.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.70666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 289         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011394631 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.93        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.2102564102564\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010520017 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.74        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.71604938271605\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 313         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010457562 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.38       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 3.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.19285714285714\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 326         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009456445 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 3.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.60919540229885\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 338         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009072096 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.36       |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.778       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.94666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008279744 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.34       |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.712       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.3247311827957\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008328383 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.33       |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.748       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.63333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 373         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008580652 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.31       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.936       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.98989898989899\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 385         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009772336 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.28       |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.668       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.32352941176471\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009228412 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.26       |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.432       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.64380952380952\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 410         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010013793 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.23       |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.83        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.94814814814815\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -173         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 422          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096630035 |\n",
      "|    clip_fraction        | 0.187        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.21        |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.63         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.0568      |\n",
      "|    value_loss           | 2.31         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.2036036036036\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 434         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009676779 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.17       |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.71        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.50701754385965\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 446         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010612138 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.13       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.897       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.8\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 458         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010799695 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.09       |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.654       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.06166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 468         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009400364 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.05       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.697       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.36422764227642\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -166         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 480          |\n",
      "|    total_timesteps      | 41984        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102421455 |\n",
      "|    clip_fraction        | 0.203        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6           |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.551        |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.061       |\n",
      "|    value_loss           | 2.15         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.66190476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 492         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009536008 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.94       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.651       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.0062015503876\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -162        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009672982 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.88       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.694       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.2969696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 516         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009590976 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.79       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.636       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0591     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.61037037037038\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -157       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 527        |\n",
      "|    total_timesteps      | 46080      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00975471 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.73      |\n",
      "|    explained_variance   | 0.817      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.438      |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0591    |\n",
      "|    value_loss           | 1.89       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.93478260869566\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -154        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 540         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008890993 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.65       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.467       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.25673758865248\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -152        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 551         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008949386 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.57       |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.648       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.60694444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -149        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009234897 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.5        |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.779       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.96598639455782\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -145        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 575         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008188832 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.44       |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.786       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.30266666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -142       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 587        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00782651 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.37      |\n",
      "|    explained_variance   | 0.749      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.995      |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.0513    |\n",
      "|    value_loss           | 2.5        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.65490196078431\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -139        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 599         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009214707 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.31       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.873       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.0\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -136        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 610         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009334747 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.22       |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.673       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.36603773584906\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -132        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 623         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008917114 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.14       |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.663       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.68641975308643\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -129         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 635          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080074165 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.13        |\n",
      "|    explained_variance   | 0.766        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.478        |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.0469      |\n",
      "|    value_loss           | 2.05         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.04121212121213\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -126        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 647         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010087672 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.08       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.63        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.39047619047619\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -122        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 659         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009608413 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.04       |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.434       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.70760233918129\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -119        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 670         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009538757 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.02       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.645       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.08045977011494\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -116        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 682         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009008913 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.99       |\n",
      "|    explained_variance   | 0.71        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.594       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.45988700564972\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -112        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 694         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010045605 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.95       |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.699       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.80444444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -109        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 706         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010605864 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.92       |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.487       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.18142076502733\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -105        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 718         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009529441 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.87       |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.746       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.56774193548387\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -102        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 730         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009062715 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.8        |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.711       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.94708994708995\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -99.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 742         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009053426 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.76       |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.773       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.33020833333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -96.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 753         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010020368 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.74       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.478       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.75794871794872\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -93.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 765         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008634685 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.72       |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.738       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.17676767676768\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -91.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 777          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086600315 |\n",
      "|    clip_fraction        | 0.169        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.68        |\n",
      "|    explained_variance   | 0.738        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.597        |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.0507      |\n",
      "|    value_loss           | 2.04         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.64875621890548\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -88.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 789         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008533038 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.62       |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.508       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.07352941176471\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -85.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 803         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009224159 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.58       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.773       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.53140096618357\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -83.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 815         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009659871 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.55       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.643       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.94952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -80.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 827         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008598918 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.49       |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.669       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.36807511737089\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -78.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 839         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009945527 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.48       |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.648       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.76388888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -75.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 851         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010094624 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.49       |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.715       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.16712328767123\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -74.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 863         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010059614 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.49       |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.937       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.55495495495495\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -72         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 875         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010354301 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.48       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.746       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.96711111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -69.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 887         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008324379 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.43       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.584       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.34649122807018\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -67.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 899         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010131044 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.42       |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.64        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.73419913419913\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -65.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 910         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009359652 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.36       |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.724       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0567     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.108547008547\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -64.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 921         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009472478 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.35       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.49620253164557\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -62.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 932         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009060691 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.35       |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.638       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.8575\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -60.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 943         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009988953 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.703       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.20987654320987\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -58.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 954        |\n",
      "|    total_timesteps      | 82944      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00907007 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.39      |\n",
      "|    explained_variance   | 0.726      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.81       |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0524    |\n",
      "|    value_loss           | 2.28       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.59268292682927\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -57.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 965         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009032648 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.34       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.57        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.96305220883534\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -55.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 976         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009594894 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.37       |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.629       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.33095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -53.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 987         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009328652 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.37       |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.531       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.69333333333333\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -52.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 998          |\n",
      "|    total_timesteps      | 87040        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0089118555 |\n",
      "|    clip_fraction        | 0.177        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.33        |\n",
      "|    explained_variance   | 0.69         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.978        |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0511      |\n",
      "|    value_loss           | 2.54         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.0875968992248\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -50          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 1009         |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094726635 |\n",
      "|    clip_fraction        | 0.197        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.31        |\n",
      "|    explained_variance   | 0.712        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.576        |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.0516      |\n",
      "|    value_loss           | 2.37         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.46819923371648\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -48.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1019        |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008858927 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.375       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.8310606060606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -47.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1031        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009120977 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.28       |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.603       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.18426966292135\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -46.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1042        |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008492153 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.25       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.769       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.051      |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.52666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -45.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1054        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008715874 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.19       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.87472527472528\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -44.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 91         |\n",
      "|    time_elapsed         | 1065       |\n",
      "|    total_timesteps      | 93184      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00809766 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.17      |\n",
      "|    explained_variance   | 0.763      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.612      |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0485    |\n",
      "|    value_loss           | 2.22       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.20869565217392\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -43.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 1076         |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085677225 |\n",
      "|    clip_fraction        | 0.176        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.17        |\n",
      "|    explained_variance   | 0.745        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.635        |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.0495      |\n",
      "|    value_loss           | 2.64         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.5426523297491\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -41.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1087        |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010033179 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.15       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.986       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.87588652482269\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -40.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1098        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010142235 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.14       |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.19508771929824\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -39.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1110        |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009575535 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.11       |\n",
      "|    explained_variance   | 0.769       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.798       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.5201388888889\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -37.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 96           |\n",
      "|    time_elapsed         | 1121         |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075875884 |\n",
      "|    clip_fraction        | 0.161        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.06        |\n",
      "|    explained_variance   | 0.763        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.938        |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.0495      |\n",
      "|    value_loss           | 2.44         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.82886597938145\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -36.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 1132         |\n",
      "|    total_timesteps      | 99328        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078314785 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.05        |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.764        |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.049       |\n",
      "|    value_loss           | 2.5          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.14013605442177\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -35.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 98           |\n",
      "|    time_elapsed         | 1143         |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085269995 |\n",
      "|    clip_fraction        | 0.179        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.03        |\n",
      "|    explained_variance   | 0.783        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.908        |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.0507      |\n",
      "|    value_loss           | 2.64         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.42895622895622\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -35.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1155        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008008083 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.06       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.97        |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.71133333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -34         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1167        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008064671 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 27.72\n",
      "Overall Average Successful Assignments: 70.24357537570471\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22486ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -194.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 3.933333333333333\n",
      "All assignments history: [6, 7, 3, 7, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -188     |\n",
      "| time/              |          |\n",
      "|    fps             | 92       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.5\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007871486 |\n",
      "|    clip_fraction        | 0.0665      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.0964     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.26        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    value_loss           | 17.5        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.066666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009085756 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | 0.0547      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 12.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.633333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009580106 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0717      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.89        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    value_loss           | 9.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.826666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007958632 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0789      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.33        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0394     |\n",
      "|    value_loss           | 9.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009833293 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0953      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.12        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    value_loss           | 7.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.02857142857143\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009503806 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    value_loss           | 7.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.05\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 93         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01026584 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.48      |\n",
      "|    explained_variance   | 0.124      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.81       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    value_loss           | 6.1        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.14074074074074\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011466472 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    value_loss           | 5.49        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.693333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011817009 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    value_loss           | 5.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.24242424242424\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012521282 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.75        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 4.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.03333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014586864 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.676       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.051      |\n",
      "|    value_loss           | 4.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.98974358974359\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014689593 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.231       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.72        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 4.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.871428571428574\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015085638 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.429       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 4.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.644444444444446\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 173        |\n",
      "|    total_timesteps      | 15360      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01443071 |\n",
      "|    clip_fraction        | 0.284      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.46      |\n",
      "|    explained_variance   | 0.346      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.77       |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0529    |\n",
      "|    value_loss           | 4.4        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.979166666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013922788 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 4.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.254901960784316\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 198         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011071662 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0492     |\n",
      "|    value_loss           | 4.15        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.525925925925925\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 210         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010711817 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.785       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    value_loss           | 4.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.72280701754386\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 221        |\n",
      "|    total_timesteps      | 19456      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01118879 |\n",
      "|    clip_fraction        | 0.201      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.45      |\n",
      "|    explained_variance   | 0.5        |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.505      |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0493    |\n",
      "|    value_loss           | 4.19       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.67333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 233         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009847907 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 4.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.53650793650794\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010936543 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.91        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 3.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.32727272727273\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 257         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010889249 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.421       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 3.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.15942028985507\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009229308 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.955       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0462     |\n",
      "|    value_loss           | 3.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.83611111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 280         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010790304 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.85        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 3.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.42933333333333\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -181      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 87        |\n",
      "|    iterations           | 25        |\n",
      "|    time_elapsed         | 291       |\n",
      "|    total_timesteps      | 25600     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0103977 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -6.41     |\n",
      "|    explained_variance   | 0.613     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 2.4       |\n",
      "|    n_updates            | 240       |\n",
      "|    policy_gradient_loss | -0.0503   |\n",
      "|    value_loss           | 3.81      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.02307692307692\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -181      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 87        |\n",
      "|    iterations           | 26        |\n",
      "|    time_elapsed         | 303       |\n",
      "|    total_timesteps      | 26624     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0094827 |\n",
      "|    clip_fraction        | 0.16      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -6.41     |\n",
      "|    explained_variance   | 0.645     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 2.2       |\n",
      "|    n_updates            | 250       |\n",
      "|    policy_gradient_loss | -0.0485   |\n",
      "|    value_loss           | 3.52      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.58271604938272\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 315        |\n",
      "|    total_timesteps      | 27648      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01003469 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.4       |\n",
      "|    explained_variance   | 0.646      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.351      |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0514    |\n",
      "|    value_loss           | 3.57       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.09761904761905\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 326        |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00986525 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.39      |\n",
      "|    explained_variance   | 0.655      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.11       |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0505    |\n",
      "|    value_loss           | 3.68       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.43218390804598\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 338         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009432791 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.38       |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1           |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 3.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.815555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 349         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008995717 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.51        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 3.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.30107526881721\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009242878 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 3.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.62291666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 373         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009153084 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.592       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.92323232323233\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 385         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008960558 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.33       |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.05       |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.19607843137256\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 396         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010013364 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.32       |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.906       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.48\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 409         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009906221 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.3        |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.672       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 3.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.69074074074074\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -175         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 420          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096990755 |\n",
      "|    clip_fraction        | 0.168        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.28        |\n",
      "|    explained_variance   | 0.732        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.21         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.0516      |\n",
      "|    value_loss           | 3.01         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.8972972972973\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -174         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 432          |\n",
      "|    total_timesteps      | 37888        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094417725 |\n",
      "|    clip_fraction        | 0.158        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.26        |\n",
      "|    explained_variance   | 0.719        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.34         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.0516      |\n",
      "|    value_loss           | 3.13         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.11754385964912\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009839864 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.24       |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.2991452991453\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009109105 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.21       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 3.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.485\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 467         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010553397 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.19       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.71056910569105\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 478         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009771792 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.17       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.479       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.90793650793651\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 490         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009389278 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.14       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.595       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.11472868217054\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 501         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009493113 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.1        |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.524       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.2969696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 513         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010639699 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.05       |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.4562962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 524         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010648068 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.976       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.6304347826087\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -162        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 536         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010354478 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.96       |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.446       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.81702127659574\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 548         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007989519 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.92       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.9625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -159        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 559         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009515814 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.88       |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.883       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.17959183673469\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -156        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 571         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009558171 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.82       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.753       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.416\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -154        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 583         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008993093 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.76       |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.617       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.65359477124183\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -152        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 594         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009001914 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.7        |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.569       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.92820512820512\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -150        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 606         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009674838 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.69       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.763       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.17232704402515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -147        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 618         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009019456 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.65       |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.39753086419753\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -145        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 630         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008719705 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.62       |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.656       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.6460606060606\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -142         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 642          |\n",
      "|    total_timesteps      | 56320        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090211965 |\n",
      "|    clip_fraction        | 0.181        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.57        |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.826        |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0546      |\n",
      "|    value_loss           | 2.4          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.875\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -140       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 653        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00760584 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.52      |\n",
      "|    explained_variance   | 0.676      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.01       |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0485    |\n",
      "|    value_loss           | 2.94       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.15087719298245\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -137        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 665         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009940661 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.44       |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.799       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.4\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -135        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 676         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008036848 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.39       |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.742       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.68587570621469\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -132         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 688          |\n",
      "|    total_timesteps      | 60416        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088027865 |\n",
      "|    clip_fraction        | 0.166        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.33        |\n",
      "|    explained_variance   | 0.693        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.81         |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0521      |\n",
      "|    value_loss           | 2.38         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.95222222222222\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -129      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 87        |\n",
      "|    iterations           | 60        |\n",
      "|    time_elapsed         | 699       |\n",
      "|    total_timesteps      | 61440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0089377 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -5.27     |\n",
      "|    explained_variance   | 0.699     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.448     |\n",
      "|    n_updates            | 590       |\n",
      "|    policy_gradient_loss | -0.0541   |\n",
      "|    value_loss           | 2.41      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.17814207650274\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -126        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 711         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008563577 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.21       |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.41612903225807\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -123        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 723         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009440196 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.13       |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.324       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0587     |\n",
      "|    value_loss           | 1.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.6994708994709\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -120        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 734         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008603882 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.05       |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.769       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.96458333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -117       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 746        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00853703 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.01      |\n",
      "|    explained_variance   | 0.692      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.819      |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0478    |\n",
      "|    value_loss           | 1.94       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.2348717948718\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -114        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 758         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008486589 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.97       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.791       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.50404040404041\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -111        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 770         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009123217 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.92       |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.635       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.77412935323383\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -108        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 782         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009293221 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.87       |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.494       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.05392156862744\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -106        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 792         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008002104 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.83       |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.67        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.38357487922706\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -103         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 803          |\n",
      "|    total_timesteps      | 70656        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083037205 |\n",
      "|    clip_fraction        | 0.165        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.82        |\n",
      "|    explained_variance   | 0.654        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.598        |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0489      |\n",
      "|    value_loss           | 1.94         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.67619047619047\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -101        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 813         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008250673 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.78       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.616       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.98403755868544\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -97.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 825         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009575404 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.74       |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.646       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.2824074074074\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -95.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 835         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009012731 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.68       |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.477       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 1.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.5607305936073\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -92.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 846         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008813763 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.67       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.633       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.85585585585585\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -90        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 857        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00906112 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.64      |\n",
      "|    explained_variance   | 0.607      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.71       |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.052     |\n",
      "|    value_loss           | 2.42       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.14577777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -88         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 869         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010264273 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.64       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.493       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.47368421052632\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -85.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 881         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009924187 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.61       |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.896       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.84848484848484\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -83.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 892         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009822836 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.63       |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.332       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 1.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.22564102564102\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -80.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 904        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00963455 |\n",
      "|    clip_fraction        | 0.197      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.59      |\n",
      "|    explained_variance   | 0.662      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.908      |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.0537    |\n",
      "|    value_loss           | 2.17       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.61181434599156\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -78.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 915         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008448539 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.57       |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.615       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.97166666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -76.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 925         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008884655 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.54       |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.602       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 2           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.35061728395061\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -74.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 934         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008609053 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.52       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.538       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 1.96        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.72357723577235\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -72.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 944         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009822901 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.51       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.614       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 2.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.10281124497992\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -71         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 955         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009493637 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.48       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.529       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 2.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.47777777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -69.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 965         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009741962 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.48       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.749       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.84549019607843\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -68          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 89           |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 975          |\n",
      "|    total_timesteps      | 87040        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095783435 |\n",
      "|    clip_fraction        | 0.174        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.49        |\n",
      "|    explained_variance   | 0.65         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.503        |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0486      |\n",
      "|    value_loss           | 2.09         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.24728682170543\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -66.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 986         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008739341 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.44       |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.493       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.64061302681992\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -64.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 996         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008699599 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.38       |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.624       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.07878787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -62         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1007        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009348044 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.37       |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.428       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 1.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.47340823970038\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -61         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1017        |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009188521 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.36       |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.636       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.88592592592593\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -58.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1027        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009627812 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.32       |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.644       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.3040293040293\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -57.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1037        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009433863 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.29       |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.496       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.70289855072464\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -55.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1048        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009794546 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.609       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.08243727598567\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -54.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1057        |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010309491 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.497       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0505     |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.48794326241135\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -51.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1068        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008728689 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.23       |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.508       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.89052631578947\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -50.2     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 90        |\n",
      "|    iterations           | 95        |\n",
      "|    time_elapsed         | 1079      |\n",
      "|    total_timesteps      | 97280     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0083925 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -4.18     |\n",
      "|    explained_variance   | 0.69      |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.02      |\n",
      "|    n_updates            | 940       |\n",
      "|    policy_gradient_loss | -0.0503   |\n",
      "|    value_loss           | 2.18      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.3\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -48.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1089        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009448584 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.14       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.674       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.70103092783505\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -46.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1099        |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009378545 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.13       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.488       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.10272108843537\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -44.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1110        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008565042 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.1        |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.523       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.4922558922559\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -42.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1120        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008704105 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.06       |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.585       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.87266666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -40.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1131        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009755811 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.07       |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.391       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 1.85        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 11.66\n",
      "Overall Average Successful Assignments: 64.67624682101145\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2392f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -196.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 3.533333333333333\n",
      "All assignments history: [3, 8, 6, 8, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -187     |\n",
      "| time/              |          |\n",
      "|    fps             | 108      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.666666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008310031 |\n",
      "|    clip_fraction        | 0.0759      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.307      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.12        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 17          |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.11111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -189        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008883057 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.158      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.13        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    value_loss           | 13          |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.733333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -189        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007819099 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | -0.0347     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.03        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    value_loss           | 10.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.72\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008980414 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0347      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.23        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 9.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.477777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009288812 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0579      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.81        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    value_loss           | 8.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009917567 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0555      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    value_loss           | 6.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.891666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010324748 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0557      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 6.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.12592592592593\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011678368 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.0579      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.37        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 5.66        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.473333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010193871 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.0739      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.985       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 5.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.86060606060606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012577679 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.0923      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.586       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 5.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.03333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014799238 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.76        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 4.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.54871794871795\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013650143 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.66        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 4.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.17142857142857\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013814667 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.17        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 4.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.54222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013171003 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.682       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 4.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.65\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013437843 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.78        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 3.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.24313725490196\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 173         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011846866 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.67        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 3.93        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.766666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 184         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011122798 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.12        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 3.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.164912280701756\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012149276 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.15        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 3.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.586666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011848924 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.581       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 3.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.37460317460317\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012065045 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.579       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.5         |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 3.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.051515151515154\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011740846 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.398       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 4.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.571014492753626\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -183         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 234          |\n",
      "|    total_timesteps      | 23552        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108235665 |\n",
      "|    clip_fraction        | 0.203        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.42        |\n",
      "|    explained_variance   | 0.609        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.17         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0532      |\n",
      "|    value_loss           | 3.86         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.03888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010308804 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.637       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0507      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 3.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.66133333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 255         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011276966 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.638       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.442       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 3.88        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.15897435897436\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -182         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 265          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104198605 |\n",
      "|    clip_fraction        | 0.179        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.4         |\n",
      "|    explained_variance   | 0.666        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.957        |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0507      |\n",
      "|    value_loss           | 3.57         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.590123456790124\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009402027 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.4        |\n",
      "|    explained_variance   | 0.663       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.234       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 3.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.976190476190474\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 285         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010068184 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.38       |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.74        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 3.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.44367816091954\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009034009 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.828       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 3.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.87111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011050623 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.36       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.438       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 3.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.36774193548387\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009478202 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 3.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.858333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010483468 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.34       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.34949494949495\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 333         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010132497 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.32       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.41        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0567     |\n",
      "|    value_loss           | 3.29        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.825490196078434\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009490933 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.29       |\n",
      "|    explained_variance   | 0.721       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.896       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 3.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.2552380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 353         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010048101 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.27       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.658       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.71666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 362         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009357823 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.23       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.23603603603603\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 370         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010129759 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.21       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.665       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.69298245614035\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 379         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009207794 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.18       |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.10427350427351\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 388         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009748302 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.14       |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.57666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 397         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010162672 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.11       |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.697       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.011382113821135\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 406         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009376997 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.07       |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.726       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.51111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 415         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009444448 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.03       |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.843       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.97984496124031\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 424         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010165568 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.99       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.91        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.48030303030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 432         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010406278 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.94       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.544       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.92148148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -162        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 441         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009860187 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.89       |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.816       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.32608695652174\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 450         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008913304 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.84       |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.71        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.81418439716312\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -158        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 460         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009477914 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.81       |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.611       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.31111111111111\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -155       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 104        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 468        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00811671 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.76      |\n",
      "|    explained_variance   | 0.703      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.84       |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0492    |\n",
      "|    value_loss           | 2.68       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.81224489795918\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -153        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 477         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009240454 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.71       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.428       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.28133333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -151        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 486         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009497992 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.68       |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.427       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.75424836601307\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -149        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 495         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008521011 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.62       |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.433       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.15897435897436\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -147        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009210592 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.57       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.653       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.69937106918239\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -145        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 512         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009891084 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.51       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.548       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.25802469135803\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -143        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 521         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008193584 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.46       |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.692       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.75636363636363\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -140        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 530         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008806382 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.43       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.557       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.23928571428571\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -138        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 539         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009235072 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.39       |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.433       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.70292397660819\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -136        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 549         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009689997 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.664       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 2.16        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.17241379310344\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -133        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 557         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008713843 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.639       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.61468926553673\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -131        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 566         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008081376 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.29       |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.08666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -128        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 574         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008190675 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.2        |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.5344262295082\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -125        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 583         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009728134 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.16       |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.745       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.00752688172042\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -122       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 592        |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00852049 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.12      |\n",
      "|    explained_variance   | 0.622      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.726      |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.0508    |\n",
      "|    value_loss           | 2.69       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.47724867724868\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -119        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 601         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007991822 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.07       |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.86        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.049      |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.940625\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -116         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 107          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 610          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075600836 |\n",
      "|    clip_fraction        | 0.138        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.01        |\n",
      "|    explained_variance   | 0.632        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.815        |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.0495      |\n",
      "|    value_loss           | 2.95         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.39897435897436\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -114        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 619         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008956246 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.96       |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.687       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.87878787878788\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -111       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 66         |\n",
      "|    time_elapsed         | 627        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00814647 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.92      |\n",
      "|    explained_variance   | 0.652      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.948      |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | -0.05      |\n",
      "|    value_loss           | 2.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.37512437810945\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -108        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 636         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007979656 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.86       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.682       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.86274509803921\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -105        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 645         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008166252 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.81       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.464       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.34106280193237\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -102        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 653         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009385799 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.78       |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.339       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0482     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.83047619047619\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -99.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 662         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008857196 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.73       |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.434       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.30798122065728\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -96.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 671         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010289114 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.66       |\n",
      "|    explained_variance   | 0.561       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.942       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.80462962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -92.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 681         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008526649 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.62       |\n",
      "|    explained_variance   | 0.621       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.591       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.30593607305936\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -89         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 690         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008527146 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.55       |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.954       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.7990990990991\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -86         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 699         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008679923 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.5        |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.803       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.30488888888888\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -82.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 708         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007925849 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.44       |\n",
      "|    explained_variance   | 0.607       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.608       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.80877192982456\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -79.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 717         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008318917 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.39       |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.773       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.31688311688312\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -76.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 108          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 726          |\n",
      "|    total_timesteps      | 78848        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088192625 |\n",
      "|    clip_fraction        | 0.182        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.35        |\n",
      "|    explained_variance   | 0.6          |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.736        |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0498      |\n",
      "|    value_loss           | 2.51         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.81282051282051\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -73.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 735         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008273278 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.604       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.877       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0493     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.2987341772152\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -70.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 744         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009646783 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.28       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.606       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0498     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.79333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -67.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 752         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010097046 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.712       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.27160493827161\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -64.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 761         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008189213 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.2        |\n",
      "|    explained_variance   | 0.603       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.824       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.73821138211382\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -61.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 770        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00855588 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.19      |\n",
      "|    explained_variance   | 0.609      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.654      |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.0479    |\n",
      "|    value_loss           | 2.12       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.20803212851406\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -58.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 780         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008730574 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.17       |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0487     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.65396825396826\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -56.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 788         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009572543 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.16       |\n",
      "|    explained_variance   | 0.629       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.497       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.1043137254902\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -53.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 797         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008950824 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.1        |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.88        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.54496124031007\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -51.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 807         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008297114 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.05       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0493     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.99003831417625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -49         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 816         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009727628 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.623       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.599       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 1.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.43560606060606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -46.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 825         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009167999 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.03       |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.714       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.87715355805244\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -44.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 834         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010636405 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.742       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.31185185185186\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -41.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 843         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009701011 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.605       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.658       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 2           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.74065934065933\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -39.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 852         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009777641 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4          |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.587       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.17028985507247\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -37.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 861         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009116725 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.96       |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.5720430107527\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -36.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 870         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010234876 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.94       |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.825       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0493     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.98439716312056\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -34.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 878         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009458566 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.92       |\n",
      "|    explained_variance   | 0.566       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.582       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 2.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.41543859649123\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -32.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 887         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008664683 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.88       |\n",
      "|    explained_variance   | 0.56        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.751       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0493     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.8125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -30.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 895         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007975608 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.85       |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.618       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0458     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.20687285223367\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -29.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 904         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009452831 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.88       |\n",
      "|    explained_variance   | 0.501       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.923       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0481     |\n",
      "|    value_loss           | 2.16        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.6047619047619\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -27.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 98         |\n",
      "|    time_elapsed         | 912        |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01052642 |\n",
      "|    clip_fraction        | 0.226      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -3.87      |\n",
      "|    explained_variance   | 0.533      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.659      |\n",
      "|    n_updates            | 970        |\n",
      "|    policy_gradient_loss | -0.0549    |\n",
      "|    value_loss           | 1.96       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.00740740740741\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -26         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 922         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008863101 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.84       |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.582       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.39466666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -25         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 931         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008989862 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -3.8        |\n",
      "|    explained_variance   | 0.605       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.667       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0487     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 19.54\n",
      "Overall Average Successful Assignments: 62.57758094256751\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c16ed504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 9.4\n",
      "All assignments history: [9, 8, 3, 6, 5, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -188     |\n",
      "| time/              |          |\n",
      "|    fps             | 123      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.433333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008295934 |\n",
      "|    clip_fraction        | 0.079       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | -0.0228     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.41        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0456     |\n",
      "|    value_loss           | 14.1        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.533333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -188       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 117        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00892974 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.51      |\n",
      "|    explained_variance   | -0.0338    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.46       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0454    |\n",
      "|    value_loss           | 10.2       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -152.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.766666666666667\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -187         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 116          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080209775 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.5         |\n",
      "|    explained_variance   | 0.0491       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.29         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0405      |\n",
      "|    value_loss           | 9.55         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.266666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008967842 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0605      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.28        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    value_loss           | 8.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.3\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009018229 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.0748      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.15        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    value_loss           | 7.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.771428571428572\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -187       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 115        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 62         |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01040746 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.49      |\n",
      "|    explained_variance   | 0.0879     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.88       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    value_loss           | 6.7        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.9\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010713112 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.0923      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.93        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0448     |\n",
      "|    value_loss           | 5.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.15555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011795856 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.85        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 5.45        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.63333333333333\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -186      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 113       |\n",
      "|    iterations           | 10        |\n",
      "|    time_elapsed         | 90        |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0136586 |\n",
      "|    clip_fraction        | 0.271     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -6.47     |\n",
      "|    explained_variance   | 0.117     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.18      |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | -0.0516   |\n",
      "|    value_loss           | 4.97      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.75151515151515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012713274 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    value_loss           | 4.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.516666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014613664 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 4.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.66153846153846\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014388542 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.249       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.76        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 4.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.86190476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 127         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012674041 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    value_loss           | 4.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.373333333333335\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -185         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 112          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 136          |\n",
      "|    total_timesteps      | 15360        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114454245 |\n",
      "|    clip_fraction        | 0.209        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.45        |\n",
      "|    explained_variance   | 0.435        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.02         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0479      |\n",
      "|    value_loss           | 3.97         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.65416666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010956647 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 3.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.745098039215684\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009687119 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    value_loss           | 3.6         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.611111111111114\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 162        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00932036 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.44      |\n",
      "|    explained_variance   | 0.57       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.838      |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0473    |\n",
      "|    value_loss           | 3.78       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.44210526315789\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009544128 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.594       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 3.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.193333333333335\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 114        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 179        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01028024 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.42      |\n",
      "|    explained_variance   | 0.624      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.905      |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0515    |\n",
      "|    value_loss           | 3.39       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.87936507936508\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 188         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009730159 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.967       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 3.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.53939393939394\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009065781 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 3.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.18840579710145\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008734873 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.698       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 3.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.81666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 214         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009388494 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.38       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.562666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010589166 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.705       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 3.12        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.251282051282054\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -180         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 114          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 232          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085769165 |\n",
      "|    clip_fraction        | 0.152        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.35        |\n",
      "|    explained_variance   | 0.72         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.19         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.05        |\n",
      "|    value_loss           | 2.87         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.906172839506176\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008451785 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.34       |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.049      |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.56428571428572\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009383838 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.33       |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.566       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.15862068965517\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 263         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009153646 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.31       |\n",
      "|    explained_variance   | 0.755       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.17        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.7\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010022354 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.29       |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.595       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.20215053763441\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -177         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 112          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 281          |\n",
      "|    total_timesteps      | 31744        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096695265 |\n",
      "|    clip_fraction        | 0.179        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.27        |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.353        |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0539      |\n",
      "|    value_loss           | 2.45         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.6875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 290         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008823019 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.25       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.523       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.121212121212125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 298         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009328254 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.22       |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.854       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.55686274509804\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 308         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009147128 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.2        |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.787       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.99809523809524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 317         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009588574 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.18       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.382       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.416666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 327         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010998458 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.15       |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.757       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.821621621621624\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -169       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 344        |\n",
      "|    total_timesteps      | 37888      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00984196 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.12      |\n",
      "|    explained_variance   | 0.81       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.244      |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0548    |\n",
      "|    value_loss           | 1.9        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.20175438596492\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 362         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010674735 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.07       |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.688       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.57094017094018\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 371         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010864656 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.03       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.501       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.94833333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -165       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 379        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01025873 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.97      |\n",
      "|    explained_variance   | 0.793      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.642      |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0613    |\n",
      "|    value_loss           | 1.98       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.34634146341463\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 387         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009412149 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.92       |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.77        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.70952380952382\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -162        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 395         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009454468 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.86       |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.508       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.06356589147286\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 404         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010165435 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.81       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.774       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.4030303030303\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -158         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 109          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 412          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108767655 |\n",
      "|    clip_fraction        | 0.183        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.76        |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.361        |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.0568      |\n",
      "|    value_loss           | 1.8          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.76296296296296\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -156        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 420         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008763596 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.67       |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.665       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0587     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.10434782608695\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -153        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 428         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009562893 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.62       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.31        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.47659574468085\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -151        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 437         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009746231 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.55       |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.758       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.84583333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -148        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 445         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010384233 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.46       |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.785       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.21768707482993\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -145        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 454         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010210564 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.42       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.389       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.568\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 462         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009073956 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.318       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.90849673202614\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -139        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 472         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010101098 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.29       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.619       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 1.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.25\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -136        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 482         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010090817 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.25       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.287       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.5937106918239\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -133        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 491         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009589339 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.2        |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.773       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.89876543209877\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -131        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 500         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008789849 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.17       |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.741       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.24606060606061\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -127        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 511         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009191183 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.18       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.647       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 1.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.53333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -125        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 520         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008931121 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.12       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.541       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.84093567251462\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -122        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 530         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009466583 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.1        |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.211       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.1632183908046\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -119        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 541         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009415188 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.05       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.705       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.46440677966102\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -116         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 109          |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 550          |\n",
      "|    total_timesteps      | 60416        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073286532 |\n",
      "|    clip_fraction        | 0.132        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.99        |\n",
      "|    explained_variance   | 0.664        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.672        |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0481      |\n",
      "|    value_loss           | 2.53         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.77111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -113        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 559         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009657064 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.94       |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.335       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.04153005464481\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -110        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 567         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009738099 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.92       |\n",
      "|    explained_variance   | 0.624       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.563       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.38387096774194\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -107        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 577         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010553818 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.9        |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.766       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.68148148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -105        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 586         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009226913 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.9        |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.403       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 1.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.97291666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -102        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 596         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008858487 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.84       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.663       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 1.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.26871794871795\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -99.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 605         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008384937 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.79       |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.66        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.5989898989899\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -96.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 614         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009179769 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.78       |\n",
      "|    explained_variance   | 0.609       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.583       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.94626865671641\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -94.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 622         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008529841 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.75       |\n",
      "|    explained_variance   | 0.638       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.861       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.29901960784314\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -92.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 631         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008860033 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.76       |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.526       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.64734299516908\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -90.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 640         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009175195 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.71       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.699       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.94952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -88.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 649         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007817747 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.7        |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.422       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 1.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.28262910798122\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -86.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 110          |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 658          |\n",
      "|    total_timesteps      | 72704        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076340064 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.7         |\n",
      "|    explained_variance   | 0.602        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.574        |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.0472      |\n",
      "|    value_loss           | 2.05         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.61018518518519\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -84.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 667         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009467338 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.72       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.568       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.93424657534247\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -83         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 676         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009077145 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.68       |\n",
      "|    explained_variance   | 0.621       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.61        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.23243243243243\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -81.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 684         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009948327 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.66       |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.484       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.552\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -79.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 694         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008857014 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.61       |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.601       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.86491228070176\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -77.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 703         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010259294 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.59       |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.611       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.14805194805194\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -76.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 711         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010847164 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.57       |\n",
      "|    explained_variance   | 0.607       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.495       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.42051282051283\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -75.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 719         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008977719 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.53       |\n",
      "|    explained_variance   | 0.545       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.806       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.71561181434599\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -73.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 727         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009327127 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.53       |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.265       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 1.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.00083333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -72.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 735         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009230336 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.51       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.637       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 1.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.27818930041153\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -71         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 743         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010001024 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.5        |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.49        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.5739837398374\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -70         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 752         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010326356 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.5        |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.451       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.87228915662651\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -68         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 760         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010821266 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.48       |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.591       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.14444444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -67         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 768         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008403671 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.45       |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.866       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.4321568627451\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -65.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 776         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010528869 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.46       |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.577       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.7139534883721\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -64.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 785         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009429515 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.47       |\n",
      "|    explained_variance   | 0.597       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.641       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.98007662835249\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -63.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 793         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009520777 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.49       |\n",
      "|    explained_variance   | 0.604       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.809       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.26287878787879\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -63        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 88         |\n",
      "|    time_elapsed         | 801        |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00837817 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.5       |\n",
      "|    explained_variance   | 0.568      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.92       |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | -0.0493    |\n",
      "|    value_loss           | 2.35       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.53558052434457\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -62.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 809         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008572399 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.47       |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.699       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.81111111111112\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -61         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 817         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008646395 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.44       |\n",
      "|    explained_variance   | 0.602       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.834       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.07985347985348\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -60.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 826         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008506613 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.4        |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.876       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.37101449275362\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -58.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 834         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009917614 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.748       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.64014336917563\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -58.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 842         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010298353 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.608       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.842       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.9049645390071\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -56.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 113          |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 850          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101391245 |\n",
      "|    clip_fraction        | 0.213        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.4         |\n",
      "|    explained_variance   | 0.573        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.843        |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.053       |\n",
      "|    value_loss           | 2.52         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.14947368421052\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -56.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 859         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009477222 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.775       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.05       |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.41527777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -55.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 867         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009928349 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.821       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.68109965635739\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -54.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 875         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010142819 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.39       |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.912       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.95782312925171\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -53.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 98         |\n",
      "|    time_elapsed         | 883        |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01044531 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -4.39      |\n",
      "|    explained_variance   | 0.606      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.776      |\n",
      "|    n_updates            | 970        |\n",
      "|    policy_gradient_loss | -0.0537    |\n",
      "|    value_loss           | 2.02       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.23434343434343\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -52.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 891         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009214539 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.585       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.562       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0492     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.51866666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -51.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 899         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008379083 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.4        |\n",
      "|    explained_variance   | 0.56        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.909       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 9.12\n",
      "Overall Average Successful Assignments: 64.45339415530923\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5465dbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -190.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 5.2\n",
      "All assignments history: [3, 7, 10, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -189     |\n",
      "| time/              |          |\n",
      "|    fps             | 140      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 7.666666666666667\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -188         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078055803 |\n",
      "|    clip_fraction        | 0.0668       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.51        |\n",
      "|    explained_variance   | -0.0526      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.96         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0452      |\n",
      "|    value_loss           | 17.9         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.088888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -189        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008630132 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | 0.0338      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.766666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007970974 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0464      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.31        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    value_loss           | 10.1        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.293333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008927486 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.5        |\n",
      "|    explained_variance   | 0.0728      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.25        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    value_loss           | 8.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.3\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009993939 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    value_loss           | 7.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.676190476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010167882 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.43        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    value_loss           | 6.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.925\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009789298 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    value_loss           | 6.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.659259259259258\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011395293 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.13        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    value_loss           | 5.31        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.913333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010949783 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.72        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 5.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -134.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.581818181818182\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012206845 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    value_loss           | 5.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.727777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013621549 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.194       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    value_loss           | 4.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -122.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.512820512820515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017348364 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.69        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 4.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.78095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014928485 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 4.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.204444444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012968821 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.81        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0482     |\n",
      "|    value_loss           | 4.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.479166666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014205534 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.586       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 4.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.858823529411765\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 136          |\n",
      "|    total_timesteps      | 17408        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113603715 |\n",
      "|    clip_fraction        | 0.208        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.47        |\n",
      "|    explained_variance   | 0.411        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.75         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0494      |\n",
      "|    value_loss           | 4.46         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.144444444444446\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -186      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 127       |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 144       |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0105574 |\n",
      "|    clip_fraction        | 0.175     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -6.47     |\n",
      "|    explained_variance   | 0.441     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.915     |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.0472   |\n",
      "|    value_loss           | 4.47      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.203508771929826\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -186       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 153        |\n",
      "|    total_timesteps      | 19456      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01044101 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.47      |\n",
      "|    explained_variance   | 0.465      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.53       |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0471    |\n",
      "|    value_loss           | 4.46       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.066666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010879609 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.709       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 4.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.723809523809525\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011426304 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.506       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 4.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.727272727272727\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011454992 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0515     |\n",
      "|    value_loss           | 4.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.930434782608696\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010401257 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.798       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 4.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.147222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 195         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010019686 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.751       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 4.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.538666666666664\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -184         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 125          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 204          |\n",
      "|    total_timesteps      | 25600        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098750945 |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.46        |\n",
      "|    explained_variance   | 0.555        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.22         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0496      |\n",
      "|    value_loss           | 4.41         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.9025641025641\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009868838 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.57        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 4.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.175308641975306\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 222         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009862291 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.568       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 4.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.319047619047616\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 230         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009748999 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.568       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0492     |\n",
      "|    value_loss           | 4.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.38850574712644\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 239         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009010764 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.44       |\n",
      "|    explained_variance   | 0.581       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.791       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 4.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.45777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 248         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009838508 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.604       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 3.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.46236559139785\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008519083 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.43       |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 3.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.4375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 265         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008037557 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.604       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    value_loss           | 4.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.56363636363636\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 273         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008081228 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 3.94        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.63725490196079\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -180         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 123          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 281          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073620686 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.4         |\n",
      "|    explained_variance   | 0.629        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.0461      |\n",
      "|    value_loss           | 3.84         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.653333333333336\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 123        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 290        |\n",
      "|    total_timesteps      | 35840      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00835054 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -6.39      |\n",
      "|    explained_variance   | 0.633      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.52       |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0491    |\n",
      "|    value_loss           | 3.78       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.60740740740741\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 298         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009370655 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.38       |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.327       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 3.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.49189189189189\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -178         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 123          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 306          |\n",
      "|    total_timesteps      | 37888        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095777875 |\n",
      "|    clip_fraction        | 0.17         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -6.36        |\n",
      "|    explained_variance   | 0.633        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.46         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.0517      |\n",
      "|    value_loss           | 3.67         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.421052631578945\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009085238 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.33        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 3.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.2991452991453\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 323         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010040432 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.33       |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 3.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.108333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 331         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010412071 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.31       |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.682       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 3.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.87479674796748\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008304672 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.3        |\n",
      "|    explained_variance   | 0.634       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.962       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.05       |\n",
      "|    value_loss           | 3.71        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.56666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 347         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009734672 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.27       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 3.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.248062015503876\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010056954 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.24       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 3.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.95909090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 364         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010085724 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.21       |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 3.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.6162962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 372         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010305228 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.18       |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.906       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 3.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.26376811594203\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 381         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010007862 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.15       |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 3.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.93617021276596\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 389         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009575555 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.13       |\n",
      "|    explained_variance   | 0.607       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 3.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.55\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 397         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009389411 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.1        |\n",
      "|    explained_variance   | 0.607       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 3.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.202721088435375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 405         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009240963 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.07       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.994       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 3.34        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.836\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 413         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010388495 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -6.03       |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.799       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 3.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.47320261437908\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -162        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 422         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010307781 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.99       |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.791       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 3.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.1025641025641\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 430         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011157137 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.96       |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 3.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.704402515723274\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -158        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 438         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009955557 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.93       |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.881       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.309876543209874\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -157        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 446         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009436486 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.88       |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.499       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.92363636363636\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -155        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009298824 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.84       |\n",
      "|    explained_variance   | 0.57        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.776       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.588095238095235\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -153        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009866028 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.8        |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.854       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 3.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.22456140350877\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -151        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009454876 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.76       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.33        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0565     |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.88045977011494\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -149        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 479         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009565886 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.75       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.638       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.52090395480226\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -147        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 488         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009642502 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.69       |\n",
      "|    explained_variance   | 0.539       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.156666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -145        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 496         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009876104 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.65       |\n",
      "|    explained_variance   | 0.541       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.79125683060109\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -143        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010246839 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.59       |\n",
      "|    explained_variance   | 0.504       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.959       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 3.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.43225806451613\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -140        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 513         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010210409 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.53       |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.044444444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -137        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 521         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009907075 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.49       |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.947       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.63125\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -135       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 123        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 529        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00918472 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.43      |\n",
      "|    explained_variance   | 0.468      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.38       |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0568    |\n",
      "|    value_loss           | 3.46       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.21538461538462\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -132       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 123        |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 537        |\n",
      "|    total_timesteps      | 66560      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00962797 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -5.38      |\n",
      "|    explained_variance   | 0.501      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.48       |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0553    |\n",
      "|    value_loss           | 3.06       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.77474747474747\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -130        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 545         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010209253 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.33930348258707\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -127        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 553         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009640224 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.31       |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.625       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.9235294117647\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -124        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 562         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010648883 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.26       |\n",
      "|    explained_variance   | 0.504       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.48599033816426\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -122        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 570         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008538282 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.18       |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.06952380952382\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -119        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 578         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009236479 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.13       |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.992       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.63380281690141\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -117        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 586         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008989077 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -5.08       |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.343       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.20277777777778\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -114         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 123          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 594          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097510535 |\n",
      "|    clip_fraction        | 0.222        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -5.02        |\n",
      "|    explained_variance   | 0.534        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.894        |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.0593      |\n",
      "|    value_loss           | 2.65         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.78356164383561\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -111        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 602         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009238926 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.98       |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.912       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.37477477477478\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -108        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 611         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010412553 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.96       |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.845       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.93511111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -105        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 619         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009647086 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.792       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.48947368421052\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -103        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 627         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008466182 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.85       |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.04329004329004\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -99.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 635         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009405954 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.83       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.943       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.62222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -96.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 643         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009871873 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.79       |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.782       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.13502109704642\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -94.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 652         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010348331 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.78       |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.689       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.6675\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -91.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 660         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009499811 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.74       |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0569     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.18765432098765\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -88.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 668         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008681273 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.69       |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.562       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.70243902439024\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -85.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 675         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009900115 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.62       |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.467       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.23212851405623\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -82.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 682         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010212762 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.58       |\n",
      "|    explained_variance   | 0.466       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.745       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.7452380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -79.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 689         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009506203 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.54       |\n",
      "|    explained_variance   | 0.476       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.697       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.23372549019608\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -77.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 696         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009005028 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.49       |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.781       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.75193798449612\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -74.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 125          |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 704          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096902605 |\n",
      "|    clip_fraction        | 0.205        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -4.46        |\n",
      "|    explained_variance   | 0.488        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.825        |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.0545      |\n",
      "|    value_loss           | 2.44         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.2360153256705\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -72         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 710         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009796329 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.46       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.955       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.7090909090909\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -69.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 718         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009429409 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.41       |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.834       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.1932584269663\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -66.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 725         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009490648 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.36       |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.855       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.67481481481481\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -63.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 732         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009564696 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.28       |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.86        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.15457875457875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -60.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 740         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010593975 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.543       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.61014492753623\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -58.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 747         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010649715 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.22       |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.75        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.08100358422939\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -56.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 754         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009623687 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.21       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.796       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.53971631205674\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -54.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 761         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011492182 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.22       |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.99087719298245\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -51.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 768         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010846492 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.18       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.924       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0573     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.43402777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -49.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 775         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009946339 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.17       |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.88041237113401\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -47.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 782         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009590281 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.12       |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.3251700680272\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -45.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 788         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010212144 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.08       |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.834       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.76498316498316\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -42.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 795         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009573715 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.04       |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.98        |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.198\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -40.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 801         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009836305 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -4.02       |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.897       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 10.34\n",
      "Overall Average Successful Assignments: 51.25220674493513\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate',\n",
    "    'Min_Eligibility': 'MinEligibility'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "        num_features = tasks.shape[1]\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(num_features,), dtype=np.float32)\n",
    "        self.eligible_vehicle_indices = []\n",
    "        self.update_action_space()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]        \n",
    "        \n",
    "    def update_action_space(self):\n",
    "        task_eligibility = self.tasks.iloc[self.current_task]['MinEligibility']\n",
    "        eligible_vehicles = self.vehicles[self.vehicles['Eligible'] >= task_eligibility]\n",
    "        self.eligible_vehicle_indices = eligible_vehicles.index.tolist()\n",
    "        if len(self.eligible_vehicle_indices) == 0:\n",
    "            self.action_space = spaces.Discrete(1)  # Prevents invalid action space of size 0\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(len(self.eligible_vehicle_indices))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.update_action_space()\n",
    "        #print(\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            self.update_action_space()\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        \n",
    "        \n",
    "        #three lines below can be uncommented for more detailed output\n",
    "        #print(f\"Task Details: {task.to_dict()}\")\n",
    "        #print(f\"Vehicle Details: {vehicle.to_dict()}\")\n",
    "        #print(f\"Step: Task {self.current_task}, Action {action}, Reward {reward}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "\n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7585f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
