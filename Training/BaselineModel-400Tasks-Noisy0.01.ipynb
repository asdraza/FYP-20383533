{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccbd4362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -384.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 9.666666666666666\n",
      "All assignments history: [21, 15, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -364     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -290.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.208333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008452786 |\n",
      "|    clip_fraction        | 0.0572      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.392      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.83        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 20.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -204.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.666666666666664\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -365         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 148          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0121161975 |\n",
      "|    clip_fraction        | 0.183        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | -0.0178      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.04         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0606      |\n",
      "|    value_loss           | 13.9         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -360.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.479166666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013572001 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.049       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.89        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0624     |\n",
      "|    value_loss           | 11          |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -334.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.4\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014125632 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0123      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.01        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 9.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.111111111111114\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015602806 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0268      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.87        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 8.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -200.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.30952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016597547 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.0117     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0615     |\n",
      "|    value_loss           | 7.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -336.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017534494 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.000783   |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.81        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 6.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -344.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.28703703703704\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018275179 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.00819     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 5.29        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -320.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.0\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019808877 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00504     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.723       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0621     |\n",
      "|    value_loss           | 4.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -368.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.21212121212121\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019770492 |\n",
      "|    clip_fraction        | 0.433       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | -0.000239   |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.331       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0587     |\n",
      "|    value_loss           | 4.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -386.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.201388888888886\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 117        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 104        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02241905 |\n",
      "|    clip_fraction        | 0.505      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | -0.00242   |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0253     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0667    |\n",
      "|    value_loss           | 3.36       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -382.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.84615384615385\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023006912 |\n",
      "|    clip_fraction        | 0.502       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | -0.00117    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.599       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -306.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.101190476190474\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -369       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 127        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02543999 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.00419    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.687      |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0671    |\n",
      "|    value_loss           | 2.79       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -340.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.28333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027705377 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00621     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.183      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -350.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.322916666666664\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 149        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02476724 |\n",
      "|    clip_fraction        | 0.544      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.0162     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.57       |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0577    |\n",
      "|    value_loss           | 2.94       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -316.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.333333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028290145 |\n",
      "|    clip_fraction        | 0.594       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0421      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -308.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.541666666666664\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 171        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02493783 |\n",
      "|    clip_fraction        | 0.557      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.103      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.389     |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0635    |\n",
      "|    value_loss           | 2.83       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -236.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.280701754385966\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 182         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024650192 |\n",
      "|    clip_fraction        | 0.516       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.287      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -216.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.229166666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 193         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022695728 |\n",
      "|    clip_fraction        | 0.452       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.131       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -204.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.273809523809526\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020432064 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.302      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -194.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.28030303030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021506835 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0604     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -166.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.666666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019101568 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0906     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.81597222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 239         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020719554 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.314       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.96666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 251         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021087663 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.949       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.68269230769231\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 262         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020913351 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.354       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.55555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 274         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018203009 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.648809523809526\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 285         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016464043 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.382       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.98        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0567     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.42816091954023\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 296         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016710516 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.495       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.20277777777778\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 99         |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 308        |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01773422 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.375      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.152     |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.0599    |\n",
      "|    value_loss           | 2.53       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.98118279569893\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 99         |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 320        |\n",
      "|    total_timesteps      | 31744      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01672404 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.378      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.27       |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.063     |\n",
      "|    value_loss           | 2.83       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.47395833333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 332         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018793646 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.68939393939394\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018078946 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.239      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.58823529411765\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 354         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013893138 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.947       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.69761904761904\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 366        |\n",
      "|    total_timesteps      | 35840      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01576361 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.43       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.94       |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0572    |\n",
      "|    value_loss           | 2.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.56018518518519\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 379         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016578592 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0229     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.52927927927928\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 391         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016110959 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.02        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.41008771929825\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 403         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015977876 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0205      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.16025641025641\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 415         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013432721 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.532       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.82291666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 426         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014955146 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0652     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 91.53861788617886\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 438         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013294859 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 93.15674603174604\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 449         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015000613 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 94.74612403100775\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 462         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016108872 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.472       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.129       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 95.94507575757575\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 475         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014407938 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.852       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0579     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.3537037037037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 487         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014075861 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.78260869565217\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 500         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014074129 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.51        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 100.18439716312056\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 93         |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 512        |\n",
      "|    total_timesteps      | 48128      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01575045 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.512      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.675      |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0593    |\n",
      "|    value_loss           | 2.35       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 101.43923611111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 524         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014510967 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.335       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 102.64455782312925\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 537         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013376299 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00536    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 103.86833333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 549         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014912229 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.936       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0593     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 105.02941176470588\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -370      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 93        |\n",
      "|    iterations           | 51        |\n",
      "|    time_elapsed         | 560       |\n",
      "|    total_timesteps      | 52224     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0150967 |\n",
      "|    clip_fraction        | 0.285     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.23     |\n",
      "|    explained_variance   | 0.527     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | -0.373    |\n",
      "|    n_updates            | 500       |\n",
      "|    policy_gradient_loss | -0.0616   |\n",
      "|    value_loss           | 2.26      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.09615384615384\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -370         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 93           |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 572          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0138282105 |\n",
      "|    clip_fraction        | 0.239        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.23        |\n",
      "|    explained_variance   | 0.533        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.883        |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.0562      |\n",
      "|    value_loss           | 2.37         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 107.12421383647799\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 92         |\n",
      "|    iterations           | 53         |\n",
      "|    time_elapsed         | 583        |\n",
      "|    total_timesteps      | 54272      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01608251 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.539      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.265      |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0606    |\n",
      "|    value_loss           | 2.29       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.17746913580247\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 596         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013656786 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0604     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.18636363636364\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 608         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015228025 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.563       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.15625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 620         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014181684 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0359     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0591     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.99853801169591\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 631         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014121214 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1           |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 643         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014634388 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.549       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.759       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 112.75706214689265\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -369         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 91           |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 657          |\n",
      "|    total_timesteps      | 60416        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0134666795 |\n",
      "|    clip_fraction        | 0.244        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.21        |\n",
      "|    explained_variance   | 0.546        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.00817      |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0586      |\n",
      "|    value_loss           | 2.12         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.57222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 670         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014321551 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.523       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.37        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0628     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 114.47814207650273\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -369         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 91           |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 683          |\n",
      "|    total_timesteps      | 62464        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0148340305 |\n",
      "|    clip_fraction        | 0.259        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.2         |\n",
      "|    explained_variance   | 0.572        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.2          |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.0591      |\n",
      "|    value_loss           | 2.1          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 115.36827956989248\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 696         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015993897 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.27        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0631     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.23941798941799\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 709         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014061332 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.199      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0616     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 117.1171875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 723         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015367426 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.609       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 117.92435897435898\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 737         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015073923 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.553       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.909       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.68560606060606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 750         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015450563 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.394       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.47636815920399\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -367       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 90         |\n",
      "|    iterations           | 67         |\n",
      "|    time_elapsed         | 761        |\n",
      "|    total_timesteps      | 68608      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01300111 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.555      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.386      |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0601    |\n",
      "|    value_loss           | 2.21       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.24142156862744\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 773         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014952801 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.567       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0658     |\n",
      "|    value_loss           | 2.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.02777777777777\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -366       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 785        |\n",
      "|    total_timesteps      | 70656      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01492515 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.16      |\n",
      "|    explained_variance   | 0.571      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.412      |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0628    |\n",
      "|    value_loss           | 2.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.76666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -366       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 797        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01582183 |\n",
      "|    clip_fraction        | 0.295      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.15      |\n",
      "|    explained_variance   | 0.561      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.418      |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.0671    |\n",
      "|    value_loss           | 2.25       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.45422535211267\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 810         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014117492 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.558       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0337      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.13078703703704\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -365       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 822        |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01545864 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.14      |\n",
      "|    explained_variance   | 0.582      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.386      |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.0642    |\n",
      "|    value_loss           | 2.05       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.81849315068493\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -365         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 89           |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 835          |\n",
      "|    total_timesteps      | 74752        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0146957245 |\n",
      "|    clip_fraction        | 0.286        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.14        |\n",
      "|    explained_variance   | 0.562        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.592        |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0669      |\n",
      "|    value_loss           | 2.18         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.509009009009\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 847         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014456153 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.55        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.807       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.06333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 858         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015459761 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.6030701754386\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 870         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016389739 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.285       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.14393939393939\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 882         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015107783 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.581       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.512       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.66880341880342\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -363       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 895        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01370378 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.1       |\n",
      "|    explained_variance   | 0.571      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.264      |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.0641    |\n",
      "|    value_loss           | 2.18       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.16666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 906         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014032079 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0636      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.64166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 918         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015677875 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.599       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.411       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.1090534979424\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 930         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014492317 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.06       |\n",
      "|    explained_variance   | 0.597       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.487       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.5701219512195\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 942         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016158968 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.04       |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.358       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.0421686746988\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 956         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016043091 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.587       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0625      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.54861111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 969         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014672231 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8          |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.94        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.16176470588235\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -358       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 85         |\n",
      "|    time_elapsed         | 981        |\n",
      "|    total_timesteps      | 87040      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01701279 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.99      |\n",
      "|    explained_variance   | 0.622      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.19       |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0725    |\n",
      "|    value_loss           | 1.97       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.84883720930233\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -358       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 994        |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01476244 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.97      |\n",
      "|    explained_variance   | 0.64       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.381      |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.0678    |\n",
      "|    value_loss           | 2          |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.49616858237547\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -357        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1005        |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015492625 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.96       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.428       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0737     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.22253787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1017        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015951993 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.279       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.93352059925093\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -355        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1030        |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016873533 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.92       |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.167       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0741     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.67777777777778\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -354      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 88        |\n",
      "|    iterations           | 90        |\n",
      "|    time_elapsed         | 1041      |\n",
      "|    total_timesteps      | 92160     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0169486 |\n",
      "|    clip_fraction        | 0.346     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -7.9      |\n",
      "|    explained_variance   | 0.586     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.192     |\n",
      "|    n_updates            | 890       |\n",
      "|    policy_gradient_loss | -0.0757   |\n",
      "|    value_loss           | 2.07      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.4835164835165\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -353        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1055        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018481834 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.88       |\n",
      "|    explained_variance   | 0.591       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0781     |\n",
      "|    value_loss           | 1.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.27717391304347\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -352        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1068        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016706605 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.84       |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.612       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0775     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.0573476702509\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -351       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 1081       |\n",
      "|    total_timesteps      | 95232      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01564281 |\n",
      "|    clip_fraction        | 0.346      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.79      |\n",
      "|    explained_variance   | 0.567      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0459     |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0765    |\n",
      "|    value_loss           | 2.16       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.84397163120568\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -350        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1094        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016175129 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.76       |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.287       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0737     |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.6236842105263\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -348        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1109        |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016553488 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.73       |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.58        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0754     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.39756944444446\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -346       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 96         |\n",
      "|    time_elapsed         | 1125       |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01726577 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.69      |\n",
      "|    explained_variance   | 0.494      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.619      |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.0785    |\n",
      "|    value_loss           | 2.43       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 139.1692439862543\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -345        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1140        |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017982222 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.65       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.58        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.079      |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.02636054421768\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -343        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1156        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016379552 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.6        |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.257       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.076      |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.88888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -341        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1170        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017105611 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.56       |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.302       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0782     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 141.73583333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -339        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1184        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015945384 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.5        |\n",
      "|    explained_variance   | 0.466       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.386       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.081      |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -68.94\n",
      "Overall Average Successful Assignments: 93.80311464777955\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adaaab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -356.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.583333333333332\n",
      "All assignments history: [15, 12, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -373     |\n",
      "| time/              |          |\n",
      "|    fps             | 91       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -354.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.833333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008593367 |\n",
      "|    clip_fraction        | 0.0608      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.256      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.99        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 16.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -354.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.22222222222222\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -372         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 79           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0111498125 |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | -0.0824      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.82         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.057       |\n",
      "|    value_loss           | 12.3         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -336.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.020833333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013588307 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0652     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0604     |\n",
      "|    value_loss           | 9.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -372.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.133333333333333\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -374         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 77           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 5120         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0138241695 |\n",
      "|    clip_fraction        | 0.242        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | 0.0224       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.24         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0581      |\n",
      "|    value_loss           | 9.27         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -378.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.5\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014680834 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0367      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.38        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0567     |\n",
      "|    value_loss           | 8.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -378.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.404761904761905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015717085 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.00391    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 6.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -372.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.84375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017567875 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0044      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 5.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.212962962962962\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 74         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 123        |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01822649 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.00225    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.32       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0608    |\n",
      "|    value_loss           | 5.09       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.7\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018469337 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.00228     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.732       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 4.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.85606060606061\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020468554 |\n",
      "|    clip_fraction        | 0.447       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.000423    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 3.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -136.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.479166666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021220336 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | -0.00133    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.105       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.12179487179487\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025026433 |\n",
      "|    clip_fraction        | 0.561       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00148     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.248      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -198.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.25\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023031581 |\n",
      "|    clip_fraction        | 0.525       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00147     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.357      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.68333333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 74         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 205        |\n",
      "|    total_timesteps      | 15360      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02816537 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.000218   |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.857      |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0679    |\n",
      "|    value_loss           | 2.89       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.578125\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -371      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 219       |\n",
      "|    total_timesteps      | 16384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0287018 |\n",
      "|    clip_fraction        | 0.615     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.27     |\n",
      "|    explained_variance   | 0.00441   |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.69      |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 3.02      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.04411764705883\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029444318 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0297      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.482       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.35648148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 246         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026906352 |\n",
      "|    clip_fraction        | 0.551       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0909      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.558       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.64473684210526\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 261         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026480267 |\n",
      "|    clip_fraction        | 0.532       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.335       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.58333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 74         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 275        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02240815 |\n",
      "|    clip_fraction        | 0.462      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.155      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.29       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0565    |\n",
      "|    value_loss           | 2.82       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.25396825396825\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 292         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027400602 |\n",
      "|    clip_fraction        | 0.52        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0628     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.5530303030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025086507 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 92.7572463768116\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 320         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026183838 |\n",
      "|    clip_fraction        | 0.478       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.152      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 94.51041666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 335        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02505777 |\n",
      "|    clip_fraction        | 0.52       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.217      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.662      |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0622    |\n",
      "|    value_loss           | 2.91       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 96.08333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019939475 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.72        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 3.01        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.88782051282051\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 365         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020142736 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.95        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.48148148148148\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 72         |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 380        |\n",
      "|    total_timesteps      | 27648      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02417868 |\n",
      "|    clip_fraction        | 0.474      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.249      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.375     |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0621    |\n",
      "|    value_loss           | 2.9        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 100.88988095238095\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 72         |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 395        |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01884527 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.261      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.175      |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0561    |\n",
      "|    value_loss           | 2.8        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 102.01724137931035\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 409         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021881472 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 103.2388888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 423         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020400876 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.99        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 104.21236559139786\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 436         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019239863 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.422       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 105.0703125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 451         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020056069 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.559       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.03030303030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 464         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019159283 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.90686274509804\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 478         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018468399 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.295       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.491       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0569     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 107.93095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 491         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019328773 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.309       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0604     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.81712962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 507         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019109469 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.63963963963964\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 522         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016237354 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.72        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.30701754385964\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 72         |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 536        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02020812 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.308      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.547      |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0611    |\n",
      "|    value_loss           | 2.97       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.16880341880342\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 72         |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 548        |\n",
      "|    total_timesteps      | 39936      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01639559 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.35       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.27       |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0546    |\n",
      "|    value_loss           | 2.67       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 112.15208333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 72         |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 563        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01679086 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.334      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.1        |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0585    |\n",
      "|    value_loss           | 2.92       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.29471544715447\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 577         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018775217 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.882       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.89682539682539\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 591         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019558996 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0642     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 115.1124031007752\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 604         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018135633 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.437       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0615     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.32386363636364\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 619         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017896583 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.129       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 117.39259259259259\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 72         |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 634        |\n",
      "|    total_timesteps      | 46080      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01593141 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.341      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.69       |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0581    |\n",
      "|    value_loss           | 2.75       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.55797101449275\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 651         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018469062 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.92        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.71808510638297\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -370      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 47        |\n",
      "|    time_elapsed         | 664       |\n",
      "|    total_timesteps      | 48128     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0185907 |\n",
      "|    clip_fraction        | 0.352     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.25     |\n",
      "|    explained_variance   | 0.362     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.249     |\n",
      "|    n_updates            | 460       |\n",
      "|    policy_gradient_loss | -0.0586   |\n",
      "|    value_loss           | 2.68      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.92534722222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 677         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019444639 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.498       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.15646258503402\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 72         |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 689        |\n",
      "|    total_timesteps      | 50176      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01875608 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.4        |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0503     |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0612    |\n",
      "|    value_loss           | 2.67       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.45166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 703         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018552497 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0628     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.63888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 717         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016858347 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.07        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.81570512820512\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 731         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014899163 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.432       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.61        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.96383647798743\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 746         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016794965 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.424       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.05555555555554\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 72         |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 761        |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01903048 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.449      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.198      |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.0595    |\n",
      "|    value_loss           | 2.59       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.9848484848485\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 776         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018657582 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.658       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.86755952380952\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 792         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018740509 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.689       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0629     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.61111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 808         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020755162 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.62        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.42241379310346\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 824         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019286176 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.33        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.18502824858757\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 839         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018272117 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.95\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -369       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 72         |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 853        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01851288 |\n",
      "|    clip_fraction        | 0.341      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.458      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.343      |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0596    |\n",
      "|    value_loss           | 2.53       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.7308743169399\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 867         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017899763 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.441       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.3279569892473\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 882         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020853814 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.114      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.91798941798942\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 897         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018051699 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0163      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.63020833333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -369       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 71         |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 911        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01922628 |\n",
      "|    clip_fraction        | 0.326      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.427      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.946      |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0619    |\n",
      "|    value_loss           | 2.68       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.37692307692308\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 927         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018519834 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.392       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.12247474747474\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 942         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020409495 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.911       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.6865671641791\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 957         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022842158 |\n",
      "|    clip_fraction        | 0.44        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.1593137254902\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 972         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020972557 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.59541062801932\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 986         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019149695 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 139.08095238095237\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 1002        |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018025246 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 139.65962441314554\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 1016        |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019340824 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.127      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.22453703703704\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 1031        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018525612 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.7648401826484\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 1046        |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018647207 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.986       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0621     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 141.26013513513513\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 1061        |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017910227 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0764     |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 141.72222222222223\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -367       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 71         |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 1076       |\n",
      "|    total_timesteps      | 76800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01737801 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.422      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.567      |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0636    |\n",
      "|    value_loss           | 2.7        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 142.1392543859649\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 1090        |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020703834 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.728       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 142.53679653679654\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 1105        |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020773163 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 142.9903846153846\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 1119        |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020133734 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.09        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.43776371308016\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 1134        |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020062737 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.65        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.96666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1149        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018139068 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 144.4187242798354\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 1163        |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019287445 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.914       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 144.9369918699187\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 1178        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018808149 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.4347389558233\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 1192        |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019803528 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.478       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.434       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.85615079365078\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -365       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 71         |\n",
      "|    iterations           | 84         |\n",
      "|    time_elapsed         | 1207       |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02064159 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.834      |\n",
      "|    n_updates            | 830        |\n",
      "|    policy_gradient_loss | -0.0638    |\n",
      "|    value_loss           | 2.44       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.27941176470588\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 1222        |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019448811 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.666       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.7248062015504\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 1236        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020976197 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.25        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0646     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 147.11206896551724\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -364       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 71         |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 1253       |\n",
      "|    total_timesteps      | 89088      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02320718 |\n",
      "|    clip_fraction        | 0.474      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.447      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.73       |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0691    |\n",
      "|    value_loss           | 2.66       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 147.4185606060606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1267        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020754647 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 147.83426966292134\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1283        |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020108834 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.191       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 148.1851851851852\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1297        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019418906 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.817       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 148.57967032967034\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1313        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020905517 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.109      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 148.98097826086956\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1328        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019425303 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 149.33960573476702\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1343        |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020720903 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.478       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0646     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 149.66932624113474\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1358        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019874077 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0263     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 150.0622807017544\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1373        |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018565506 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.3         |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.065      |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 150.44270833333334\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -363      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 96        |\n",
      "|    time_elapsed         | 1388      |\n",
      "|    total_timesteps      | 98304     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0209326 |\n",
      "|    clip_fraction        | 0.419     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.2      |\n",
      "|    explained_variance   | 0.443     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.108     |\n",
      "|    n_updates            | 950       |\n",
      "|    policy_gradient_loss | -0.0699   |\n",
      "|    value_loss           | 2.68      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 150.68986254295532\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1402        |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020928761 |\n",
      "|    clip_fraction        | 0.43        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.15       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 150.99659863945578\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1417        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020774763 |\n",
      "|    clip_fraction        | 0.433       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.694       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 151.3905723905724\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1432        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020610962 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0398     |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 151.78166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1446        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020786503 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.701       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -42.52\n",
      "Overall Average Successful Assignments: 111.66750411962408\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae551aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -376.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.0\n",
      "All assignments history: [15, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -376     |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -366.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.291666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009014997 |\n",
      "|    clip_fraction        | 0.0685      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.12       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.1         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 18.2        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -310.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.61111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011487106 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0219     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.19        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 13.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -238.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.416666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012373907 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0799     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.16        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 11.5        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -390.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.916666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014495728 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.00351    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.21        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0607     |\n",
      "|    value_loss           | 9.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -390.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.166666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015988164 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0367     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.72        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 7.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.214285714285715\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016302105 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.0127     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0607     |\n",
      "|    value_loss           | 7.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -318.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.354166666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 117         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017567044 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.0262     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.953       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 5.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.23148148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017708652 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.00663    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0593     |\n",
      "|    value_loss           | 5.17        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -190.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.9\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019487405 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.00266     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 4.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -190.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.81060606060606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019675888 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00264     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.119       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 4.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -236.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.458333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020971753 |\n",
      "|    clip_fraction        | 0.47        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0058      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0763     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 3.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -260.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.06410256410256\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -374       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 69         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 191        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02254678 |\n",
      "|    clip_fraction        | 0.495      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | -0.00466   |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.183      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0601    |\n",
      "|    value_loss           | 3.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -270.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.32738095238095\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 206         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023986556 |\n",
      "|    clip_fraction        | 0.528       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.000361    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.352      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0607     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -268.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.57222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 221         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026119336 |\n",
      "|    clip_fraction        | 0.577       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00475     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.676       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -244.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.421875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027801465 |\n",
      "|    clip_fraction        | 0.592       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00258     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -214.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.911764705882355\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 250         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029635044 |\n",
      "|    clip_fraction        | 0.623       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.00872     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.886       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.629629629629626\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -374       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 69         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 264        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02823316 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0479     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.015     |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0607    |\n",
      "|    value_loss           | 2.91       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.921052631578945\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 278         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026617387 |\n",
      "|    clip_fraction        | 0.556       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0855      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.318      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.770833333333336\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -374      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 20        |\n",
      "|    time_elapsed         | 293       |\n",
      "|    total_timesteps      | 20480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0251627 |\n",
      "|    clip_fraction        | 0.515     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.27     |\n",
      "|    explained_variance   | 0.11      |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | -0.0524   |\n",
      "|    n_updates            | 190       |\n",
      "|    policy_gradient_loss | -0.0593   |\n",
      "|    value_loss           | 2.95      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -244.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.96031746031746\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -374       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 69         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 307        |\n",
      "|    total_timesteps      | 21504      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02471577 |\n",
      "|    clip_fraction        | 0.486      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.145      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 3.21       |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0581    |\n",
      "|    value_loss           | 2.98       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -280.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.496212121212125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025818884 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.161      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -214.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.2463768115942\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 337         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024533339 |\n",
      "|    clip_fraction        | 0.526       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.77777777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 351         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025162822 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0693      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.29333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022157613 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0883      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.91025641025641\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 376         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020952586 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -212.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.37962962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 388         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021731766 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0999     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.74404761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 401         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022276053 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.786       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.06       |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.17241379310344\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 413         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022690937 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0402      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.7\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 426         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019609217 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.273       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0598     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.11290322580645\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -373      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 31        |\n",
      "|    time_elapsed         | 438       |\n",
      "|    total_timesteps      | 31744     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0217803 |\n",
      "|    clip_fraction        | 0.44      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.26     |\n",
      "|    explained_variance   | 0.301     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.66      |\n",
      "|    n_updates            | 300       |\n",
      "|    policy_gradient_loss | -0.0628   |\n",
      "|    value_loss           | 2.83      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.63802083333333\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -373      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 32        |\n",
      "|    time_elapsed         | 451       |\n",
      "|    total_timesteps      | 32768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0179811 |\n",
      "|    clip_fraction        | 0.331     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.26     |\n",
      "|    explained_variance   | 0.287     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.769     |\n",
      "|    n_updates            | 310       |\n",
      "|    policy_gradient_loss | -0.0583   |\n",
      "|    value_loss           | 2.82      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.00757575757575\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020305373 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.913       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.45343137254902\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 474         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019400863 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.70476190476191\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 485         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017863862 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.85        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.20138888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 496         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019919826 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.305       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.101      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.40990990990991\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 508         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019411927 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.1         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0616     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 91.6907894736842\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 519         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018475771 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.283       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.049      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 94.06410256410257\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 531         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020255465 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.88        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 3.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 96.29375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 542         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020609044 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0653     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.53455284552845\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 553         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018361991 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0776     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 100.5952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 564         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020965926 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.335       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.02        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 102.5251937984496\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 576         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020327024 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.918       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 104.33143939393939\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 587         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021634152 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.91        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.11296296296297\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 76         |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 599        |\n",
      "|    total_timesteps      | 46080      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02149816 |\n",
      "|    clip_fraction        | 0.445      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.339      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.165     |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0658    |\n",
      "|    value_loss           | 2.83       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 107.80072463768116\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 609         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020266246 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0787     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.52127659574468\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 77         |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 620        |\n",
      "|    total_timesteps      | 48128      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02140602 |\n",
      "|    clip_fraction        | 0.413      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.342      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.5        |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0642    |\n",
      "|    value_loss           | 2.76       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.19791666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 77         |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 631        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02045387 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.35       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.74       |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0618    |\n",
      "|    value_loss           | 2.79       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 112.82312925170068\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 642         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021413969 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 114.32666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 652         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022072688 |\n",
      "|    clip_fraction        | 0.437       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.582       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 115.72058823529412\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 663         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023573685 |\n",
      "|    clip_fraction        | 0.458       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0833     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 117.08333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 673         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021045893 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.885       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.38836477987421\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 684         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021717388 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.852       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.52777777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 695         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023336537 |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0677     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.55909090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 705         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022945335 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.58184523809524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 716         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020898826 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.33        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.33625730994152\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 727         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019401442 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.655       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 738         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020554017 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.546       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.88276836158192\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 749         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018842328 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0608      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0624     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.67361111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 760         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021359224 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.065      |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.46584699453553\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 770         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020559266 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.502       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.19489247311827\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 781         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020441674 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.173      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0621     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.9431216931217\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 792         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018403929 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.64713541666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 803         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020015704 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.30641025641026\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 813         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018833738 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0692     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.9419191919192\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 823         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018171921 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.351       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.252      |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.56467661691542\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 834         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019312121 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.225       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.21200980392157\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 844         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020162355 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.297       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.85628019323673\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 855         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018779326 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.989       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.47380952380954\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 865         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018010717 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0588     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.11032863849766\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 876         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019878775 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.246      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.72453703703704\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 886         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020108327 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.603       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.3173515981735\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -364       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 896        |\n",
      "|    total_timesteps      | 74752      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01781914 |\n",
      "|    clip_fraction        | 0.34       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.42       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.731      |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0628    |\n",
      "|    value_loss           | 2.56       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.87387387387386\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 906         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020336378 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.913       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.4311111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 917         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019801937 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.193      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0665     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.9890350877193\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -363       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 76         |\n",
      "|    time_elapsed         | 927        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01855797 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.421      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.124      |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | -0.0672    |\n",
      "|    value_loss           | 2.68       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.57467532467533\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -363       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 77         |\n",
      "|    time_elapsed         | 938        |\n",
      "|    total_timesteps      | 78848      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01998787 |\n",
      "|    clip_fraction        | 0.408      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.16      |\n",
      "|    explained_variance   | 0.443      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.853      |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0704    |\n",
      "|    value_loss           | 2.73       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.14529914529913\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 948         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021607589 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.879       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.07       |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.6993670886076\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 959         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017868452 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.25        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.28958333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 969         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020011123 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.243       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.82407407407408\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 980         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018844198 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0782      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.33536585365854\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -361       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 990        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01894705 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.12      |\n",
      "|    explained_variance   | 0.447      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.05       |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.069     |\n",
      "|    value_loss           | 2.6        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.89156626506025\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 1001        |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018933335 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 139.4156746031746\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 1012        |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019382041 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0718     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 139.9313725490196\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 1024        |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016345724 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0846     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0629     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.46899224806202\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 1035        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019499665 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.143       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.97413793103448\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1046        |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019239563 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.06       |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.825       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 141.53787878787878\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -358       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 88         |\n",
      "|    time_elapsed         | 1057       |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01901363 |\n",
      "|    clip_fraction        | 0.379      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.04      |\n",
      "|    explained_variance   | 0.448      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.13       |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | -0.0698    |\n",
      "|    value_loss           | 2.29       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 142.07958801498128\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1069        |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019964349 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0713     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 142.5888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -357        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1080        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017766915 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.99       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.535       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.1510989010989\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1092        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018947225 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.97       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.70471014492753\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -355        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1103        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019854277 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.92       |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.695       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0777     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 144.28225806451613\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -354        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1114        |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019755568 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.9        |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.336       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0751     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 144.70212765957447\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -353        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1125        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019964354 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.89       |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.163       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0729     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.11666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -352       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 1137       |\n",
      "|    total_timesteps      | 97280      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01964344 |\n",
      "|    clip_fraction        | 0.396      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.9       |\n",
      "|    explained_variance   | 0.487      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.88       |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0731    |\n",
      "|    value_loss           | 2.42       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.5390625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -352        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1148        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016767062 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.88       |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.438       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.0077319587629\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -350        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1160        |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019619295 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.85       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0934     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0757     |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.48469387755102\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -349        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1171        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020625148 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.81       |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.795       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0765     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.9638047138047\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -348        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1182        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019605206 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.79       |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.195       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0749     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 147.38333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -348        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1194        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020399585 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.76       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0779     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -54.52\n",
      "Overall Average Successful Assignments: 101.440395403689\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "469e3495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -360.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.75\n",
      "All assignments history: [15, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -375     |\n",
      "| time/              |          |\n",
      "|    fps             | 111      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -274.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.833333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008114351 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.229      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.99        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 16.4        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -334.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.333333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012039351 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.157      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 11.6        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -362.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.270833333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011689592 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0556      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.19        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 10.4        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -338.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.883333333333333\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -373         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 92           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 5120         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0132242525 |\n",
      "|    clip_fraction        | 0.227        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | -0.0283      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.37         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.057       |\n",
      "|    value_loss           | 9.33         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.666666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015469594 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0354      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.82        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 7.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -358.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.892857142857146\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016916664 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0184      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 6.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -372.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.447916666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016461857 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0123      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.907       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 6.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -374.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.72222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018536441 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.0136     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.944       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0629     |\n",
      "|    value_loss           | 4.87        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.1\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020152409 |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0144      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.547       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 4.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -342.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.60606060606061\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019555315 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00139     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.613       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 3.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.666666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021120232 |\n",
      "|    clip_fraction        | 0.477       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00996     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.635       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 3.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.66025641025641\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022172827 |\n",
      "|    clip_fraction        | 0.508       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00341     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.135      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0631     |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.63095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022641648 |\n",
      "|    clip_fraction        | 0.511       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00834     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.794       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0591     |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 66.77777777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024837073 |\n",
      "|    clip_fraction        | 0.557       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0105      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.06       |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.27604166666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 176         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028486462 |\n",
      "|    clip_fraction        | 0.608       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0238      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.256       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.18137254901961\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026625894 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0531      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.679       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.15277777777777\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 92         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 198        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02940902 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0748     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.235      |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.066     |\n",
      "|    value_loss           | 2.96       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.60087719298245\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 209         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027047807 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0855      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.341      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.6125\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 92         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 221        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02518652 |\n",
      "|    clip_fraction        | 0.529      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.105      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.51       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0601    |\n",
      "|    value_loss           | 2.94       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.4047619047619\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025992554 |\n",
      "|    clip_fraction        | 0.544       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0998      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.79166666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 92         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 244        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02493776 |\n",
      "|    clip_fraction        | 0.541      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.136      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.092     |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0621    |\n",
      "|    value_loss           | 2.87       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 92.82246376811594\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 92         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 255        |\n",
      "|    total_timesteps      | 23552      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02429341 |\n",
      "|    clip_fraction        | 0.532      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.154      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.68       |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0631    |\n",
      "|    value_loss           | 2.95       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 93.82291666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 266         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022517981 |\n",
      "|    clip_fraction        | 0.459       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.32        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 94.97\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 277         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021804757 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0616     |\n",
      "|    value_loss           | 3.05        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 96.35576923076923\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 290         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022142565 |\n",
      "|    clip_fraction        | 0.456       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.193       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 96.68518518518519\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022233367 |\n",
      "|    clip_fraction        | 0.464       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.314      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.33928571428571\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 312         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021375496 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.52        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 3.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.25574712643679\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 323         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020552523 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.279      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.11944444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 334         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019693375 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.5         |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.89516129032258\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 345         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021377627 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 100.59635416666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019047309 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.367       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0591     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 101.69444444444444\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 367        |\n",
      "|    total_timesteps      | 33792      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02313988 |\n",
      "|    clip_fraction        | 0.451      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.257      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.919      |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.063     |\n",
      "|    value_loss           | 2.86       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 103.03921568627452\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 378         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019669576 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 104.45952380952382\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019335702 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.249       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.241      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0604     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 105.93287037037037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 401         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021041233 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.283       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.685       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 107.52027027027027\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 412         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018324535 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.322       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.8530701754386\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 423         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019950695 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0173     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.15598290598291\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018682947 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.278       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.65833333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 445        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01978251 |\n",
      "|    clip_fraction        | 0.379      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.277      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.17       |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.061     |\n",
      "|    value_loss           | 2.8        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.33333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 457         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019029465 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.276      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 115.2936507936508\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 468         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019203287 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.669       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0574     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.80232558139535\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 480        |\n",
      "|    total_timesteps      | 44032      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01973597 |\n",
      "|    clip_fraction        | 0.382      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.311      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.417      |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0595    |\n",
      "|    value_loss           | 2.77       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.10984848484848\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 491        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01986581 |\n",
      "|    clip_fraction        | 0.387      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.314      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.272     |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.0607    |\n",
      "|    value_loss           | 2.82       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.97222222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 502         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020546861 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0621     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.94384057971014\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 513         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018778834 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.21       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.94680851063829\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 524         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019559855 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.13        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.85590277777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 536         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019528627 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.656       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.68877551020408\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 547         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020509582 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0598     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.59333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 558         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016818505 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.183      |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0587     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.4624183006536\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 569         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020700362 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.03        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0616     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.24358974358974\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 581        |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01936036 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.28       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0485     |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.0592    |\n",
      "|    value_loss           | 2.92       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.94811320754717\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 592         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017658215 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.377       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.92283950617283\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 603         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019149799 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.81818181818181\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 614         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018148893 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0606     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.71130952380952\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 626         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018498773 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0546     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0598     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.52923976608187\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 636         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014686378 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.462       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.19971264367817\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 648         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013860701 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.84745762711864\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 659        |\n",
      "|    total_timesteps      | 60416      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01644348 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.333      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.802      |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0614    |\n",
      "|    value_loss           | 2.87       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.575\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 671         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017098382 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.182      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.33879781420765\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 682         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017484443 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.655       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.01478494623655\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 693         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016525768 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.888       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.8293650793651\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 704         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017119214 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.518       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.59244791666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -369       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 715        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01578179 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.377      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.35       |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0571    |\n",
      "|    value_loss           | 2.68       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.15128205128204\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 727         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017556261 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.8181818181818\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 738         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016062455 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.179      |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.4863184079602\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 749         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014937866 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.612       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.25367647058823\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 761         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016489051 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.382       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0226      |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0642     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.07004830917873\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 772         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016473865 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.712       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.77023809523808\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -367       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 783        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01598585 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.25      |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.0577    |\n",
      "|    value_loss           | 2.53       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 139.4894366197183\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 795         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016168002 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.12       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0565     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.2939814814815\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 806         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016422171 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0615     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.98059360730593\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 818         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018830813 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.19       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0624     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 141.6858108108108\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 829         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017947096 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.343       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 142.31555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 840         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016990438 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.451       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.17763157894737\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 852         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017681703 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.494       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.91666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 863         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016247556 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.2        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 144.70405982905982\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 874         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020134931 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.029       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.49789029535864\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 886         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016470779 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.863       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.32708333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 897         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017247219 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0659     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 147.16666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 908         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018455276 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.951       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 148.0030487804878\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 920         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018189047 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.161       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 148.7781124497992\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 931         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019482419 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.334       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.457       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 149.54861111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 942         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018620055 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.205      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.068      |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 150.3607843137255\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -362       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 85         |\n",
      "|    time_elapsed         | 953        |\n",
      "|    total_timesteps      | 87040      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01777594 |\n",
      "|    clip_fraction        | 0.349      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.15      |\n",
      "|    explained_variance   | 0.386      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.915      |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0666    |\n",
      "|    value_loss           | 2.79       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 151.1424418604651\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 965         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017121233 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.969       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 151.90613026819923\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 977         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017879233 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.321       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 152.6657196969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 989         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016494054 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 153.36704119850188\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1000        |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020377211 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.852       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 154.07592592592593\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1011        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019800628 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0249      |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0724     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 154.78205128205127\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1023        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018318918 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.461       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0682     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 155.45289855072463\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1034        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018743133 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 156.09767025089604\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1046        |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019380376 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.06       |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.523       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 156.71099290780143\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -357       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 1057       |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01930901 |\n",
      "|    clip_fraction        | 0.417      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.04      |\n",
      "|    explained_variance   | 0.38       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.47       |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0723    |\n",
      "|    value_loss           | 2.54       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 157.23070175438596\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -357        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1069        |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020244084 |\n",
      "|    clip_fraction        | 0.43        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0137     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 157.73697916666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1079        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020385832 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0782     |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 158.2310996563574\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -355        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1090        |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020834366 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.264       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0742     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 158.7372448979592\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -353        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1100        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018298142 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.01       |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0732     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 159.26262626262627\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -353        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1111        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020819105 |\n",
      "|    clip_fraction        | 0.449       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.99       |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.394       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0761     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 159.75166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -352        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1122        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020279624 |\n",
      "|    clip_fraction        | 0.43        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.97       |\n",
      "|    explained_variance   | 0.35        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.198       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -24.36\n",
      "Overall Average Successful Assignments: 113.62958413838942\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab1662bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -388.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 7.333333333333333\n",
      "All assignments history: [13, 15, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -372     |\n",
      "| time/              |          |\n",
      "|    fps             | 117      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -300.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.583333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008654668 |\n",
      "|    clip_fraction        | 0.0577      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.169      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.97        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 19.2        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -398.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.305555555555557\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011495512 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0551     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 13.2        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -370.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.5625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013549415 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.00561    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 10.2        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -276.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.916666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014224793 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0292      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 8.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -252.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.333333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015551875 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0182      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.42        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 7.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.392857142857146\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015271752 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0167      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 7.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.0625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017476203 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0222      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 5.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.68518518518518\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019716203 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0137      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.52        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 4.68        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.34166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018883113 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0102      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.556       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 4.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.77272727272727\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019670976 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0144      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.216       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 3.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.51388888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022250354 |\n",
      "|    clip_fraction        | 0.5         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0158      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.31        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 3.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 93.23076923076923\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 92         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 143        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02393654 |\n",
      "|    clip_fraction        | 0.524      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.00851    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0622    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0645    |\n",
      "|    value_loss           | 3.04       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.0952380952381\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 155        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02542736 |\n",
      "|    clip_fraction        | 0.55       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.0124     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.663      |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0622    |\n",
      "|    value_loss           | 2.79       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 101.86666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027493346 |\n",
      "|    clip_fraction        | 0.578       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0228      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.55        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.16145833333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 179         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026022382 |\n",
      "|    clip_fraction        | 0.573       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0745      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.06        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.75980392156863\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 191         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026115946 |\n",
      "|    clip_fraction        | 0.567       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.179      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.94907407407408\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025569115 |\n",
      "|    clip_fraction        | 0.528       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -136.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.96052631578948\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026896015 |\n",
      "|    clip_fraction        | 0.561       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.449       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -156.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.5375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026335705 |\n",
      "|    clip_fraction        | 0.539       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.5436507936508\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 238         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026270248 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.649       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -184.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.70454545454545\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 250        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02360605 |\n",
      "|    clip_fraction        | 0.475      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.25       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.72       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0633    |\n",
      "|    value_loss           | 3.02       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.21739130434783\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 261         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021332178 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.70138888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 273         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022955736 |\n",
      "|    clip_fraction        | 0.448       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00934     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -152.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.59\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 283         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022581358 |\n",
      "|    clip_fraction        | 0.468       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.71153846153847\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 294         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023249269 |\n",
      "|    clip_fraction        | 0.454       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.877       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.065      |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 112.41975308641975\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023386315 |\n",
      "|    clip_fraction        | 0.478       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.309       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.483       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.86607142857143\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021300854 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.342       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 115.20689655172414\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 327         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021556206 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.367       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.91944444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021435631 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.334       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.16397849462365\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021160018 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.90364583333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021911273 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.63888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023055684 |\n",
      "|    clip_fraction        | 0.445       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.74019607843137\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 90         |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 386        |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02205086 |\n",
      "|    clip_fraction        | 0.426      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.337      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0136    |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.0643    |\n",
      "|    value_loss           | 3.05       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.5904761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022253754 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.667       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.18055555555556\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 411        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02050018 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.367      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.31       |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.0635    |\n",
      "|    value_loss           | 2.77       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.64414414414415\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 423         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022158055 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0967      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.24561403508773\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019995973 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0573      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.4551282051282\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 447         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023742992 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.393       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.89375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 459         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018618818 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.793       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.5\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018196158 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0411     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.02579365079364\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 483         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019164884 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.824       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0615     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.48643410852713\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 495        |\n",
      "|    total_timesteps      | 44032      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01795838 |\n",
      "|    clip_fraction        | 0.359      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.352      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.53       |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.066     |\n",
      "|    value_loss           | 2.92       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.9659090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 507         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017806489 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.537       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.42962962962963\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 519        |\n",
      "|    total_timesteps      | 46080      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01500708 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.43       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.109      |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0563    |\n",
      "|    value_loss           | 2.6        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.94021739130434\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 531         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015698604 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.38475177304966\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 543         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015097665 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.596       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 141.72743055555554\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 554         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019897545 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.192       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.0221088435374\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 567         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017396253 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.633       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 144.25333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 578         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016699158 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.4281045751634\n",
      "All assignments history: []\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 400      |\n",
      "|    ep_rew_mean          | -369     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 88       |\n",
      "|    iterations           | 51       |\n",
      "|    time_elapsed         | 591      |\n",
      "|    total_timesteps      | 52224    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.015944 |\n",
      "|    clip_fraction        | 0.292    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | -8.23    |\n",
      "|    explained_variance   | 0.385    |\n",
      "|    learning_rate        | 0.00018  |\n",
      "|    loss                 | 2.08     |\n",
      "|    n_updates            | 500      |\n",
      "|    policy_gradient_loss | -0.0607  |\n",
      "|    value_loss           | 2.74     |\n",
      "--------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.63621794871796\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 602         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016554132 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 147.85849056603774\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 614         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018501746 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 148.99382716049382\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 626         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017091148 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.863       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 150.0909090909091\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -368       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 639        |\n",
      "|    total_timesteps      | 56320      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01733476 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.451      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.1        |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.062     |\n",
      "|    value_loss           | 2.62       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 151.13541666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 651         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017855953 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0455     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 152.2266081871345\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 663         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019260097 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.816       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 153.16522988505747\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 675         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019709652 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.128      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0653     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 96.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 154.114406779661\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 687         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020079745 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0789      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 154.89305555555555\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -367       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 699        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01812967 |\n",
      "|    clip_fraction        | 0.335      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.435      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.14       |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0616    |\n",
      "|    value_loss           | 2.55       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 155.56284153005464\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -367       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 711        |\n",
      "|    total_timesteps      | 62464      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01931861 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.394      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.06       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0631    |\n",
      "|    value_loss           | 2.75       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 156.30376344086022\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 723         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019355712 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.167      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 157.00132275132276\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 736         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015603254 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.155       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0589     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 157.75130208333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 748         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020117642 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.432       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 158.53974358974358\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 761         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020266809 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.189       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 159.16414141414143\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 772         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016675727 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.696       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0646     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 159.81716417910448\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 784         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020267714 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 160.5171568627451\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 796         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018602077 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.515       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 161.28985507246378\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 808         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020370718 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.279       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 96.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 161.99761904761905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 820         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018059492 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 162.69131455399062\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 832         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021751378 |\n",
      "|    clip_fraction        | 0.454       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.579       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 163.48032407407408\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 844         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020712897 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.486       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 164.10045662100455\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 856         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021864438 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0682     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 164.77364864864865\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 868         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021160347 |\n",
      "|    clip_fraction        | 0.437       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.577       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0699     |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 165.4477777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 880         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022037975 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.442       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0658     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 166.10197368421052\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 892         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020095259 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.478       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 166.732683982684\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 904         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020337293 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.139       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 167.3846153846154\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 916         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019736577 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.327       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 168.04324894514767\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 928         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024296805 |\n",
      "|    clip_fraction        | 0.471       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.437       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 168.76770833333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 940         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019973189 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 169.5216049382716\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 952         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022107804 |\n",
      "|    clip_fraction        | 0.447       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.503       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0729     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 170.22052845528455\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 964         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021722734 |\n",
      "|    clip_fraction        | 0.432       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 136.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 170.93172690763052\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 976         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021567624 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.521       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 171.57638888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 988         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022046488 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.668       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0725     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 172.26176470588234\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -357        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 1001        |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022160422 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.04       |\n",
      "|    explained_variance   | 0.335       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 172.96511627906978\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -356       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 1012       |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01980419 |\n",
      "|    clip_fraction        | 0.425      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.02      |\n",
      "|    explained_variance   | 0.4        |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.182      |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.0715    |\n",
      "|    value_loss           | 2.49       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 173.6714559386973\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -355        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1024        |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022552736 |\n",
      "|    clip_fraction        | 0.464       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.01       |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.71        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0761     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 174.32670454545453\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -354        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1036        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020267101 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.98       |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.253       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0766     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 174.97378277153558\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -353        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1047        |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020422643 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0725     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 175.62037037037038\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -352       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 1059       |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02380855 |\n",
      "|    clip_fraction        | 0.487      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.91      |\n",
      "|    explained_variance   | 0.331      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.12       |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.0815    |\n",
      "|    value_loss           | 3.06       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 176.26556776556777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -351        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1071        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021517415 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.89       |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.607       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 176.89673913043478\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -350       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 92         |\n",
      "|    time_elapsed         | 1083       |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02282073 |\n",
      "|    clip_fraction        | 0.457      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.86      |\n",
      "|    explained_variance   | 0.296      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.231      |\n",
      "|    n_updates            | 910        |\n",
      "|    policy_gradient_loss | -0.0761    |\n",
      "|    value_loss           | 2.71       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 177.4910394265233\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -349        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1095        |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022217425 |\n",
      "|    clip_fraction        | 0.466       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.84       |\n",
      "|    explained_variance   | 0.314       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.577       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0766     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 178.09751773049646\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -348        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1108        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024847094 |\n",
      "|    clip_fraction        | 0.476       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.82       |\n",
      "|    explained_variance   | 0.35        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.743       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0749     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 178.7078947368421\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -346       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 1120       |\n",
      "|    total_timesteps      | 97280      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01959733 |\n",
      "|    clip_fraction        | 0.426      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.78      |\n",
      "|    explained_variance   | 0.299      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.171      |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0754    |\n",
      "|    value_loss           | 2.79       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 179.28472222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -345        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1133        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023478867 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.75       |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.559       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0791     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 179.8737113402062\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -343        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1144        |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022109166 |\n",
      "|    clip_fraction        | 0.464       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.71       |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.899       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0793     |\n",
      "|    value_loss           | 3.09        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 180.43707482993196\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -342        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1155        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022100594 |\n",
      "|    clip_fraction        | 0.475       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.68       |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.453       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0809     |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 181.00084175084174\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -340        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1166        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021663662 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.65       |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.157       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0801     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 181.56833333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -338        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1178        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022601655 |\n",
      "|    clip_fraction        | 0.482       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.61       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0818     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: 26.52\n",
      "Overall Average Successful Assignments: 133.7614106469218\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc4f8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -382.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 9.75\n",
      "All assignments history: [12, 15, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -373     |\n",
      "| time/              |          |\n",
      "|    fps             | 103      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -356.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.75\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -378        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008774238 |\n",
      "|    clip_fraction        | 0.0614      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0881      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.51        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 17          |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -370.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.916666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010800336 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0239     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.06        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 13.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -368.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.020833333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013994105 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0116     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.18        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 10.2        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -384.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.8\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014318451 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0423      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.43        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0574     |\n",
      "|    value_loss           | 9.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -336.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.27777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015835542 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.00834    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.87        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 8.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -356.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.845238095238095\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017576778 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0112      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 6.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -310.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.75\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016885508 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.00941     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 6.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -336.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.787037037037038\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -374       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 97         |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01911741 |\n",
      "|    clip_fraction        | 0.434      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.016      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.614      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0652    |\n",
      "|    value_loss           | 4.92       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.68333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019254882 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0125      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0606     |\n",
      "|    value_loss           | 4.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.34848484848485\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 92         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 121        |\n",
      "|    total_timesteps      | 11264      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02159512 |\n",
      "|    clip_fraction        | 0.49       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.00965    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.26       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0668    |\n",
      "|    value_loss           | 3.58       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.30555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022062061 |\n",
      "|    clip_fraction        | 0.484       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.0125      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0788     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 3.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -320.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.22435897435897\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 145         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022581324 |\n",
      "|    clip_fraction        | 0.505       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.0108      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.38        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 3.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.583333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 157         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024951445 |\n",
      "|    clip_fraction        | 0.572       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.00603     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.49444444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026704457 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.0121      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.814       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.359375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026124131 |\n",
      "|    clip_fraction        | 0.581       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0284      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.405       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.16176470588235\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027930059 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.075       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.586       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 68.22222222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 204         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026081298 |\n",
      "|    clip_fraction        | 0.556       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.261      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0615     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 72.44298245614036\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 216        |\n",
      "|    total_timesteps      | 19456      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02550501 |\n",
      "|    clip_fraction        | 0.528      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.175      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.372     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0621    |\n",
      "|    value_loss           | 2.85       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.53333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023460887 |\n",
      "|    clip_fraction        | 0.46        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.23412698412699\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025472008 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.2         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.43181818181819\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 251         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025360359 |\n",
      "|    clip_fraction        | 0.52        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.195       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.72826086956522\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 264         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022160785 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.959       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0591     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.51041666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 276        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02044482 |\n",
      "|    clip_fraction        | 0.412      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.23       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.71       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0558    |\n",
      "|    value_loss           | 2.85       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -156.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.35666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 287         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020700442 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -152.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.23076923076923\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 299         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021006785 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.777       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0569     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.23456790123457\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 312         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021468984 |\n",
      "|    clip_fraction        | 0.433       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.868       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.22321428571429\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 324        |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02004679 |\n",
      "|    clip_fraction        | 0.387      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.282      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.199     |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0569    |\n",
      "|    value_loss           | 2.83       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.13793103448276\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 335         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020825189 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.684       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.91111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 346         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020067537 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.19        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0606     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 92.56451612903226\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 359         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023480445 |\n",
      "|    clip_fraction        | 0.471       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.16        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0657     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 93.6015625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 371         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021601677 |\n",
      "|    clip_fraction        | 0.432       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.59        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 94.39646464646465\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 382         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017665546 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -122.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 95.0906862745098\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 394         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018619042 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.425       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 95.63809523809523\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 407         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017754335 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.621       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 95.97222222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 418         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021158878 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.87        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.30855855855856\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 431         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020372128 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.335       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0744     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0591     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.98245614035088\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 442         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019520216 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.346       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.80128205128206\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020136587 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 100.77083333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 466         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020991318 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0868      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 101.96747967479675\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 478         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018847916 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0579     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 103.20039682539682\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 490         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020696027 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.659       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 104.36627906976744\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 502         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022093972 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0683     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 105.41098484848484\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 514         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018752534 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.46        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.42407407407407\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 526         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019950517 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0606     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 107.4945652173913\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 538         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022194687 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.369       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.51595744680851\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 550         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019367509 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.736       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.33333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 562         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019389302 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.2         |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.45068027210884\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -369       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 573        |\n",
      "|    total_timesteps      | 50176      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01950253 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.349      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.6        |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0623    |\n",
      "|    value_loss           | 2.87       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.54\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 585         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019068733 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.39        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.712       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 112.5326797385621\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 596         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019795261 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.389       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.42307692307692\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 608         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020969888 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.61        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0665     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 114.04559748427673\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 620         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018651586 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.302       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 114.60493827160494\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 632         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020401508 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 115.33333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -368       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 644        |\n",
      "|    total_timesteps      | 56320      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02210877 |\n",
      "|    clip_fraction        | 0.449      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.415      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.285      |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0679    |\n",
      "|    value_loss           | 2.64       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.08184523809524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 656         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017076703 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.88        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 117.04093567251462\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 667         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020170355 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.334       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.00431034482759\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -368       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 58         |\n",
      "|    time_elapsed         | 679        |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01869753 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.423      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.114     |\n",
      "|    n_updates            | 570        |\n",
      "|    policy_gradient_loss | -0.0638    |\n",
      "|    value_loss           | 2.64       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.01977401129943\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 692         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017977837 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.337      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.12361111111112\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 705         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017247926 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.185      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0598     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.1775956284153\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 718         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017876996 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.02956989247312\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 730         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019408556 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.13       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.66137566137566\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 741         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017693548 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.252       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.27734375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 752         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017383615 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00926     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.9551282051282\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 764         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017356044 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.104      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.66666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 776         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018120358 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.809       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.5410447761194\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 788         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017652486 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.843       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0642     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.31985294117646\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 800         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019465046 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0642     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.08695652173913\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 811         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019063108 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.87        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.90238095238095\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -366       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 821        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01880711 |\n",
      "|    clip_fraction        | 0.359      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.777      |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.062     |\n",
      "|    value_loss           | 2.59       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.6537558685446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 832         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019797087 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0692      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.43055555555554\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 843         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020834364 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.44        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.954       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.17922374429224\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 853         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019879488 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.247       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.9572072072072\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -366       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 864        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01988304 |\n",
      "|    clip_fraction        | 0.419      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.406      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.33       |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0677    |\n",
      "|    value_loss           | 2.78       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.72\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 875         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020704523 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00611    |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.4561403508772\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -365      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 87        |\n",
      "|    iterations           | 76        |\n",
      "|    time_elapsed         | 886       |\n",
      "|    total_timesteps      | 77824     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0206433 |\n",
      "|    clip_fraction        | 0.402     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.21     |\n",
      "|    explained_variance   | 0.431     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | -0.232    |\n",
      "|    n_updates            | 750       |\n",
      "|    policy_gradient_loss | -0.0673   |\n",
      "|    value_loss           | 2.7       |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.12337662337663\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 896         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018281998 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0499     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.76175213675214\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 907         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019817824 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.231       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.35337552742615\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 917         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021075644 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.551       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.92083333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 927         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021147057 |\n",
      "|    clip_fraction        | 0.444       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.17        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.47736625514403\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 938         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018724468 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.206       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.03048780487805\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 949         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019626731 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.94        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.56626506024097\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 960         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021516925 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.07440476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 970         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020423785 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.442       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.46        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.5735294117647\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 981         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020326883 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.505       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.04166666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 991         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019598566 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.724       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.73659003831418\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1002        |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021058708 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0672     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 139.4535984848485\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1012        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019519681 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0186      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.065      |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.04119850187266\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1023        |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019888327 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.983       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0659     |\n",
      "|    value_loss           | 2.51        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.63611111111112\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1034        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021874148 |\n",
      "|    clip_fraction        | 0.437       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0211     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 141.35256410256412\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -362       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 91         |\n",
      "|    time_elapsed         | 1045       |\n",
      "|    total_timesteps      | 93184      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02058221 |\n",
      "|    clip_fraction        | 0.418      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.18      |\n",
      "|    explained_variance   | 0.405      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0732    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.066     |\n",
      "|    value_loss           | 2.55       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 142.09692028985506\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1058        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019912094 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.368       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0658     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 142.77329749103941\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -361       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 1070       |\n",
      "|    total_timesteps      | 95232      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02000555 |\n",
      "|    clip_fraction        | 0.402      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.18      |\n",
      "|    explained_variance   | 0.382      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.116      |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0675    |\n",
      "|    value_loss           | 2.7        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.43262411347519\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1082        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021821372 |\n",
      "|    clip_fraction        | 0.437       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 144.02105263157895\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1095        |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019171044 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 144.62934027777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1107        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020438887 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.136      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.34536082474227\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1119        |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020289622 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.37        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.0518707482993\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1131        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021638632 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.995       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.75\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1143        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020862594 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 147.485\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1155        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018927762 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.701       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0658     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -53.2\n",
      "Overall Average Successful Assignments: 101.96039537952929\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21ed5134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -360.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.083333333333332\n",
      "All assignments history: [16, 13, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -371     |\n",
      "| time/              |          |\n",
      "|    fps             | 106      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -258.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.541666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007813905 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.275      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.56        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 18.6        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -224.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.083333333333336\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 34         |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01180188 |\n",
      "|    clip_fraction        | 0.182      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | -0.231     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.25       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0609    |\n",
      "|    value_loss           | 12.3       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -254.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.854166666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011455764 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.03       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.83        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 10.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.56666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014373007 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0763     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 8.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -122.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 75.98611111111111\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 70         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01353933 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | -0.0173    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.82       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0538    |\n",
      "|    value_loss           | 8.47       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.27380952380952\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016832951 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0148      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0621     |\n",
      "|    value_loss           | 6.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.34375\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -374       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 95         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01688889 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.000445   |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.37       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0594    |\n",
      "|    value_loss           | 6          |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.11111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018280422 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.00288    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.863       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 4.9         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -234.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.75833333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020043887 |\n",
      "|    clip_fraction        | 0.436       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.00309     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.775       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 4.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.31818181818181\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021253992 |\n",
      "|    clip_fraction        | 0.46        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.00916     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.166       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 3.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -134.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.45833333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021272253 |\n",
      "|    clip_fraction        | 0.448       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00644     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.303       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 3.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -204.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.93589743589743\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 154        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02411851 |\n",
      "|    clip_fraction        | 0.542      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.00645    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.26       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0673    |\n",
      "|    value_loss           | 3.02       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -204.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.48809523809524\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 165        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02373309 |\n",
      "|    clip_fraction        | 0.528      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.00823    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.784      |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0598    |\n",
      "|    value_loss           | 2.88       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -266.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.40555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024640337 |\n",
      "|    clip_fraction        | 0.548       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.011       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.199       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -238.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.32291666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 189         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029114533 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0251      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.591       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.59313725490196\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 201         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029738532 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0614      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.768       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.88        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -232.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.75925925925925\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 212         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025298249 |\n",
      "|    clip_fraction        | 0.548       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0896      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.333      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -242.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.76315789473684\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024909269 |\n",
      "|    clip_fraction        | 0.548       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.153      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 3.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -152.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.78333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026038337 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.333       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.42063492063492\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 246         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026721966 |\n",
      "|    clip_fraction        | 0.574       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 91.06060606060606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 258         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026607033 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0657     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 92.69202898550725\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 269        |\n",
      "|    total_timesteps      | 23552      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02291521 |\n",
      "|    clip_fraction        | 0.455      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.172      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.421     |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0569    |\n",
      "|    value_loss           | 2.81       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 93.92361111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025014907 |\n",
      "|    clip_fraction        | 0.531       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.212       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 95.24666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 292         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022530891 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0128      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 95.96794871794872\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023503944 |\n",
      "|    clip_fraction        | 0.48        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.802       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 96.27160493827161\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023537543 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.1         |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 96.68154761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 328         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022565609 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.387      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.16091954022988\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022426736 |\n",
      "|    clip_fraction        | 0.457       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.194       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.28055555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 351         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025120458 |\n",
      "|    clip_fraction        | 0.472       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.194      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.5241935483871\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021696303 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.47        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.890625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021931743 |\n",
      "|    clip_fraction        | 0.445       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.578       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.04797979797979\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 386        |\n",
      "|    total_timesteps      | 33792      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02086594 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.309      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.879      |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0607    |\n",
      "|    value_loss           | 2.69       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.36274509803921\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 397         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019979134 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.699       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -162.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.45\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 409         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019605791 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0818      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -166.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.51851851851852\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 420         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016969636 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.558       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.71846846846847\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 432         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018400446 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.9298245614035\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 444        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01732458 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.362      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.63       |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0605    |\n",
      "|    value_loss           | 2.63       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.84188034188034\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 456         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015548077 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.188       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0559     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.7\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 467         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019316554 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.686       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -196.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.4430894308943\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 479         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016219169 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.558       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0589     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -198.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.17063492063492\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 490         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018747851 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0199     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.20930232558139\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 501         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016000012 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.689       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 2.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.7159090909091\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 513        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01485426 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.371      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.71       |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.0579    |\n",
      "|    value_loss           | 2.94       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.0\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 524         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015748609 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.259       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.78804347826087\n",
      "All assignments history: []\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 400      |\n",
      "|    ep_rew_mean          | -370     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 87       |\n",
      "|    iterations           | 46       |\n",
      "|    time_elapsed         | 536      |\n",
      "|    total_timesteps      | 47104    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.015133 |\n",
      "|    clip_fraction        | 0.264    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | -8.24    |\n",
      "|    explained_variance   | 0.416    |\n",
      "|    learning_rate        | 0.00018  |\n",
      "|    loss                 | 1.51     |\n",
      "|    n_updates            | 450      |\n",
      "|    policy_gradient_loss | -0.0561  |\n",
      "|    value_loss           | 2.49     |\n",
      "--------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -204.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.48226950354609\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 548         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017806225 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.33        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.38368055555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 559         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016565008 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.13        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.45408163265306\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 571         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016128328 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.44        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.192      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.43166666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -369       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 582        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01838402 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.468      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0634     |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.0608    |\n",
      "|    value_loss           | 2.52       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.68137254901961\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 593         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015132165 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.00320512820512\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -369       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 604        |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01677315 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.714      |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.0582    |\n",
      "|    value_loss           | 2.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.14779874213836\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 616         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015162859 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0607     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.86265432098766\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -369         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 88           |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 628          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0145739075 |\n",
      "|    clip_fraction        | 0.266        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.23        |\n",
      "|    explained_variance   | 0.42         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.07         |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.0599      |\n",
      "|    value_loss           | 2.66         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 100.46212121212122\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 639         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017884469 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0587     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 101.09077380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 651         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015008155 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.424       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 101.72953216374269\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 662         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016600568 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.435       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 102.4066091954023\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -368      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 88        |\n",
      "|    iterations           | 58        |\n",
      "|    time_elapsed         | 673       |\n",
      "|    total_timesteps      | 59392     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0169718 |\n",
      "|    clip_fraction        | 0.323     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.22     |\n",
      "|    explained_variance   | 0.432     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.09      |\n",
      "|    n_updates            | 570       |\n",
      "|    policy_gradient_loss | -0.0617   |\n",
      "|    value_loss           | 2.64      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 103.07344632768361\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 685         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014470633 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 104.24583333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -368       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 699        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01698319 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.434      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.842      |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0618    |\n",
      "|    value_loss           | 2.53       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 105.27459016393442\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -368       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 710        |\n",
      "|    total_timesteps      | 62464      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01637265 |\n",
      "|    clip_fraction        | 0.309      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.449      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.42       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0623    |\n",
      "|    value_loss           | 2.49       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.00672043010752\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 720         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018465381 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.476       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.49        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.06       |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.83730158730158\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 731         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016897425 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.121      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 107.68359375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 741         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017749844 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.813       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.52948717948718\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 751         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017674703 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.478       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0797     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.71464646464646\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 761         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017409585 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.206       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.64676616915423\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -367      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 88        |\n",
      "|    iterations           | 67        |\n",
      "|    time_elapsed         | 772       |\n",
      "|    total_timesteps      | 68608     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0171436 |\n",
      "|    clip_fraction        | 0.317     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.2      |\n",
      "|    explained_variance   | 0.502     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.835     |\n",
      "|    n_updates            | 660       |\n",
      "|    policy_gradient_loss | -0.0638   |\n",
      "|    value_loss           | 2.43      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.58700980392157\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 783         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018585853 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.292      |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 112.6256038647343\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -366       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 88         |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 795        |\n",
      "|    total_timesteps      | 70656      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01926829 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.463      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.224     |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0655    |\n",
      "|    value_loss           | 2.41       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.75238095238095\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 806         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020225137 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.504       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 114.99882629107981\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 818         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018584039 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.117       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.15393518518519\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 828         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016114451 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.051      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 117.40867579908675\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 837         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017939685 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.257      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.59346846846847\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 846         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018107964 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.71        |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.75333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 856         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016217578 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.605       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.8969298245614\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 866         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018706698 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.53        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.382       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.0108225108225\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 877         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018362813 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.0801282051282\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 886         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018443896 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.808       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.14873417721519\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 897         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018353201 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.723       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.28645833333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 906         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018877769 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.524       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0667     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.49691358024691\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 917         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018558905 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.517       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.433       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.60162601626017\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 927         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019815238 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.549       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.134      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.73192771084337\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 937         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016575113 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.4         |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 2.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.87003968253967\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 947         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018759063 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.1         |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.01470588235293\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 956         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019438222 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.04       |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.18        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.1327519379845\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 966         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018925551 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.519       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.843       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.19061302681993\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 976         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018889576 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8          |\n",
      "|    explained_variance   | 0.532       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.377       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.25\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -357        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 986         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017544879 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.98       |\n",
      "|    explained_variance   | 0.561       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.338       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.2687265917603\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 997         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017116971 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.96       |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.94        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0731     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.28148148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -355        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1006        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018984305 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.93       |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0757     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.27380952380952\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -354        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1016        |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020303873 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.92       |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.329       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.25996376811594\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -353       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 92         |\n",
      "|    time_elapsed         | 1026       |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01957703 |\n",
      "|    clip_fraction        | 0.413      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.88      |\n",
      "|    explained_variance   | 0.54       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.765      |\n",
      "|    n_updates            | 910        |\n",
      "|    policy_gradient_loss | -0.0738    |\n",
      "|    value_loss           | 2.17       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 139.27956989247312\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -352        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1037        |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018679226 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.85       |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.408       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0751     |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.3005319148936\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -351        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1047        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018422678 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.81       |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.294       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0762     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 141.26929824561404\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -350        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1057        |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015961671 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.76       |\n",
      "|    explained_variance   | 0.526       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.86        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0749     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 136.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 142.22135416666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -348       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 92         |\n",
      "|    iterations           | 96         |\n",
      "|    time_elapsed         | 1066       |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01777437 |\n",
      "|    clip_fraction        | 0.386      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.73      |\n",
      "|    explained_variance   | 0.577      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.246      |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.0749    |\n",
      "|    value_loss           | 2.21       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.19845360824743\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -347        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1076        |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016812626 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.68       |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.713       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0743     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 144.13095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -345        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1087        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019137414 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.62       |\n",
      "|    explained_variance   | 0.52        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.452       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0778     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.03787878787878\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -344        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1097        |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018119771 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.57       |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.709       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0796     |\n",
      "|    value_loss           | 3.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.96\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -341        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1107        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017990015 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.5        |\n",
      "|    explained_variance   | 0.526       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0801     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -58.62\n",
      "Overall Average Successful Assignments: 104.6966254406593\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdc466c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -388.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 7.083333333333333\n",
      "All assignments history: [13, 12, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -375     |\n",
      "| time/              |          |\n",
      "|    fps             | 126      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -342.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.75\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008803466 |\n",
      "|    clip_fraction        | 0.0666      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0821     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.12        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 15.8        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.083333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012811149 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0702     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -360.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.3125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013584996 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0523      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 10.2        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -344.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.18333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015134622 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.004       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.79        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 8.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -316.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.736111111111114\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016223652 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0235      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 7.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -350.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.05952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017383717 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0102      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 6.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -400.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.28125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017075624 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.00202     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.951       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0573     |\n",
      "|    value_loss           | 5.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -196.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.71296296296296\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 104        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 88         |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02023479 |\n",
      "|    clip_fraction        | 0.446      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | -0.00162   |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.69       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0672    |\n",
      "|    value_loss           | 4.71       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -336.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.025\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 103        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 98         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01996264 |\n",
      "|    clip_fraction        | 0.43       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.00655    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.432      |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0616    |\n",
      "|    value_loss           | 4.28       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -336.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.50757575757576\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020353466 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00218     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.011      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 3.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -332.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.15972222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022954417 |\n",
      "|    clip_fraction        | 0.496       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00931     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.184       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 3.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -228.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.17307692307692\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 103        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 128        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02322429 |\n",
      "|    clip_fraction        | 0.512      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.00217    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.488      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0598    |\n",
      "|    value_loss           | 3.07       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -308.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.42857142857143\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023697782 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.00246     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.799       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -272.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.65\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027096543 |\n",
      "|    clip_fraction        | 0.61        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.0032      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.02        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -256.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.114583333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029422242 |\n",
      "|    clip_fraction        | 0.639       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00617     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.99        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0657     |\n",
      "|    value_loss           | 2.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -228.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.11274509803921\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 102        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 169        |\n",
      "|    total_timesteps      | 17408      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03026528 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.0133     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.6        |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0618    |\n",
      "|    value_loss           | 2.84       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.14351851851852\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 179         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027682107 |\n",
      "|    clip_fraction        | 0.57        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.371       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.26754385964912\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 102        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 189        |\n",
      "|    total_timesteps      | 19456      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02537607 |\n",
      "|    clip_fraction        | 0.496      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.151      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0203    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0612    |\n",
      "|    value_loss           | 2.81       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.81666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 102        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 200        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02309369 |\n",
      "|    clip_fraction        | 0.481      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.191      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.338      |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0603    |\n",
      "|    value_loss           | 2.79       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.2936507936508\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 210         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020802338 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.131       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 3.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.50378787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020790303 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.728       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.72463768115942\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021576935 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 2.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.92708333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020606175 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.201       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 78.77666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 246         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021530911 |\n",
      "|    clip_fraction        | 0.444       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.131      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.73717948717949\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 255         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019053705 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.244      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0593     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.02777777777777\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 104        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 265        |\n",
      "|    total_timesteps      | 27648      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02034398 |\n",
      "|    clip_fraction        | 0.375      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.272      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0682    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.054     |\n",
      "|    value_loss           | 2.79       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -266.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.16071428571429\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 275         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023268575 |\n",
      "|    clip_fraction        | 0.451       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.233      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.66954022988506\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 285         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020993518 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.09166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 294         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020613976 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.128      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.29838709677419\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 303         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021908041 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.762       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 91.55208333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 104        |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 312        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02529549 |\n",
      "|    clip_fraction        | 0.483      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.32       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.602      |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0632    |\n",
      "|    value_loss           | 2.87       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 93.68181818181819\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019943029 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.161       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 3.18        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 95.30392156862744\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022276212 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.76        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 2.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.1595238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020337995 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -134.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.59722222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 348         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022855448 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.22       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.63738738738739\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 106        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 357        |\n",
      "|    total_timesteps      | 37888      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01996506 |\n",
      "|    clip_fraction        | 0.379      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.323      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.475      |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0583    |\n",
      "|    value_loss           | 2.77       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.66008771929825\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 365         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021974817 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.424       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 100.8076923076923\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020836148 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.92        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 102.25208333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019806497 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 103.33739837398375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 392         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021088243 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0608      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 104.82142857142857\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 401        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02366568 |\n",
      "|    clip_fraction        | 0.434      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.317      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.243      |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | -0.0603    |\n",
      "|    value_loss           | 2.98       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.09108527131782\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 410         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020573474 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.897       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 107.30113636363636\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 419         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018597048 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.192       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.50185185185185\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 428         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022466253 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.329       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.59420289855072\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 437         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018474359 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.237       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0589     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.72517730496453\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 446         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019220289 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.328       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 2.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.86111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 454         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016793707 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.573       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.05102040816327\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018291106 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0624     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 114.085\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 472         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019436086 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.136      |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 114.94934640522875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 481         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018683203 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.117       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 115.86378205128206\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -369       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 490        |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01793233 |\n",
      "|    clip_fraction        | 0.334      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.351      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.9        |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.0606    |\n",
      "|    value_loss           | 2.64       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.88679245283019\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 499         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017911678 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0487     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0574     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 117.82870370370371\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 508         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019904107 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.451       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0607     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.85303030303031\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 517         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019306766 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.14        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0659     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.98065476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 525         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019122943 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.34795321637426\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -368       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 57         |\n",
      "|    time_elapsed         | 535        |\n",
      "|    total_timesteps      | 58368      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01871874 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.362      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0224    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0608    |\n",
      "|    value_loss           | 2.8        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.48132183908046\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 544         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018030718 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0851     |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.54661016949153\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 553         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018429521 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0598     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.6361111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 562         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016101208 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.186       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.73360655737704\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 570         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016639188 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.898       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.88575268817205\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 579         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017226674 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.432       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.151      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0607     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.96825396825396\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 588         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016186692 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.476       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.03255208333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 597         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014598615 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.97        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0589     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.99358974358975\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 606         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017475974 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.29        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.94065656565655\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 615         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016188148 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.335       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.93283582089552\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 624         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015785525 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0999     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0598     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.92156862745097\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 634         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015781594 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.203      |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0621     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.90217391304347\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 643         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014351431 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.161       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.825\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 652         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013943259 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.501       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0437      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.7206572769953\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 661         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013377233 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.506       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.867       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.5023148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 670         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014959292 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.46        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.2659817351598\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -363       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 679        |\n",
      "|    total_timesteps      | 74752      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01403142 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.13      |\n",
      "|    explained_variance   | 0.504      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.297      |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0602    |\n",
      "|    value_loss           | 2.61       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 138.11036036036037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 687         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014099255 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.161      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 139.12444444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 696         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017056163 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.504       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 140.1140350877193\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 706         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014736613 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.52        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.199      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 141.05519480519482\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014785408 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.324       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 142.07692307692307\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 724         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015983209 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.408       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0621     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 143.14240506329114\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 733         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016155452 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.269       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 144.090625\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -359       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 742        |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01741551 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.07      |\n",
      "|    explained_variance   | 0.44       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.49       |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | -0.0647    |\n",
      "|    value_loss           | 2.63       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 145.070987654321\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -359         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 110          |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 751          |\n",
      "|    total_timesteps      | 82944        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0155274905 |\n",
      "|    clip_fraction        | 0.316        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.05        |\n",
      "|    explained_variance   | 0.5          |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | -0.148       |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0658      |\n",
      "|    value_loss           | 2.51         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.0060975609756\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -358       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 761        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01501281 |\n",
      "|    clip_fraction        | 0.278      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.04      |\n",
      "|    explained_variance   | 0.522      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.269      |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.0632    |\n",
      "|    value_loss           | 2.35       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 146.8343373493976\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -357        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 769         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013384962 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.03       |\n",
      "|    explained_variance   | 0.515       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.238      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.06       |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 147.5922619047619\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 778         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014236708 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.01       |\n",
      "|    explained_variance   | 0.515       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.134       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 148.3205882352941\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -355        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 788         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015255548 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.01       |\n",
      "|    explained_variance   | 0.55        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.574       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0606     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 149.12403100775194\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -355        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 797         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015455805 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.99       |\n",
      "|    explained_variance   | 0.512       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 149.9463601532567\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -354        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 806         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013897149 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.96       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 150.68655303030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -353        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 814         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013326504 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.861       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 151.43632958801498\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -352        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 822         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016054895 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.92       |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.31        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 152.1287037037037\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -351         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 110          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 831          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0151684545 |\n",
      "|    clip_fraction        | 0.297        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -7.89        |\n",
      "|    explained_variance   | 0.462        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.677        |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.0672      |\n",
      "|    value_loss           | 2.71         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 152.82051282051282\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -350        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 840         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015750656 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.86       |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.427       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 153.48822463768116\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -349        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 848         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014186612 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.86       |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.819       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 154.1890681003584\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -348        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 858         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016036741 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.84       |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.248       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 154.93173758865248\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -346        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 867         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013684955 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.79       |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.511       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 155.58771929824562\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -345        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 876         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015570692 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.76       |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.449       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0677     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 156.26128472222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -344        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 885         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014530431 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.73       |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.31        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 156.98539518900344\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -342        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 894         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014695473 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.69       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.997       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 157.69557823129253\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -340        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 903         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013616287 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.64       |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.677       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 158.34680134680136\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -339        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 912         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014974788 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.59       |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0714     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 158.98916666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -337        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 922         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015496928 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.54       |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.14        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -27.78\n",
      "Overall Average Successful Assignments: 105.15988652043026\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aa9ce6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -370.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.333333333333334\n",
      "All assignments history: [7, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -378     |\n",
      "| time/              |          |\n",
      "|    fps             | 130      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -316.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.541666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -378        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008195158 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.174      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.19        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 18          |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -366.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.36111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -378        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011230716 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.118      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.7         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 14          |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -358.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.8125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -376        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012955575 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.067      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.16        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0591     |\n",
      "|    value_loss           | 11          |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.18333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -376        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014888383 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.00949    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.92        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 8.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -240.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.59722222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -376        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015553451 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0211      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 8.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -348.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.11904761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016664093 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.014      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 7.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -318.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.822916666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017800968 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.00574    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 6.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -242.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.138888888888886\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018348357 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.00414    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.41        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0604     |\n",
      "|    value_loss           | 4.99        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -328.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.141666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020530485 |\n",
      "|    clip_fraction        | 0.447       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00434     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.627       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 4.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -294.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.59848484848485\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019696988 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0111      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.26        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0606     |\n",
      "|    value_loss           | 3.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -302.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.74305555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020732356 |\n",
      "|    clip_fraction        | 0.448       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0079      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.562       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 3.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.98076923076923\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022843283 |\n",
      "|    clip_fraction        | 0.512       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00466     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.561       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 3.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -238.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.517857142857146\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 124         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025509063 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00538     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.158      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.90555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024989333 |\n",
      "|    clip_fraction        | 0.572       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0159      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.17        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -122.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.854166666666664\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 115        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 141        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02445037 |\n",
      "|    clip_fraction        | 0.561      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.0458     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0954    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0606    |\n",
      "|    value_loss           | 2.76       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.029411764705884\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026790299 |\n",
      "|    clip_fraction        | 0.575       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0985      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.736111111111114\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024578076 |\n",
      "|    clip_fraction        | 0.531       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.353      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.25438596491227\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025126673 |\n",
      "|    clip_fraction        | 0.514       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.067       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.025\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 115        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 176        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02454624 |\n",
      "|    clip_fraction        | 0.503      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.311      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.188     |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0601    |\n",
      "|    value_loss           | 2.48       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.33333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025461541 |\n",
      "|    clip_fraction        | 0.503       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.298       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.899       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.84469696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 195         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022509225 |\n",
      "|    clip_fraction        | 0.472       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.76        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0625     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 73.9927536231884\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023532726 |\n",
      "|    clip_fraction        | 0.505       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.03819444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021902174 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.41666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023436625 |\n",
      "|    clip_fraction        | 0.47        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.73        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0672     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.25320512820512\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022220582 |\n",
      "|    clip_fraction        | 0.48        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0722     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 80.70987654320987\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021286212 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.382       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.33        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.11309523809524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 250         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018914128 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.2        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.47126436781609\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 114        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 259        |\n",
      "|    total_timesteps      | 29696      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02197221 |\n",
      "|    clip_fraction        | 0.408      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.416      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0971    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0618    |\n",
      "|    value_loss           | 2.51       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.67777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020574855 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.21       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 85.68279569892474\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 106        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 296        |\n",
      "|    total_timesteps      | 31744      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02012441 |\n",
      "|    clip_fraction        | 0.395      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.42       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.572      |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.064     |\n",
      "|    value_loss           | 2.52       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -122.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.6875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021480013 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.36        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.6489898989899\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020530837 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.442       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0618     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.78921568627452\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019603558 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 90.32857142857142\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 331         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020895507 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.44        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.024      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 91.71296296296296\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021927485 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.166       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 93.09009009009009\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 347         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022523958 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0672     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 94.47587719298245\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020480622 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0566      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0658     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 95.82264957264957\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 364         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022818094 |\n",
      "|    clip_fraction        | 0.44        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.464       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 97.17291666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 372         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020990346 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.131      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.63617886178862\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 381        |\n",
      "|    total_timesteps      | 41984      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02046039 |\n",
      "|    clip_fraction        | 0.375      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.483      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.16       |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0624    |\n",
      "|    value_loss           | 2.39       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.98412698412699\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -371       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 389        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01896058 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.506      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.11       |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | -0.061     |\n",
      "|    value_loss           | 2.27       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 101.1608527131783\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018590707 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.512       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.628       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0629     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 102.47727272727273\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 407         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018938836 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.486       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.266       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 103.80740740740741\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 417         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019507537 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.5         |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 105.08695652173913\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 425         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019629944 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.549       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0184     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.32624113475177\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 434         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018177154 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.235       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 107.53472222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019130409 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.542       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.33        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.68027210884354\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 454        |\n",
      "|    total_timesteps      | 50176      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01843172 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.536      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.06       |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0647    |\n",
      "|    value_loss           | 2.33       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.77333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 462        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01780366 |\n",
      "|    clip_fraction        | 0.338      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.575      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.155     |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.0629    |\n",
      "|    value_loss           | 2.15       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.84640522875817\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 473         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020112656 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.57        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.426       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.88301282051282\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 483         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018911166 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.549       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.448       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 112.84276729559748\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 492         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018605543 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.318       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0677     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.76234567901234\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -369       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 500        |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01788637 |\n",
      "|    clip_fraction        | 0.316      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.598      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.1        |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.061     |\n",
      "|    value_loss           | 2.09       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 114.61060606060606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 509         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017687077 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.577       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 115.42410714285714\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -369       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 518        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01799986 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.576      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0672     |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0615    |\n",
      "|    value_loss           | 2.13       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.17397660818713\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -369       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 57         |\n",
      "|    time_elapsed         | 527        |\n",
      "|    total_timesteps      | 58368      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01669696 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.582      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.22       |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0615    |\n",
      "|    value_loss           | 2.12       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.88218390804597\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 538         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016377132 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.578       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.159       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0629     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 117.51694915254237\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 547         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018527556 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.577       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.589       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0653     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.1375\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -368       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 556        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01808229 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.611      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.188     |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.063     |\n",
      "|    value_loss           | 2.11       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.77732240437159\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 565         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018854452 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.4247311827957\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 574         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018865302 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0667     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.05952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 584         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018011497 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.698       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.58854166666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 592         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019444622 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.597       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.265      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.22051282051282\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 601         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020418046 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0916     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.83333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 611         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019166829 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.9         |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0653     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.33333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 619         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019865219 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.853       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.87990196078431\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -366       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 110        |\n",
      "|    iterations           | 68         |\n",
      "|    time_elapsed         | 628        |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02116742 |\n",
      "|    clip_fraction        | 0.404      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.588      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.337      |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | -0.0706    |\n",
      "|    value_loss           | 2.21       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.41304347826087\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 637         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017400693 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.621       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.91666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 646         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020228028 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.812       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.46361502347418\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 655         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016953457 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.131      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.98032407407408\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 663         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019747691 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.645       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.282       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 1.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.4269406392694\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -365       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 111        |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 672        |\n",
      "|    total_timesteps      | 74752      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01606258 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.642      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.699      |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0649    |\n",
      "|    value_loss           | 2.07       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.84909909909909\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -364       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 111        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 680        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01895722 |\n",
      "|    clip_fraction        | 0.387      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.601      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.586      |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0701    |\n",
      "|    value_loss           | 2.2        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.31555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 688         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018999781 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.202       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0682     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.76315789473684\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 697         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016921401 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.186       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.15259740259741\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 705         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021565128 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.866       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.5630341880342\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 713         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020652799 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.273       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.95042194092827\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 722         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020231932 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.911       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.33854166666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 730         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019961108 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.601       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0672     |\n",
      "|    value_loss           | 1.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.75411522633746\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 400       |\n",
      "|    ep_rew_mean          | -363      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 112       |\n",
      "|    iterations           | 81        |\n",
      "|    time_elapsed         | 738       |\n",
      "|    total_timesteps      | 82944     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0207286 |\n",
      "|    clip_fraction        | 0.423     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.17     |\n",
      "|    explained_variance   | 0.627     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.22      |\n",
      "|    n_updates            | 800       |\n",
      "|    policy_gradient_loss | -0.0732   |\n",
      "|    value_loss           | 2.06      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.1798780487805\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -362       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 747        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01940947 |\n",
      "|    clip_fraction        | 0.402      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.16      |\n",
      "|    explained_variance   | 0.62       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.295      |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.0729    |\n",
      "|    value_loss           | 2          |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.63253012048193\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 755         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019292327 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.624       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.434       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0703     |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.06448412698413\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 763         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022145132 |\n",
      "|    clip_fraction        | 0.44        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.912       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0732     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.5529411764706\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 771         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021111844 |\n",
      "|    clip_fraction        | 0.434       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.169       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0767     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.97868217054264\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 780         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019842694 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.37        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.4434865900383\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 788         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020063266 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.617       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0927      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0731     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.9592803030303\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 796         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018613268 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.274       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.43913857677902\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 805         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021474417 |\n",
      "|    clip_fraction        | 0.448       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.462       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.075      |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.96296296296296\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 813         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019886479 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.168       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0758     |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.4331501831502\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 821         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020489607 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0153      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0743     |\n",
      "|    value_loss           | 2           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.92028985507247\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -357        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 830         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020904625 |\n",
      "|    clip_fraction        | 0.457       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.06       |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0309     |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0788     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.3942652329749\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 838         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019296486 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.488       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0747     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.84751773049646\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 846         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019031554 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.03       |\n",
      "|    explained_variance   | 0.594       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.28        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0746     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.2877192982456\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -355        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 855         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021067327 |\n",
      "|    clip_fraction        | 0.458       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.01       |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0786     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.81163194444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -354        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 863         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020680869 |\n",
      "|    clip_fraction        | 0.458       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8          |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.144      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0772     |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.30927835051546\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -353        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 872         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020410988 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8          |\n",
      "|    explained_variance   | 0.615       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.273       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0747     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.74659863945578\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -352        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 880         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020095417 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.98       |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.874       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0777     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.15740740740742\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -351        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 888         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021444254 |\n",
      "|    clip_fraction        | 0.456       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.96       |\n",
      "|    explained_variance   | 0.594       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0533      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0789     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.58583333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -350       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 114        |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 897        |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02081874 |\n",
      "|    clip_fraction        | 0.421      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.94      |\n",
      "|    explained_variance   | 0.593      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.635      |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.0753    |\n",
      "|    value_loss           | 2.04       |\n",
      "----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -77.76\n",
      "Overall Average Successful Assignments: 98.98217524968075\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf5c617a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -392.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 5.75\n",
      "All assignments history: [16, 13, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | -371     |\n",
      "| time/              |          |\n",
      "|    fps             | 149      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -354.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.5\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009275534 |\n",
      "|    clip_fraction        | 0.079       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.165      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.09        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 15.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -330.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.38888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012158265 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.25       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 12          |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -342.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.041666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013506864 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0367     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -390.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.1\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -375         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 5120         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0137939565 |\n",
      "|    clip_fraction        | 0.242        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | -0.0126      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.6          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0551      |\n",
      "|    value_loss           | 9.65         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.19444444444444\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -374       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 48         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01551627 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0111     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.18       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0588    |\n",
      "|    value_loss           | 8          |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -96.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.67857142857143\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016710503 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.004      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 6.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -210.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.09375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -375        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016809147 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.00305    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 6.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -398.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.129629629629626\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017680727 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | -0.00203    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.788       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 5.3         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -394.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.575\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019742884 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00559     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.652       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0606     |\n",
      "|    value_loss           | 4.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -344.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.64393939393939\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021331316 |\n",
      "|    clip_fraction        | 0.49        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00424     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0665     |\n",
      "|    value_loss           | 3.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -212.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.666666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021629762 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00161     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.06       |\n",
      "|    value_loss           | 3.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -136.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.60897435897436\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 124        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 106        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02400681 |\n",
      "|    clip_fraction        | 0.528      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.00136    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.276     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0634    |\n",
      "|    value_loss           | 2.98       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.61904761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023818828 |\n",
      "|    clip_fraction        | 0.527       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.00424     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.234       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.99444444444445\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025922105 |\n",
      "|    clip_fraction        | 0.585       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00369     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.947       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -184.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.932291666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030020319 |\n",
      "|    clip_fraction        | 0.637       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.00905     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0677     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -288.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.10294117647059\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028848695 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0214      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0883     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.27777777777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 150         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028280614 |\n",
      "|    clip_fraction        | 0.586       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0507      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -274.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.69298245614036\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025462158 |\n",
      "|    clip_fraction        | 0.543       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -284.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.99166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -374        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021412725 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.279      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0579     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -244.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.14285714285714\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -374       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 121        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 177        |\n",
      "|    total_timesteps      | 21504      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02219131 |\n",
      "|    clip_fraction        | 0.464      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.184      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.02       |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0599    |\n",
      "|    value_loss           | 2.78       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 65.60984848484848\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022402145 |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.198       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.236       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 67.6231884057971\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -373        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023369128 |\n",
      "|    clip_fraction        | 0.496       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00604    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 69.39236111111111\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 120        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 203        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02192004 |\n",
      "|    clip_fraction        | 0.454      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.257      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.22       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0636    |\n",
      "|    value_loss           | 2.86       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 70.82\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -373       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 120        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 211        |\n",
      "|    total_timesteps      | 25600      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02023521 |\n",
      "|    clip_fraction        | 0.427      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.268      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.298      |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0617    |\n",
      "|    value_loss           | 2.83       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 71.82051282051282\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021739308 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.305       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 74.21913580246914\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018033732 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.314       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.304       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 76.27083333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 236         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018776236 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.88        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0591     |\n",
      "|    value_loss           | 2.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 77.39080459770115\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 245         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020955514 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.237       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 79.73333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 253         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018823585 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0249     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -96.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.3225806451613\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 261         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018282447 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.35        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.615       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -168.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 81.88541666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 270         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016965924 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.819       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -162.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.46969696969697\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 121        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 278        |\n",
      "|    total_timesteps      | 33792      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01742975 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.337      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.249      |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0579    |\n",
      "|    value_loss           | 2.71       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 82.79656862745098\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015464096 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 2.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -162.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 83.33095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 294         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016366277 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.31        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 84.97685185185185\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 303         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017913204 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.242      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0565     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 86.43693693693693\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 311         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018671427 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 87.48026315789474\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 319         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019244585 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.964       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 2.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 88.76923076923077\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 328         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017191907 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.179       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 89.96875\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 121        |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 336        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01618826 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.399      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.25       |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0582    |\n",
      "|    value_loss           | 2.59       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 91.15650406504065\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 344         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015880281 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.45        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 92.3452380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 352         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017486613 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1           |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 93.46317829457364\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018066894 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.264      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 94.64204545454545\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 121        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 369        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01723435 |\n",
      "|    clip_fraction        | 0.309      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.423      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.288     |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.0575    |\n",
      "|    value_loss           | 2.61       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 95.85925925925926\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -372       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 121        |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 377        |\n",
      "|    total_timesteps      | 46080      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01736503 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.434      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.256     |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.059     |\n",
      "|    value_loss           | 2.7        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 96.93659420289855\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 386         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017058887 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.79        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 2.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 98.08687943262412\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 394         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014397889 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.216      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 99.41840277777777\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -372        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 403         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017290358 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0195      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 100.26700680272108\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 411         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017397068 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 101.44333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 419         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016231805 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0975     |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 102.74673202614379\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 428         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016646829 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0727     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0624     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 103.95192307692308\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -371        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 436         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015261999 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.613       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0598     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 105.27201257861635\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016382556 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.519       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.903       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0612     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 106.50617283950618\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 453         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015193176 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.895       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 107.64393939393939\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -370       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 122        |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 461        |\n",
      "|    total_timesteps      | 56320      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01569869 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.512      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.889      |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0596    |\n",
      "|    value_loss           | 2.42       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 108.78571428571429\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 469         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016964823 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.224       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0577     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 109.89327485380117\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -370        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 477         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016655872 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.519       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.187      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 110.92672413793103\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 486         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016580295 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.545       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0497      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 111.94491525423729\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 494         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016196491 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.516       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0616     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 112.90972222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 502         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016125191 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 113.85655737704919\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 510         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015142246 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.79        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 114.78494623655914\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 519         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018970422 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.511       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 115.63756613756614\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -369        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 527         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015665803 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.107      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 116.49088541666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 535         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017822135 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.506       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 117.31282051282051\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 543         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016735606 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0631     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.07449494949495\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -368        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 552         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016277283 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.506       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 118.66417910447761\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 560         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015331007 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.532       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0578      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 119.38970588235294\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 568         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017636642 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.542       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.1086956521739\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -367        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 576         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015968889 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.522       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.41        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 120.82142857142857\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 585         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016260583 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.44366197183099\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 593         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016276732 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.388       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0642     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 121.9386574074074\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 601         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017050333 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.327       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 122.56849315068493\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -366        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 609         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017407455 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.39        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.14301801801801\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 617         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016129669 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.578       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.985       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 123.67222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 625         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015617316 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.282       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.23574561403508\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 633         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016889846 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 124.8409090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 640         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015979534 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0591     |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 125.4508547008547\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -365        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 648         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015233353 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.04535864978902\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 655         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017571585 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.678       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0657     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 126.62604166666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 663         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017076395 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.591       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0521      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.19753086419753\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -364        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 671         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016723499 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0932      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0658     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 127.7571138211382\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 678         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016703118 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.599       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.537       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.32128514056225\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -363        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 686         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017319832 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.572       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.853       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 128.88194444444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 693         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016354643 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.246       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.42745098039217\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -362        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 701         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016505877 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.427       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 129.9331395348837\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 708         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014418451 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0397      |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.47318007662835\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -361        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013689976 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.387       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 130.99526515151516\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 723         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013585124 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0153      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 131.4943820224719\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -360        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 730         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017053641 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.363       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.01203703703703\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -359        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 737         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015602584 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.622       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.155       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0665     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 132.5265567765568\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -358        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 744         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014224531 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.607       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.224       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.0108695652174\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -357        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 751         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015417134 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.03       |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.8         |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 133.54121863799284\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 758         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016937256 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.553       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.569       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.04787234042553\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -356        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 764         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017311007 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.01       |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 134.58157894736843\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | -355         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 771          |\n",
      "|    total_timesteps      | 97280        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0155531475 |\n",
      "|    clip_fraction        | 0.304        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8           |\n",
      "|    explained_variance   | 0.558        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.913        |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0672      |\n",
      "|    value_loss           | 2.16         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.06076388888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -354        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 778         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016333686 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.98       |\n",
      "|    explained_variance   | 0.545       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0283      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0729     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 135.56615120274915\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 400        |\n",
      "|    ep_rew_mean          | -354       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 784        |\n",
      "|    total_timesteps      | 99328      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01823219 |\n",
      "|    clip_fraction        | 0.396      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -7.97      |\n",
      "|    explained_variance   | 0.565      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.163      |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0744    |\n",
      "|    value_loss           | 2.11       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.0841836734694\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -353        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 791         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016442172 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.96       |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.452       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 136.59848484848484\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -352        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 797         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018553358 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.582       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.241       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0756     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 137.10666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | -352        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 803         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015625842 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.92       |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0488      |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.073      |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -78.68\n",
      "Overall Average Successful Assignments: 95.79366296663945\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.01.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks400.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295af3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
