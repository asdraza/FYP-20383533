{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26bbdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -192.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 4.266666666666667\n",
      "All assignments history: [6, 7, 5, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -190     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008834934 |\n",
      "|    clip_fraction        | 0.0689      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.225      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.27        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 16.8        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011975803 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0915     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0598     |\n",
      "|    value_loss           | 11.1        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -184.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.0\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -185         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0132727865 |\n",
      "|    clip_fraction        | 0.23         |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | -0.025       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.7          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0605      |\n",
      "|    value_loss           | 9.19         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -166.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014738515 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0688      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.01        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 8.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.277777777777779\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014772854 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0676      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.42        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 7.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.17142857142857\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016122986 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0946      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.79        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 6.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.075\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 120        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 67         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01744662 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.103      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.74       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0614    |\n",
      "|    value_loss           | 6.09       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -196.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.792592592592593\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017671555 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.377       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0593     |\n",
      "|    value_loss           | 5.45        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.28\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020312376 |\n",
      "|    clip_fraction        | 0.455       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 4.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.909090909090908\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021407079 |\n",
      "|    clip_fraction        | 0.477       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 4.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -176.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.561111111111112\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021271884 |\n",
      "|    clip_fraction        | 0.452       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.231       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.929       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 4.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -162.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.646153846153846\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023635946 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.263       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0679     |\n",
      "|    value_loss           | 4.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.752380952380953\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024976104 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.496       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 4.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -134.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.377777777777776\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027089475 |\n",
      "|    clip_fraction        | 0.579       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.233       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 4.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.766666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028180292 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 4.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -152.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.87450980392157\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 118        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 147        |\n",
      "|    total_timesteps      | 17408      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02754188 |\n",
      "|    clip_fraction        | 0.574      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.415      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 4.75       |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0659    |\n",
      "|    value_loss           | 4.43       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.507407407407406\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 155         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027482104 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 4.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.642105263157895\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028236046 |\n",
      "|    clip_fraction        | 0.591       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 4.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.31\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027231213 |\n",
      "|    clip_fraction        | 0.566       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.376       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0679     |\n",
      "|    value_loss           | 4.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.136507936507936\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025449865 |\n",
      "|    clip_fraction        | 0.547       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.514       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.407       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 4.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.921212121212122\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 119        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 188        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02558305 |\n",
      "|    clip_fraction        | 0.531      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.533      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.59       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0698    |\n",
      "|    value_loss           | 4.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.49855072463768\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023542248 |\n",
      "|    clip_fraction        | 0.508       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.97        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.07       |\n",
      "|    value_loss           | 4.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.72222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 206         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021484807 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.222       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 3.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.925333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021404654 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 3.7         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.253846153846155\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 119        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 223        |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01869446 |\n",
      "|    clip_fraction        | 0.398      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.641      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.628      |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0648    |\n",
      "|    value_loss           | 3.74       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.469135802469136\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 231         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021004416 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.78        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 3.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.916666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 239         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019585297 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.86        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 3.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.23448275862069\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 119        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 247        |\n",
      "|    total_timesteps      | 29696      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01932421 |\n",
      "|    clip_fraction        | 0.387      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.701      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.99       |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0661    |\n",
      "|    value_loss           | 3.37       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.962222222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 255         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019579437 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.22        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.61075268817204\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 264         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018176094 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.878       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0631     |\n",
      "|    value_loss           | 3.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.0875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019069145 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 3.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.57171717171717\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017434403 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 3.18        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.107843137254903\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 289         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016986107 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0904     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.56\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 297         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015565397 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.016666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016495995 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.14        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0646     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.454054054054055\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017329494 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 2.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.182456140350876\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017587222 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.9008547008547\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 332         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017439192 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0672     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.465\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 341         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016795516 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.96747967479675\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -182      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 119       |\n",
      "|    iterations           | 41        |\n",
      "|    time_elapsed         | 350       |\n",
      "|    total_timesteps      | 41984     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0179614 |\n",
      "|    clip_fraction        | 0.332     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.26     |\n",
      "|    explained_variance   | 0.8       |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.57      |\n",
      "|    n_updates            | 400       |\n",
      "|    policy_gradient_loss | -0.0656   |\n",
      "|    value_loss           | 2.63      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.377777777777776\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 119        |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 359        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01893273 |\n",
      "|    clip_fraction        | 0.388      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.811      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0486    |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | -0.0697    |\n",
      "|    value_loss           | 2.39       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.666666666666668\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -182      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 119       |\n",
      "|    iterations           | 43        |\n",
      "|    time_elapsed         | 367       |\n",
      "|    total_timesteps      | 44032     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0172632 |\n",
      "|    clip_fraction        | 0.337     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.26     |\n",
      "|    explained_variance   | 0.816     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.294     |\n",
      "|    n_updates            | 420       |\n",
      "|    policy_gradient_loss | -0.0665   |\n",
      "|    value_loss           | 2.36      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.883333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 119        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 375        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01880328 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.833      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.59       |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.0705    |\n",
      "|    value_loss           | 2.16       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.216296296296296\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018022526 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.197       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.518840579710144\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 392         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017928297 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0284      |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.802836879432625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 400         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016915381 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0687      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 2.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.111111111111114\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 120        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 408        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01761493 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.861      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.91       |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0688    |\n",
      "|    value_loss           | 1.97       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.331972789115646\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 417         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018081252 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.161       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.576\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 425         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017718416 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.112      |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0706     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.84313725490196\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 434         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018618952 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.544       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 1.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.3474358974359\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 443         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019325038 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.238       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.83396226415094\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 453         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019534908 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.157       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0742     |\n",
      "|    value_loss           | 1.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.22592592592593\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -180      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 119       |\n",
      "|    iterations           | 54        |\n",
      "|    time_elapsed         | 461       |\n",
      "|    total_timesteps      | 55296     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0177862 |\n",
      "|    clip_fraction        | 0.359     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.24     |\n",
      "|    explained_variance   | 0.888     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.559     |\n",
      "|    n_updates            | 530       |\n",
      "|    policy_gradient_loss | -0.0728   |\n",
      "|    value_loss           | 1.65      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.70787878787879\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 470         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017696982 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.817       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 1.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.01428571428571\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 479         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016618717 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.218       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 1.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.421052631578945\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 490         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017913679 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0129      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0722     |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.795402298850576\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 501         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017994465 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.148      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0724     |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.16949152542373\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 511         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019042958 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.642       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0752     |\n",
      "|    value_loss           | 1.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.48888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 521         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019226156 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.301       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0738     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.77704918032787\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 532         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020305753 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.204      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0747     |\n",
      "|    value_loss           | 1.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.005376344086024\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 542         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017878588 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.524       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0745     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.24021164021164\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 552         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019484457 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.202      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0793     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.63333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 561         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018229634 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.242       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0755     |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.03897435897436\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 571         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020238332 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.287      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0766     |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.412121212121214\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 581         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019118506 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00398    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0792     |\n",
      "|    value_loss           | 1.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.85572139303483\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 590         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019484662 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.286      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0777     |\n",
      "|    value_loss           | 1.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.21470588235294\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 600         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017742578 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0265      |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 1.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.66086956521739\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 609         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019595597 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.136      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0768     |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.09428571428571\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 619         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019851469 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.177       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0788     |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.6037558685446\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 115        |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 628        |\n",
      "|    total_timesteps      | 72704      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01873893 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.921      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0613     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0775    |\n",
      "|    value_loss           | 1.3        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.14537037037037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 637         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018899117 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0367      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0745     |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.68036529680365\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 115        |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 648        |\n",
      "|    total_timesteps      | 74752      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01849572 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.925      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.133      |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0745    |\n",
      "|    value_loss           | 1.26       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.236936936936935\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 658         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019951018 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.304      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0782     |\n",
      "|    value_loss           | 1.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.71111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 667         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020422462 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.237      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0814     |\n",
      "|    value_loss           | 1.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.174561403508775\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 677         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021037892 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0727     |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.084      |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.64935064935065\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 688         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019046865 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.163      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0797     |\n",
      "|    value_loss           | 1.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.13333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 697         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017866384 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.04        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0798     |\n",
      "|    value_loss           | 1.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.6337552742616\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 707         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019081198 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.109      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0779     |\n",
      "|    value_loss           | 1.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.12583333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 716         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021045681 |\n",
      "|    clip_fraction        | 0.432       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0841     |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0812     |\n",
      "|    value_loss           | 1.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.623045267489715\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 725         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019807119 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.142      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0793     |\n",
      "|    value_loss           | 1.15        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.1219512195122\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 735         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021495694 |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.321      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0838     |\n",
      "|    value_loss           | 1           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.58152610441767\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 744         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018315252 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0803      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0775     |\n",
      "|    value_loss           | 1.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.02460317460317\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 753         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017565142 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00322     |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.079      |\n",
      "|    value_loss           | 1.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.44627450980392\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 763         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018939473 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.135      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0796     |\n",
      "|    value_loss           | 1.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.88139534883721\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 772        |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01665417 |\n",
      "|    clip_fraction        | 0.349      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.18      |\n",
      "|    explained_variance   | 0.929      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.273     |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.0783    |\n",
      "|    value_loss           | 1.2        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.28659003831417\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 782         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019708192 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.199       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0829     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.693939393939395\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 790         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020728525 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.231      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0794     |\n",
      "|    value_loss           | 1.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.0749063670412\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 799         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020576984 |\n",
      "|    clip_fraction        | 0.442       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.159      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0857     |\n",
      "|    value_loss           | 0.942       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.434074074074076\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 809         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019470362 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0142     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.082      |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.794871794871796\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 817         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018951619 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.264      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0814     |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.11014492753623\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 828         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017820416 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.276      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0802     |\n",
      "|    value_loss           | 1.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.42939068100358\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 837         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023421867 |\n",
      "|    clip_fraction        | 0.473       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.182      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.085      |\n",
      "|    value_loss           | 1.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.806382978723406\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 846         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017984977 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0826     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0805     |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.19157894736842\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -172       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 857        |\n",
      "|    total_timesteps      | 97280      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01712209 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.14      |\n",
      "|    explained_variance   | 0.933      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0812    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0806    |\n",
      "|    value_loss           | 1.08       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.545138888888886\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -172       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 96         |\n",
      "|    time_elapsed         | 868        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02053162 |\n",
      "|    clip_fraction        | 0.428      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.13      |\n",
      "|    explained_variance   | 0.936      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0741     |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.0835    |\n",
      "|    value_loss           | 1.02       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.89965635738832\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 877         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020940345 |\n",
      "|    clip_fraction        | 0.433       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.231      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0832     |\n",
      "|    value_loss           | 1.1         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.22244897959184\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 887         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020743836 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.268      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0837     |\n",
      "|    value_loss           | 1.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.53400673400673\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 897         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019654231 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0521     |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0839     |\n",
      "|    value_loss           | 1.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.86533333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -171       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 906        |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01905809 |\n",
      "|    clip_fraction        | 0.393      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.1       |\n",
      "|    explained_variance   | 0.936      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.232     |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.0824    |\n",
      "|    value_loss           | 1.02       |\n",
      "----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -52.02\n",
      "Overall Average Successful Assignments: 32.53568418133415\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c1627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -196.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 3.6\n",
      "All assignments history: [5, 7, 8, 8, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    fps             | 156      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -156.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.333333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009220557 |\n",
      "|    clip_fraction        | 0.0673      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.195      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.04        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 15.2        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012137878 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0192     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 10.8        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.1\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013185285 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.00166    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.24        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 9.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.653333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013767006 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0468      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.33        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 8.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.466666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015380889 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0559      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 7.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.466666666666665\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 65         |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01608657 |\n",
      "|    clip_fraction        | 0.336      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0594     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.13       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0588    |\n",
      "|    value_loss           | 6.71       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.908333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017777745 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0635      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 5.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.42222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018290175 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0768      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.92        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 5.55        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.0\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021149626 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0964      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 4.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.90909090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022699725 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.419       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 4.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -194.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.85\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023055002 |\n",
      "|    clip_fraction        | 0.497       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 4.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.42051282051282\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023158278 |\n",
      "|    clip_fraction        | 0.51        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.812       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 4.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.7\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024849242 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.744       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0665     |\n",
      "|    value_loss           | 4.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.586666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023235159 |\n",
      "|    clip_fraction        | 0.504       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 4.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.7625\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024146384 |\n",
      "|    clip_fraction        | 0.511       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0286      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0659     |\n",
      "|    value_loss           | 4.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.88627450980392\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023588559 |\n",
      "|    clip_fraction        | 0.514       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.74        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0703     |\n",
      "|    value_loss           | 4.45        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.18518518518518\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 169        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02128157 |\n",
      "|    clip_fraction        | 0.44       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.484      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.972      |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0638    |\n",
      "|    value_loss           | 4.18       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.189473684210526\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 179         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021547463 |\n",
      "|    clip_fraction        | 0.454       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.51        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 4.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.95\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 189         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022750916 |\n",
      "|    clip_fraction        | 0.487       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 4.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.37460317460317\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 198         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023938147 |\n",
      "|    clip_fraction        | 0.497       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.551       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0716     |\n",
      "|    value_loss           | 4.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.27272727272727\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 207         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023016047 |\n",
      "|    clip_fraction        | 0.484       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.655       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 4.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 217         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022668483 |\n",
      "|    clip_fraction        | 0.474       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.615       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 3.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.916666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 226         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022891846 |\n",
      "|    clip_fraction        | 0.49        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.14        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0731     |\n",
      "|    value_loss           | 3.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.17333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 235        |\n",
      "|    total_timesteps      | 25600      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02176237 |\n",
      "|    clip_fraction        | 0.444      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.624      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.757      |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0705    |\n",
      "|    value_loss           | 4          |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.54615384615385\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 245        |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02152161 |\n",
      "|    clip_fraction        | 0.449      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.655      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.357      |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0695    |\n",
      "|    value_loss           | 3.65       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.75555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 255         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021909554 |\n",
      "|    clip_fraction        | 0.452       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.536       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 3.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.08571428571429\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -181      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 108       |\n",
      "|    iterations           | 28        |\n",
      "|    time_elapsed         | 263       |\n",
      "|    total_timesteps      | 28672     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0218334 |\n",
      "|    clip_fraction        | 0.438     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.26     |\n",
      "|    explained_variance   | 0.662     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1.32      |\n",
      "|    n_updates            | 270       |\n",
      "|    policy_gradient_loss | -0.0708   |\n",
      "|    value_loss           | 3.71      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.44827586206897\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 273         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021567546 |\n",
      "|    clip_fraction        | 0.452       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.422       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 3.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.77333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 283         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021251125 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.174       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.21075268817204\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 293        |\n",
      "|    total_timesteps      | 31744      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02037882 |\n",
      "|    clip_fraction        | 0.402      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.699      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.607      |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0691    |\n",
      "|    value_loss           | 3.58       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.635416666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019378878 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.427       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.04646464646465\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020693256 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0736     |\n",
      "|    value_loss           | 3.41        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.431372549019606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019182649 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 3.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.817142857142855\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020828508 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.45        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 3.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.17777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018449474 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.957       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0713     |\n",
      "|    value_loss           | 3.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.43963963963964\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 348        |\n",
      "|    total_timesteps      | 37888      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02112904 |\n",
      "|    clip_fraction        | 0.418      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.739      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.433      |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0714    |\n",
      "|    value_loss           | 3.13       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.708771929824564\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018677706 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.942       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0713     |\n",
      "|    value_loss           | 3.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.84273504273504\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 365         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019601513 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.717       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0752     |\n",
      "|    value_loss           | 3.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.005\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018134234 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 2.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.167479674796745\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 384         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015169746 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.493       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 2.96        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.56825396825397\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 395         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014906834 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.973643410852716\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 404         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015711779 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.817       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.068      |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.403030303030306\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 414         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015273828 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.82962962962963\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 424        |\n",
      "|    total_timesteps      | 46080      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01580881 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.787      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.542      |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0656    |\n",
      "|    value_loss           | 2.71       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.27101449275362\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 434         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015598545 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.28        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.721985815602835\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016837219 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.523       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.13055555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015850414 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.468       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.521088435374146\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 465         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017232426 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.408       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0724     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.95466666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 475         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015305791 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.272       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.36993464052288\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 485         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016505191 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.595       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.792307692307695\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 495         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017048195 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.774       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.22389937106918\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 53         |\n",
      "|    time_elapsed         | 505        |\n",
      "|    total_timesteps      | 54272      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01900625 |\n",
      "|    clip_fraction        | 0.382      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.816      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.377      |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0745    |\n",
      "|    value_loss           | 2.32       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.64567901234568\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 514         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017261723 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.29        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.06909090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 523         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017774615 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.668       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0718     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.46190476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 533         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017658645 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.386       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0732     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.8093567251462\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 543         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017579429 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.895       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0725     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.16896551724138\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 552         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017318651 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.239       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0736     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.53220338983051\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 562         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016933937 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.467       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 2.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.89111111111111\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 571        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01673697 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.855      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.74       |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0712    |\n",
      "|    value_loss           | 2.05       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.24590163934426\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 581         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015646473 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.201       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.57419354838709\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 590         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015340529 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.394       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.90899470899471\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 600         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016273823 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.255       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.073      |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.18020833333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 610         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019460553 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.267       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.078      |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.459487179487176\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 619        |\n",
      "|    total_timesteps      | 66560      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01815004 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.857      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.11       |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.078     |\n",
      "|    value_loss           | 1.96       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.71313131313131\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 629         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017789854 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00323     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.077      |\n",
      "|    value_loss           | 1.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.98407960199005\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 639         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017114494 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0426      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0735     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.29607843137255\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 68         |\n",
      "|    time_elapsed         | 649        |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02003723 |\n",
      "|    clip_fraction        | 0.395      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.876      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0259    |\n",
      "|    n_updates            | 670        |\n",
      "|    policy_gradient_loss | -0.0774    |\n",
      "|    value_loss           | 1.76       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.5487922705314\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 658         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016217846 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.473       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.78476190476191\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 668         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017517803 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.418       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0746     |\n",
      "|    value_loss           | 1.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.04037558685446\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 677        |\n",
      "|    total_timesteps      | 72704      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01856165 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.88       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.00431    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.077     |\n",
      "|    value_loss           | 1.73       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.29351851851852\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -177       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 687        |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01901472 |\n",
      "|    clip_fraction        | 0.392      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.881      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.0786    |\n",
      "|    value_loss           | 1.66       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.56712328767123\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 696         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018474383 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0775     |\n",
      "|    value_loss           | 1.62        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.81531531531532\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 706         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018504765 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.142       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.078      |\n",
      "|    value_loss           | 1.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.086222222222226\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017570393 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.319       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0773     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.35614035087719\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 725         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020281676 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.045      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0811     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.60346320346321\n",
      "All assignments history: []\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -176     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 107      |\n",
      "|    iterations           | 77       |\n",
      "|    time_elapsed         | 734      |\n",
      "|    total_timesteps      | 78848    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.01721  |\n",
      "|    clip_fraction        | 0.365    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | -8.18    |\n",
      "|    explained_variance   | 0.888    |\n",
      "|    learning_rate        | 0.00018  |\n",
      "|    loss                 | -0.169   |\n",
      "|    n_updates            | 760      |\n",
      "|    policy_gradient_loss | -0.08    |\n",
      "|    value_loss           | 1.58     |\n",
      "--------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.87008547008547\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 744        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01973091 |\n",
      "|    clip_fraction        | 0.433      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.18      |\n",
      "|    explained_variance   | 0.888      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.328      |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.0832    |\n",
      "|    value_loss           | 1.56       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.129957805907175\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 79         |\n",
      "|    time_elapsed         | 753        |\n",
      "|    total_timesteps      | 80896      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01915665 |\n",
      "|    clip_fraction        | 0.404      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0711    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0816    |\n",
      "|    value_loss           | 1.72       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.38\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 763         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019539136 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.131       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0818     |\n",
      "|    value_loss           | 1.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 773         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017770922 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.119       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0784     |\n",
      "|    value_loss           | 1.64        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.82032520325203\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 782         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018818716 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.311       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0815     |\n",
      "|    value_loss           | 1.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.99598393574297\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 791         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020283561 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.214       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0825     |\n",
      "|    value_loss           | 1.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.16587301587302\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 801         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019229904 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0864      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0819     |\n",
      "|    value_loss           | 1.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.31686274509804\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 810         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020773096 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.216       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0819     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.47131782945736\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 819         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018106947 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.203       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0812     |\n",
      "|    value_loss           | 1.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.657471264367814\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 829         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018711984 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.091       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0827     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.84166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 838         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019480582 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0827     |\n",
      "|    value_loss           | 1.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.0187265917603\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 848         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017974406 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0796     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.23777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 857         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021113224 |\n",
      "|    clip_fraction        | 0.439       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.214       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0842     |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.42564102564103\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 867         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018403215 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.478       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0807     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.62246376811594\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -172       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 92         |\n",
      "|    time_elapsed         | 876        |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01867547 |\n",
      "|    clip_fraction        | 0.413      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.07      |\n",
      "|    explained_variance   | 0.892      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.014     |\n",
      "|    n_updates            | 910        |\n",
      "|    policy_gradient_loss | -0.083     |\n",
      "|    value_loss           | 1.56       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.840860215053766\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -171       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 886        |\n",
      "|    total_timesteps      | 95232      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01919004 |\n",
      "|    clip_fraction        | 0.411      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.07      |\n",
      "|    explained_variance   | 0.898      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.181      |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.085     |\n",
      "|    value_loss           | 1.49       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.04751773049645\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -171       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 896        |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02118487 |\n",
      "|    clip_fraction        | 0.464      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.05      |\n",
      "|    explained_variance   | 0.902      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.12       |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0881    |\n",
      "|    value_loss           | 1.33       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.256140350877196\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 907         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022635855 |\n",
      "|    clip_fraction        | 0.483       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.04       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0222      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0883     |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.46805555555556\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -169       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 96         |\n",
      "|    time_elapsed         | 916        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02093542 |\n",
      "|    clip_fraction        | 0.46       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.02      |\n",
      "|    explained_variance   | 0.888      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.0874    |\n",
      "|    value_loss           | 1.57       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.67422680412371\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 925         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017157152 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8          |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0286      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0816     |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.86598639455782\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 934         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019369101 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.98       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.133      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.085      |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.053872053872055\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 945         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018533796 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.96       |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.115      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0831     |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.25533333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 954         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020076033 |\n",
      "|    clip_fraction        | 0.43        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.182       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0854     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -21.68\n",
      "Overall Average Successful Assignments: 47.89564763828915\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3f6ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 7.266666666666667\n",
      "All assignments history: [6, 10, 11, 11, 11, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -180     |\n",
      "| time/              |          |\n",
      "|    fps             | 144      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.133333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008556066 |\n",
      "|    clip_fraction        | 0.0545      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.00934     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.37        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 15.8        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.08888888888889\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -181         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 115          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0120214615 |\n",
      "|    clip_fraction        | 0.177        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | 0.00465      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.26         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0604      |\n",
      "|    value_loss           | 11.5         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.9\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -183         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 113          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128729455 |\n",
      "|    clip_fraction        | 0.221        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | -0.0628      |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.72         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0609      |\n",
      "|    value_loss           | 9.7          |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.96\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013117511 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0301      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0569     |\n",
      "|    value_loss           | 9.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.7\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014739849 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0423      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.31        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0579     |\n",
      "|    value_loss           | 8.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.16190476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016045863 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.053       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.08        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 7.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.091666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016766433 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0598      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 6.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.822222222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017670317 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0632      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 5.88        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.433333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019627456 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0668      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 5.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.375757575757575\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021141091 |\n",
      "|    clip_fraction        | 0.465       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0926      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 5.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -166.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.83888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021896154 |\n",
      "|    clip_fraction        | 0.477       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0375     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.062      |\n",
      "|    value_loss           | 4.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.16923076923077\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023271386 |\n",
      "|    clip_fraction        | 0.51        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.27        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 4.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -166.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.804761904761904\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 131        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02400472 |\n",
      "|    clip_fraction        | 0.509      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.247      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.6        |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0661    |\n",
      "|    value_loss           | 4.6        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.502222222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024698772 |\n",
      "|    clip_fraction        | 0.545       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.41        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 4.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.958333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023188323 |\n",
      "|    clip_fraction        | 0.491       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.38        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 4.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.92549019607843\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023739727 |\n",
      "|    clip_fraction        | 0.512       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.755       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 4.33        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.94814814814815\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 170        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02196468 |\n",
      "|    clip_fraction        | 0.484      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.462      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.97       |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0661    |\n",
      "|    value_loss           | 4.21       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.431578947368422\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 179        |\n",
      "|    total_timesteps      | 19456      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02215238 |\n",
      "|    clip_fraction        | 0.48       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.506      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.721      |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.069     |\n",
      "|    value_loss           | 4.13       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.986666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020807998 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 4.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.752380952380953\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 201         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020047117 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.585       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 4           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.060606060606062\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019524619 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.599       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 3.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.756521739130434\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017877799 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.617       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.932       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 3.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.641666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 229         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018305112 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.624       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.942       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 3.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.75733333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 239         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018468592 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 3.66        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.766666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 249         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018124592 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.407       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.065      |\n",
      "|    value_loss           | 3.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.02716049382716\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018312447 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.21        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 3.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.04047619047619\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 270         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017521573 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.15862068965517\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 280         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018224668 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 3.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.217777777777776\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 290         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017319122 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.45        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 3.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.13763440860215\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 299         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017728701 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.858       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0657     |\n",
      "|    value_loss           | 3.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.2875\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017627705 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.651       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 3.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.2969696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 318         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017980061 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.726       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.22352941176471\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 106        |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 328        |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01563276 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.715      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.39       |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.0674    |\n",
      "|    value_loss           | 3.23       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.034285714285716\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 106        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 338        |\n",
      "|    total_timesteps      | 35840      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01590832 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.742      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.14       |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0668    |\n",
      "|    value_loss           | 2.98       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.57592592592592\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 105        |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 348        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01796922 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.747      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.522      |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.0693    |\n",
      "|    value_loss           | 2.94       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.909909909909906\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 357         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017100507 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.51        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.26315789473684\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 106        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 366        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01738232 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.759      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.42       |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0691    |\n",
      "|    value_loss           | 2.94       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.65128205128205\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 376         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018682968 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.327       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 2.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.92\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 385         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017320657 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.705       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.30731707317073\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 395         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017489545 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.308       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.888888888888886\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 405         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018540595 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.40775193798449\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 413         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018714499 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.207       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.94090909090909\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 423         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020173777 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.634       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0735     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.266666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 433         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017913632 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.769       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.65797101449275\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 106        |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 442        |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01750714 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.846      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.36       |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.0715    |\n",
      "|    value_loss           | 2.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.02127659574468\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 452         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017888896 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0797      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0732     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.547222222222224\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 461         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019273408 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0957     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.137414965986395\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021493655 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.13       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0773     |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.67733333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 481         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016516412 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.18562091503268\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 490         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018058872 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.584       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0739     |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.60641025641026\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 499         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020532189 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.719       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0768     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.940880503144655\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 508         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018432785 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.426       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0744     |\n",
      "|    value_loss           | 1.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.17160493827161\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 516         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019715408 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.164       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0758     |\n",
      "|    value_loss           | 1.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.39151515151515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 524         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017651124 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.11       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 1.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.56547619047619\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 107        |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 532        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01571209 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.275      |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0731    |\n",
      "|    value_loss           | 1.8        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.670175438596495\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 540         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018103369 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.185      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0778     |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.73103448275862\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 549         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019360548 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.141      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0784     |\n",
      "|    value_loss           | 1.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.80564971751412\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 108        |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 557        |\n",
      "|    total_timesteps      | 60416      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01808132 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.355      |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0769    |\n",
      "|    value_loss           | 1.44       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.943333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 565         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019038096 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.228      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0789     |\n",
      "|    value_loss           | 1.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.24699453551913\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 573         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020233419 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0618     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0811     |\n",
      "|    value_loss           | 1.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.54838709677419\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 581         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020120539 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.171       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0789     |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.81269841269841\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 590         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018671907 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.165      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.078      |\n",
      "|    value_loss           | 1.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.059375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 598         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019982032 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.197      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0801     |\n",
      "|    value_loss           | 1.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.35589743589743\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 606         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017685063 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0412     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0793     |\n",
      "|    value_loss           | 1.39        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.65858585858586\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 614         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018525463 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0252     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0793     |\n",
      "|    value_loss           | 1.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.95522388059702\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 622         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019228244 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00299     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0802     |\n",
      "|    value_loss           | 1.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.227450980392156\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 629         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020443814 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.224      |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0785     |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.43864734299517\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 637         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019465433 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.09       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0783     |\n",
      "|    value_loss           | 1.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.70095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 645         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020049796 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.225      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0817     |\n",
      "|    value_loss           | 1.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.985915492957744\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 653         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019678742 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.124       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0811     |\n",
      "|    value_loss           | 1.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.21296296296296\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 660         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021329764 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.2        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0822     |\n",
      "|    value_loss           | 1.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.36803652968037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 668         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020217448 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.178      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0815     |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.57297297297297\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 676         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017975684 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.134      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0777     |\n",
      "|    value_loss           | 1.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.79911111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 684         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020076934 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0898     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.085      |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.03333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 691         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019374497 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.189      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0804     |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.275324675324676\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 699         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020718418 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.263      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.083      |\n",
      "|    value_loss           | 1.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.505982905982904\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 707        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02130017 |\n",
      "|    clip_fraction        | 0.444      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.936      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.11      |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.0863    |\n",
      "|    value_loss           | 1.1        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.753586497890296\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020352336 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.276      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0854     |\n",
      "|    value_loss           | 1.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.035\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 722         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019273162 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.235      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0795     |\n",
      "|    value_loss           | 1.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.29382716049383\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 730         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021233736 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.176      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0862     |\n",
      "|    value_loss           | 1           |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.55365853658537\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 113         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 738         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019846207 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0369     |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0811     |\n",
      "|    value_loss           | 1.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.77028112449799\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -177       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 113        |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 746        |\n",
      "|    total_timesteps      | 84992      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02060384 |\n",
      "|    clip_fraction        | 0.426      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.267     |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0833    |\n",
      "|    value_loss           | 0.995      |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.98809523809524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 753         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019601084 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.345      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0831     |\n",
      "|    value_loss           | 0.963       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.189019607843136\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 761         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017943611 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.221      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0788     |\n",
      "|    value_loss           | 1.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.4015503875969\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -177       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 114        |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 769        |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01847215 |\n",
      "|    clip_fraction        | 0.384      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.937      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.129     |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.0816    |\n",
      "|    value_loss           | 1.15       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.58544061302682\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 776         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018462718 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0302      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0786     |\n",
      "|    value_loss           | 1.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.7469696969697\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -177      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 114       |\n",
      "|    iterations           | 88        |\n",
      "|    time_elapsed         | 783       |\n",
      "|    total_timesteps      | 90112     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0209782 |\n",
      "|    clip_fraction        | 0.426     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.19     |\n",
      "|    explained_variance   | 0.944     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | -0.219    |\n",
      "|    n_updates            | 870       |\n",
      "|    policy_gradient_loss | -0.0839   |\n",
      "|    value_loss           | 1.03      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.95280898876405\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 115        |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 791        |\n",
      "|    total_timesteps      | 91136      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01928708 |\n",
      "|    clip_fraction        | 0.41       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.951      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.189     |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0844    |\n",
      "|    value_loss           | 0.907      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.09407407407407\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 798         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019357506 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0679     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0851     |\n",
      "|    value_loss           | 1.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.25787545787546\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 806         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017894534 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.233      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0765     |\n",
      "|    value_loss           | 1.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.3963768115942\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 813         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019149745 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0217     |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0822     |\n",
      "|    value_loss           | 1.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.5326164874552\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 115         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 821         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022452554 |\n",
      "|    clip_fraction        | 0.454       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.324      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0869     |\n",
      "|    value_loss           | 0.851       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.68014184397163\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 828         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021144908 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.254      |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0839     |\n",
      "|    value_loss           | 0.945       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.80982456140351\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -174       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 116        |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 836        |\n",
      "|    total_timesteps      | 97280      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01713296 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.16      |\n",
      "|    explained_variance   | 0.934      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0891    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0796    |\n",
      "|    value_loss           | 1.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.953472222222224\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -174       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 116        |\n",
      "|    iterations           | 96         |\n",
      "|    time_elapsed         | 843        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02018198 |\n",
      "|    clip_fraction        | 0.433      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.16      |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.191     |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.0864    |\n",
      "|    value_loss           | 1.04       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.07491408934708\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 850         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019220795 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0416     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0855     |\n",
      "|    value_loss           | 1.17        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.220408163265304\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 858         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020734066 |\n",
      "|    clip_fraction        | 0.455       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0902     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0874     |\n",
      "|    value_loss           | 0.981       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.38114478114478\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 866         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018392777 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.137      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0808     |\n",
      "|    value_loss           | 1.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.53933333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 117         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 873         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018918088 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.203      |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.085      |\n",
      "|    value_loss           | 1.07        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -34.68\n",
      "Overall Average Successful Assignments: 42.468112254121095\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523807b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 9.933333333333334\n",
      "All assignments history: [7, 8, 11, 13, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -180     |\n",
      "| time/              |          |\n",
      "|    fps             | 197      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.666666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009309849 |\n",
      "|    clip_fraction        | 0.0719      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0769     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 15.6        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.533333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012694137 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0218     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0615     |\n",
      "|    value_loss           | 11.6        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.916666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 148         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012936197 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.015       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.059      |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.786666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014043413 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0533      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.25        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 9.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.8\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016289432 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.058       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.26        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 7.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.228571428571428\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 144        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 49         |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01685344 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0585     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.97       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0626    |\n",
      "|    value_loss           | 7.08       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.866666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 144         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017785117 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0682      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 6.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -84.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.214814814814815\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018122284 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0696      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.89        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 5.76        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -156.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.72\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 143        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 71         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01987244 |\n",
      "|    clip_fraction        | 0.444      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.0773     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.64       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0629    |\n",
      "|    value_loss           | 5.06       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -200.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.903030303030302\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021989664 |\n",
      "|    clip_fraction        | 0.491       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 4.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.16111111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022156637 |\n",
      "|    clip_fraction        | 0.494       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.448       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 4.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.4\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022957284 |\n",
      "|    clip_fraction        | 0.521       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 4.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.733333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025360368 |\n",
      "|    clip_fraction        | 0.571       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.708       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 4.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.817777777777778\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 142        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 107        |\n",
      "|    total_timesteps      | 15360      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02499966 |\n",
      "|    clip_fraction        | 0.557      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.244      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.25       |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.063     |\n",
      "|    value_loss           | 4.71       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.616666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 142        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 114        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02674806 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.265      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 4.39       |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0671    |\n",
      "|    value_loss           | 4.87       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.129411764705882\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 142        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 122        |\n",
      "|    total_timesteps      | 17408      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02336498 |\n",
      "|    clip_fraction        | 0.509      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.314      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.92       |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0603    |\n",
      "|    value_loss           | 4.75       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.01851851851852\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024129447 |\n",
      "|    clip_fraction        | 0.524       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 4.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.4\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 142        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 136        |\n",
      "|    total_timesteps      | 19456      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02235144 |\n",
      "|    clip_fraction        | 0.479      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.349      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.97       |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0646    |\n",
      "|    value_loss           | 4.82       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.976666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021397242 |\n",
      "|    clip_fraction        | 0.441       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.73        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 4.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.06984126984127\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019410785 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.663       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 4.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.30909090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019970499 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0639     |\n",
      "|    value_loss           | 4.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.081159420289854\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017935093 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0589     |\n",
      "|    value_loss           | 4.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.755555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019382134 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.853       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 4.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.770666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 142        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 180        |\n",
      "|    total_timesteps      | 25600      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02014887 |\n",
      "|    clip_fraction        | 0.395      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.501      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.22       |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0626    |\n",
      "|    value_loss           | 4.34       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.074358974358976\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016812038 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.06       |\n",
      "|    value_loss           | 4.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.501234567901236\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017642356 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.8         |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 4.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.154761904761905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 201         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018182904 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.555       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.908       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 4.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.301149425287356\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 209         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016550314 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0593     |\n",
      "|    value_loss           | 4.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.15555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 216         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017406847 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.615       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.772       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0636     |\n",
      "|    value_loss           | 3.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.935483870967744\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018009562 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.58        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0624     |\n",
      "|    value_loss           | 3.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.7\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 231         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016054958 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.955       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 3.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.38181818181818\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017532708 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.647       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 3.71        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.009803921568626\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 245         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017185014 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.49        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 3.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.516190476190474\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018038286 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 3.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.95925925925926\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 141        |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 259        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01808907 |\n",
      "|    clip_fraction        | 0.339      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.69       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.411      |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.0626    |\n",
      "|    value_loss           | 3.5        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.344144144144146\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 266         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016426662 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.223       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0631     |\n",
      "|    value_loss           | 3.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.717543859649126\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 274         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017390288 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.625       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.05128205128205\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 282         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018052438 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.76        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0677     |\n",
      "|    value_loss           | 3.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.45166666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 141        |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 290        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01884456 |\n",
      "|    clip_fraction        | 0.39       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.707      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.364      |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0692    |\n",
      "|    value_loss           | 3.35       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.926829268292686\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 298         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018244112 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.97        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 3.25        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.35079365079365\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019159442 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.81        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 3.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.69612403100775\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 313         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018110937 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.371       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 3.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.02878787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017038679 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 3.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.45185185185185\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 329         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018191613 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0672     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.856521739130436\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 336         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018359689 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.719       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.252482269503545\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 344         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017776385 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.232       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 2.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.72638888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 352         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018678375 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.139       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.185034013605446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 360         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017258089 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0667     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.64666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 368         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017672587 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.37        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0689     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.005228758169935\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 376         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018039232 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.333       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.30897435897436\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017705716 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.563522012578616\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 391         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017078765 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1           |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.785185185185185\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017021708 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.78787878787879\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 406         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018974384 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.226       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.852380952380955\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 414         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017575573 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 2.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.03391812865497\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 422         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016702112 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.911       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.237931034482756\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 429         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017137963 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.567       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.49152542372882\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 437         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016516095 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.826       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0724     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.75\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 445         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016833056 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.43        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.021857923497265\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 453         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017230568 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.297       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.073      |\n",
      "|    value_loss           | 2.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.28709677419355\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 461         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017349081 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.486       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.56931216931217\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 469         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017944694 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0744     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.90833333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 477         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018108048 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.000816    |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.292307692307695\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 137        |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 485        |\n",
      "|    total_timesteps      | 66560      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01794732 |\n",
      "|    clip_fraction        | 0.359      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.86       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.154      |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0745    |\n",
      "|    value_loss           | 1.98       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.62323232323232\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 137        |\n",
      "|    iterations           | 66         |\n",
      "|    time_elapsed         | 492        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01496763 |\n",
      "|    clip_fraction        | 0.29       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.853      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.269      |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | -0.0715    |\n",
      "|    value_loss           | 2.1        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.94726368159204\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 500         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016192554 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.538       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.17843137254902\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 508         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016433563 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.266       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 1.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.390338164251204\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 516         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016954463 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.242       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0743     |\n",
      "|    value_loss           | 1.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.59809523809524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 524         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018926006 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0968      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0784     |\n",
      "|    value_loss           | 1.76        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.77089201877934\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -178      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 136       |\n",
      "|    iterations           | 71        |\n",
      "|    time_elapsed         | 532       |\n",
      "|    total_timesteps      | 72704     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0181315 |\n",
      "|    clip_fraction        | 0.357     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.21     |\n",
      "|    explained_variance   | 0.88      |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.0289    |\n",
      "|    n_updates            | 700       |\n",
      "|    policy_gradient_loss | -0.0765   |\n",
      "|    value_loss           | 1.76      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.95648148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 540         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017875195 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.218       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.077      |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.141552511415526\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 547         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018174876 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0808      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0778     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.35405405405405\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 555         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018009314 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0606      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0775     |\n",
      "|    value_loss           | 1.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.568888888888885\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017322514 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.14        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0765     |\n",
      "|    value_loss           | 1.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.757894736842104\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 76         |\n",
      "|    time_elapsed         | 570        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01709269 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.907      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.102      |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | -0.076     |\n",
      "|    value_loss           | 1.55       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.03030303030303\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 77         |\n",
      "|    time_elapsed         | 578        |\n",
      "|    total_timesteps      | 78848      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01949273 |\n",
      "|    clip_fraction        | 0.396      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0559     |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0797    |\n",
      "|    value_loss           | 1.57       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.38205128205128\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 585         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018446121 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0238      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.078      |\n",
      "|    value_loss           | 1.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.743459915611815\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 79         |\n",
      "|    time_elapsed         | 593        |\n",
      "|    total_timesteps      | 80896      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01984407 |\n",
      "|    clip_fraction        | 0.42       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.911      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0576     |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0804    |\n",
      "|    value_loss           | 1.34       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.1125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 601         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018198265 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.155      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0791     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.51934156378601\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 608         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019210378 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0878     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0804     |\n",
      "|    value_loss           | 1.39        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.917886178861785\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 616         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019636707 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0395      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0829     |\n",
      "|    value_loss           | 1.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.276305220883536\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 623         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018191855 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0619     |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0791     |\n",
      "|    value_loss           | 1.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.596825396825395\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 631         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019768042 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.123      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0821     |\n",
      "|    value_loss           | 1.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.91686274509804\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 638         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021093328 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.145      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0847     |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.25426356589147\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 646         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019219223 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.13       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0815     |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.559386973180075\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 654        |\n",
      "|    total_timesteps      | 89088      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02135402 |\n",
      "|    clip_fraction        | 0.45       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.16      |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0394    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0872    |\n",
      "|    value_loss           | 1.47       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.861363636363635\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 661         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019366613 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.182      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0803     |\n",
      "|    value_loss           | 1.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.200749063670415\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 669         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017788844 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00934     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0811     |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.55407407407407\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 676         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018767398 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.243      |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0788     |\n",
      "|    value_loss           | 0.981       |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.88278388278388\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 684         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017440442 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.22       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0806     |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.20652173913044\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 692         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019220151 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.097      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0826     |\n",
      "|    value_loss           | 1.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.51254480286738\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 700         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018407386 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.131      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0806     |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.824113475177306\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 708         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017871507 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0192      |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0809     |\n",
      "|    value_loss           | 1.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.127719298245616\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018094748 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.21       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0806     |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.42986111111111\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 723         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018191021 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00677    |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0826     |\n",
      "|    value_loss           | 1.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.738144329896905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 730         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019023495 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0826     |\n",
      "|    value_loss           | 1.01        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.03265306122449\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -173       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 135        |\n",
      "|    iterations           | 98         |\n",
      "|    time_elapsed         | 738        |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01750645 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.09      |\n",
      "|    explained_variance   | 0.92       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0704     |\n",
      "|    n_updates            | 970        |\n",
      "|    policy_gradient_loss | -0.0826    |\n",
      "|    value_loss           | 1.32       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.329292929292926\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 746         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015691321 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.104      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0768     |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.592666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 753         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016828038 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.06       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.156       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0797     |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -43.96\n",
      "Overall Average Successful Assignments: 38.55969533606771\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a18102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -196.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 4.066666666666666\n",
      "All assignments history: [5, 4, 8, 13, 11, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -184     |\n",
      "| time/              |          |\n",
      "|    fps             | 173      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 8.333333333333334\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -182         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101311095 |\n",
      "|    clip_fraction        | 0.0929       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | -0.418       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.55         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0567      |\n",
      "|    value_loss           | 16.2         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.8\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013314503 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.211      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0653     |\n",
      "|    value_loss           | 11.5        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.833333333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 144        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 28         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01359898 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | 0.0235     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.09       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0614    |\n",
      "|    value_loss           | 10.4       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.733333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014445737 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.071       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.1         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 9.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -190.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.333333333333334\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 142        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 43         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01663853 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0684     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.69       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0651    |\n",
      "|    value_loss           | 7.65       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.58095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016372733 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 7.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.35\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017163774 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 6.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -194.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.422222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018208642 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0606     |\n",
      "|    value_loss           | 5.81        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.826666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020809796 |\n",
      "|    clip_fraction        | 0.46        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.85        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 5.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.363636363636363\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020771528 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.03        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0597     |\n",
      "|    value_loss           | 5.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.033333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023439145 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.608       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0632     |\n",
      "|    value_loss           | 4.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.112820512820512\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 138        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 96         |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02398899 |\n",
      "|    clip_fraction        | 0.537      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.213      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.666      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0642    |\n",
      "|    value_loss           | 4.62       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.10952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024781635 |\n",
      "|    clip_fraction        | 0.542       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.745       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0642     |\n",
      "|    value_loss           | 4.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.8\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026043389 |\n",
      "|    clip_fraction        | 0.558       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.363       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 4.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.566666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025738696 |\n",
      "|    clip_fraction        | 0.556       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.616       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 4.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.6\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022409916 |\n",
      "|    clip_fraction        | 0.483       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.436       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.114814814814814\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021822298 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.52        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 3.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.378947368421052\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 137        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 141        |\n",
      "|    total_timesteps      | 19456      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02130133 |\n",
      "|    clip_fraction        | 0.44       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.546      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.22       |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0653    |\n",
      "|    value_loss           | 3.85       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.57666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021572419 |\n",
      "|    clip_fraction        | 0.476       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0665     |\n",
      "|    value_loss           | 3.8         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.12063492063492\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 157         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021309558 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.296       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 3.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.442424242424245\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021080669 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.629       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.966       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 3.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.30724637681159\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019351546 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.674       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 3.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.519444444444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018122327 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.685       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.663       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 3.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.488\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017532332 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.744       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 3.13        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.56410256410256\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 195        |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01568822 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.726      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.846      |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0676    |\n",
      "|    value_loss           | 3.15       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.760493827160495\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015706576 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.904       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0655     |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.00714285714286\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -182         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 135          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 211          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0154372025 |\n",
      "|    clip_fraction        | 0.298        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.26        |\n",
      "|    explained_variance   | 0.757        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.816        |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.0651      |\n",
      "|    value_loss           | 2.84         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.255172413793105\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016264271 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.206       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.86222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 226         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018024892 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.761       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.670967741935485\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 135        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 234        |\n",
      "|    total_timesteps      | 31744      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01736766 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.791      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.295      |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0697    |\n",
      "|    value_loss           | 2.65       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016445305 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.903       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0677     |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.07070707070707\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 249         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018628957 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.709       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0731     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.76470588235294\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 257         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017289814 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.624       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.44761904761905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 264         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016492408 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.964       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.96296296296296\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017759638 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.181       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.43963963963964\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 280         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016790703 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.598       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.10877192982456\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 287         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018342199 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.368       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0721     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.66153846153846\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018339105 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.56        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0714     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.16166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 303         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017043877 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.164       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.68292682926829\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 135        |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 310        |\n",
      "|    total_timesteps      | 41984      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01786817 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.861      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.348      |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0736    |\n",
      "|    value_loss           | 2.1        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.05873015873016\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 318         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018672038 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.207       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.44651162790698\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 326         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018865649 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.263       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0754     |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.942424242424245\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 333         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018915158 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0836      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0765     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.35407407407408\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 135        |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 341        |\n",
      "|    total_timesteps      | 46080      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01863354 |\n",
      "|    clip_fraction        | 0.381      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.338      |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0763    |\n",
      "|    value_loss           | 1.8        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.68985507246377\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 348         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016927537 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.182       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.95744680851064\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 135        |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 356        |\n",
      "|    total_timesteps      | 48128      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01790642 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.888      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.347      |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0766    |\n",
      "|    value_loss           | 1.84       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.24027777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018756669 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00779     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0784     |\n",
      "|    value_loss           | 1.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.394557823129254\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 371         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018665496 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0886      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.079      |\n",
      "|    value_loss           | 1.56        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.59466666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 379         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019346472 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0507      |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0779     |\n",
      "|    value_loss           | 1.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.95032679738562\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 386         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018362198 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0381      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0783     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.39358974358974\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 394         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019765085 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0389     |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0819     |\n",
      "|    value_loss           | 1.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.74339622641509\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 53         |\n",
      "|    time_elapsed         | 402        |\n",
      "|    total_timesteps      | 54272      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02105546 |\n",
      "|    clip_fraction        | 0.439      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0963     |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.082     |\n",
      "|    value_loss           | 1.52       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.129629629629626\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 409        |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01883362 |\n",
      "|    clip_fraction        | 0.404      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.902      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0757    |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.0809    |\n",
      "|    value_loss           | 1.6        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.52969696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 417         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019422721 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0819     |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.90952380952381\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 425        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01830222 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.92       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0178     |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0775    |\n",
      "|    value_loss           | 1.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.30409356725146\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 57         |\n",
      "|    time_elapsed         | 432        |\n",
      "|    total_timesteps      | 58368      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01713835 |\n",
      "|    clip_fraction        | 0.364      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.916      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0418    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0776    |\n",
      "|    value_loss           | 1.46       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.6551724137931\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 440         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020232543 |\n",
      "|    clip_fraction        | 0.43        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0841     |\n",
      "|    value_loss           | 1.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.9774011299435\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 448         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019076712 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.124      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0822     |\n",
      "|    value_loss           | 1.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.29222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020133061 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.107      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0823     |\n",
      "|    value_loss           | 1.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.638251366120215\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 463        |\n",
      "|    total_timesteps      | 62464      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01956448 |\n",
      "|    clip_fraction        | 0.398      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.93       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.127     |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0794    |\n",
      "|    value_loss           | 1.16       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.95161290322581\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018561114 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0426     |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0801     |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.265608465608466\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 63         |\n",
      "|    time_elapsed         | 478        |\n",
      "|    total_timesteps      | 64512      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02037297 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.931      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.137     |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0812    |\n",
      "|    value_loss           | 1.2        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.584375\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 486         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021144569 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0828     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0844     |\n",
      "|    value_loss           | 1.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.91076923076923\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 494        |\n",
      "|    total_timesteps      | 66560      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02089912 |\n",
      "|    clip_fraction        | 0.43       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.931      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.103     |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0832    |\n",
      "|    value_loss           | 1.22       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.23434343434344\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 501         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020345218 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0536     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0827     |\n",
      "|    value_loss           | 1.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.56517412935323\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 509         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020677656 |\n",
      "|    clip_fraction        | 0.437       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.208      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.084      |\n",
      "|    value_loss           | 1.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.86764705882353\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 517         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020502934 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.189      |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0824     |\n",
      "|    value_loss           | 1.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.143961352657\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 525         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020350909 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0967     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0835     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.40952380952381\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 532         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021132551 |\n",
      "|    clip_fraction        | 0.452       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0364     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.085      |\n",
      "|    value_loss           | 1.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.668544600938965\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 540         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019304024 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0863     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0819     |\n",
      "|    value_loss           | 1.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.92962962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 548         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022956546 |\n",
      "|    clip_fraction        | 0.455       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0864     |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0847     |\n",
      "|    value_loss           | 1.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.19178082191781\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 555         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020785227 |\n",
      "|    clip_fraction        | 0.444       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.24        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0841     |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.436036036036036\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 563        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02104899 |\n",
      "|    clip_fraction        | 0.429      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.932      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0239     |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0824    |\n",
      "|    value_loss           | 1.12       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.693333333333335\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 571        |\n",
      "|    total_timesteps      | 76800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02050746 |\n",
      "|    clip_fraction        | 0.417      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.2       |\n",
      "|    explained_variance   | 0.937      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.247     |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0835    |\n",
      "|    value_loss           | 1.04       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.91140350877193\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 76         |\n",
      "|    time_elapsed         | 578        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02082856 |\n",
      "|    clip_fraction        | 0.436      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.933      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.143     |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | -0.0846    |\n",
      "|    value_loss           | 1.11       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.0978354978355\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 77         |\n",
      "|    time_elapsed         | 586        |\n",
      "|    total_timesteps      | 78848      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02197671 |\n",
      "|    clip_fraction        | 0.451      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.934      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0247    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0847    |\n",
      "|    value_loss           | 1.1        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.282051282051285\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 594         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018571984 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.128      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0799     |\n",
      "|    value_loss           | 1.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.45569620253165\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 601         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021164339 |\n",
      "|    clip_fraction        | 0.451       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0784     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0854     |\n",
      "|    value_loss           | 1.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.63166666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 609         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020269535 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.276      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0834     |\n",
      "|    value_loss           | 1.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.8\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 616         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020112278 |\n",
      "|    clip_fraction        | 0.427       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0696     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0832     |\n",
      "|    value_loss           | 1.02        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.954471544715446\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 624        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01998797 |\n",
      "|    clip_fraction        | 0.427      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.16      |\n",
      "|    explained_variance   | 0.94       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.204     |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.0845    |\n",
      "|    value_loss           | 0.99       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.09397590361446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 632         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017591044 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.127      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0799     |\n",
      "|    value_loss           | 1.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.24126984126984\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 639         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020083193 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.203      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0838     |\n",
      "|    value_loss           | 1.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.38980392156863\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 647         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019689668 |\n",
      "|    clip_fraction        | 0.429       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0856     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0852     |\n",
      "|    value_loss           | 1.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.52170542635659\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -173       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 655        |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01990778 |\n",
      "|    clip_fraction        | 0.438      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.13      |\n",
      "|    explained_variance   | 0.928      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.23      |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.0867    |\n",
      "|    value_loss           | 1.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.66130268199234\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 662         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021022338 |\n",
      "|    clip_fraction        | 0.445       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.000801   |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0851     |\n",
      "|    value_loss           | 1.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.79848484848485\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 670         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018998083 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.197      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0829     |\n",
      "|    value_loss           | 1.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 62.94681647940075\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 678         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017273627 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0894     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0809     |\n",
      "|    value_loss           | 1.15        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.11185185185185\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 687         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019513289 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.221      |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0834     |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.282051282051285\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 694         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017490126 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.218       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0807     |\n",
      "|    value_loss           | 1.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.45289855072464\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 702         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017105442 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.067      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.08       |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.61577060931899\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 710         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020184565 |\n",
      "|    clip_fraction        | 0.433       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0405     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0857     |\n",
      "|    value_loss           | 1.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.797872340425535\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 717         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019229513 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0982     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0838     |\n",
      "|    value_loss           | 1.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 63.95228070175438\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 725         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017765835 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.03       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0189     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0837     |\n",
      "|    value_loss           | 1.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.125\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -168       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 96         |\n",
      "|    time_elapsed         | 733        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01999759 |\n",
      "|    clip_fraction        | 0.443      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8         |\n",
      "|    explained_variance   | 0.922      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.257     |\n",
      "|    n_updates            | 950        |\n",
      "|    policy_gradient_loss | -0.0871    |\n",
      "|    value_loss           | 1.2        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.28797250859107\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 740         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019706717 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.97       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00993    |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0844     |\n",
      "|    value_loss           | 1.14        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.46190476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 748         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016803917 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.95       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0308     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0817     |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.63367003367003\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 756         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019403031 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.93       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0825     |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0837     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 64.81066666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 763         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018493103 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.91       |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.114      |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0859     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -17.06\n",
      "Overall Average Successful Assignments: 47.1898225195261\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5de82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -196.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 4.333333333333333\n",
      "All assignments history: [8, 7, 7, 8, 15, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    fps             | 204      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009389493 |\n",
      "|    clip_fraction        | 0.0727      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.112      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.1         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 15.9        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012721247 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0232      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -182.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 10.616666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 150        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 27         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01328467 |\n",
      "|    clip_fraction        | 0.233      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | 0.0573     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.18       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.06      |\n",
      "|    value_loss           | 10.2       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.44\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015631206 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0623      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.88        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 8.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.933333333333334\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -182         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0154308705 |\n",
      "|    clip_fraction        | 0.305        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | 0.103        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 1.99         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.061       |\n",
      "|    value_loss           | 7.68         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -136.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.666666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016403034 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 6.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.8\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 144         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017210154 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0619     |\n",
      "|    value_loss           | 6.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.525925925925925\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019147962 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0641     |\n",
      "|    value_loss           | 5.35        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.153333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019642206 |\n",
      "|    clip_fraction        | 0.432       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.064      |\n",
      "|    value_loss           | 4.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.67878787878788\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021325093 |\n",
      "|    clip_fraction        | 0.489       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0659     |\n",
      "|    value_loss           | 4.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.93888888888889\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 141        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 86         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02155388 |\n",
      "|    clip_fraction        | 0.471      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.194      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.402      |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0619    |\n",
      "|    value_loss           | 4.49       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.764102564102565\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023618085 |\n",
      "|    clip_fraction        | 0.527       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 4.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.466666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024349067 |\n",
      "|    clip_fraction        | 0.539       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.05        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.137777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024767667 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.92        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.068      |\n",
      "|    value_loss           | 4.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -136.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.979166666666668\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 141        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 115        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02536121 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.44       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.989      |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0676    |\n",
      "|    value_loss           | 3.96       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.254901960784313\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 124         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023030575 |\n",
      "|    clip_fraction        | 0.507       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.957       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0667     |\n",
      "|    value_loss           | 4.05        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -134.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.062962962962963\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024680093 |\n",
      "|    clip_fraction        | 0.535       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.51        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.01        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0706     |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.775438596491227\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022376038 |\n",
      "|    clip_fraction        | 0.478       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.648       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 3.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -152.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.35333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021922857 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.428       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0679     |\n",
      "|    value_loss           | 4.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.419047619047618\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 155         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022264417 |\n",
      "|    clip_fraction        | 0.474       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.82        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0682     |\n",
      "|    value_loss           | 3.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.403030303030302\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 138        |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 162        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02084152 |\n",
      "|    clip_fraction        | 0.445      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.6        |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.25       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0701    |\n",
      "|    value_loss           | 4.02       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.208695652173912\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020634066 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.623       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.578       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 3.88        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.180555555555557\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 179         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019038053 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.635       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0674     |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.288\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019893544 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 3.56        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.153846153846153\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 195         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019157592 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.421       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0732     |\n",
      "|    value_loss           | 3.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.002469135802468\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 204         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018327406 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 3.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.81904761904762\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 212         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019234624 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.376       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0735     |\n",
      "|    value_loss           | 3.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -156.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.544827586206896\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019407012 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.953       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.308888888888887\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 228        |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01918343 |\n",
      "|    clip_fraction        | 0.394      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.715      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.734      |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.0722    |\n",
      "|    value_loss           | 3.25       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.35483870967742\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 236         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017300148 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.279166666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018252386 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.619       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 3.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.22020202020202\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017634848 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.487       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 3.1         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.268627450980393\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 260         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017378412 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.613       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0713     |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.333333333333332\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017769866 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.586       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 2.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.514814814814816\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017408136 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.963       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.861261261261262\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 285         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016710553 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.694       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 2.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.301754385964912\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 293        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01603588 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.77       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.903      |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0691    |\n",
      "|    value_loss           | 2.95       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.71965811965812\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016473643 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.655       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.088333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015808757 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.21        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.491056910569107\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 317         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018055674 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.188       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0744     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.08888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 325         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016391646 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.447       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.652713178294572\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 334         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018079773 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0729     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.169696969696968\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -181      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 132       |\n",
      "|    iterations           | 44        |\n",
      "|    time_elapsed         | 341       |\n",
      "|    total_timesteps      | 45056     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0159107 |\n",
      "|    clip_fraction        | 0.298     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.25     |\n",
      "|    explained_variance   | 0.807     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.223     |\n",
      "|    n_updates            | 430       |\n",
      "|    policy_gradient_loss | -0.0695   |\n",
      "|    value_loss           | 2.41      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.774814814814814\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 348        |\n",
      "|    total_timesteps      | 46080      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01541128 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.799      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.824      |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0675    |\n",
      "|    value_loss           | 2.72       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.372463768115942\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -181         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 356          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0155902775 |\n",
      "|    clip_fraction        | 0.302        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.24        |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.458        |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.0691      |\n",
      "|    value_loss           | 2.52         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.968794326241134\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -181         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 363          |\n",
      "|    total_timesteps      | 48128        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0150551265 |\n",
      "|    clip_fraction        | 0.269        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.24        |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.2          |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0683      |\n",
      "|    value_loss           | 2.44         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.57638888888889\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -181         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 370          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0149352085 |\n",
      "|    clip_fraction        | 0.269        |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.24        |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 0.167        |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.0668      |\n",
      "|    value_loss           | 2.36         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.15374149659864\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 378         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014448589 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.29        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.714666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 386         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014462565 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.176       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.227450980392156\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 394         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014405347 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.301       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 2.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.684615384615384\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 402         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017147891 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.325       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0732     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.07044025157233\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 410         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015321452 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.207       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.474074074074075\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 418         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015485373 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.622       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 2.16        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.78909090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 426         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015735734 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.476       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.2\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 434         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016231693 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0584     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.63625730994152\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 442         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016494982 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.462       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.075      |\n",
      "|    value_loss           | 2.19        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.05287356321839\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 450         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015111204 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.177       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.49378531073447\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 458         |\n",
      "|    total_timesteps      | 60416       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015292114 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.445       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.861111111111114\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 466         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016452212 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.044       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.25901639344262\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 475         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015271916 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.348       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0698     |\n",
      "|    value_loss           | 1.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.597849462365595\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 483         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019037079 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.348       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0769     |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.89523809523809\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 492         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013803674 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.326       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.16770833333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 500         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016494554 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0969      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 1.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.47179487179487\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 508         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014735982 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.203       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 1.64        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.845454545454544\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 516         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014944779 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.605       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.180099502487565\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 524         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015885409 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0175      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.51470588235294\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 532         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015155546 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.011       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0693     |\n",
      "|    value_loss           | 1.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.830917874396135\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 540         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014860867 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0209      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 1.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.14666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 549         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013454715 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.286      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0682     |\n",
      "|    value_loss           | 1.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.50892018779343\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 557         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014351007 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.174       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0679     |\n",
      "|    value_loss           | 1.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.84537037037037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 565         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015421543 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 1.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.210045662100455\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 573         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015880387 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.079       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0743     |\n",
      "|    value_loss           | 1.62        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.55225225225225\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 130        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 582        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01605677 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.18      |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0157     |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0749    |\n",
      "|    value_loss           | 1.82       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.80177777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 589         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016124716 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0481      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0733     |\n",
      "|    value_loss           | 1.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.036842105263155\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 597         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015974313 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.301       |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0742     |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.245021645021644\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 606         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016291803 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.178      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0743     |\n",
      "|    value_loss           | 1.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.49316239316239\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 614         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014046231 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.425       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 1.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.881012658227846\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 622         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017952098 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0129      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0781     |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.251666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 630         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016427545 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0774      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0718     |\n",
      "|    value_loss           | 1.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.61810699588477\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 638         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015080231 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0351     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0725     |\n",
      "|    value_loss           | 1.67        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.9869918699187\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 647         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015569462 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0576     |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 1.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.33253012048193\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 655         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016158372 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.441       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0744     |\n",
      "|    value_loss           | 1.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.67301587301587\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 663         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016070796 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0133      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.075      |\n",
      "|    value_loss           | 1.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.04156862745098\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 671         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015450759 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0564     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 1.57        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.42945736434108\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 679         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016509287 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.12       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.35        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.073      |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.73946360153257\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 687         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016892336 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0574     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0756     |\n",
      "|    value_loss           | 1.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.095454545454544\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -172       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 129        |\n",
      "|    iterations           | 88         |\n",
      "|    time_elapsed         | 696        |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01756761 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.1       |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.165      |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | -0.076     |\n",
      "|    value_loss           | 1.33       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.410486891385766\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 705         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018015265 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.1        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.289       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0792     |\n",
      "|    value_loss           | 1.27        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.73037037037037\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 713         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018787315 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.229       |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0812     |\n",
      "|    value_loss           | 1.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.064468864468864\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 721         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018161155 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.131      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.082      |\n",
      "|    value_loss           | 1.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.40942028985507\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 729         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017975368 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.21        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0793     |\n",
      "|    value_loss           | 1.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.729032258064514\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 738         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018642819 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.06       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0953      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.082      |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.05957446808511\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 746         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017917417 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.04       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0811     |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.41894736842105\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 754         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017724805 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.03       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0792     |\n",
      "|    value_loss           | 1.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.738194444444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -167        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 762         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018578658 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8          |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0305     |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.083      |\n",
      "|    value_loss           | 1.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.07422680412371\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 770         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018922975 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.98       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.13       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0789     |\n",
      "|    value_loss           | 1.24        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.39523809523809\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 778         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016564235 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.96       |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0282      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0775     |\n",
      "|    value_loss           | 1.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.656565656565654\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 787         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017796813 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.93       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.344       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0798     |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.94266666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 795         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016343912 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -7.89       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.163       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0789     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -61.98\n",
      "Overall Average Successful Assignments: 32.96752581157344\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0688a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -170.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.133333333333333\n",
      "All assignments history: [9, 5, 6, 6, 6, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -187     |\n",
      "| time/              |          |\n",
      "|    fps             | 162      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.266666666666666\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 136          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090663005 |\n",
      "|    clip_fraction        | 0.0677       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | 0.0309       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.1          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0513      |\n",
      "|    value_loss           | 15.6         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.533333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -187        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011990117 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0355     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.89        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0594     |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -164.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.35\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012473352 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.00869     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.88        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 10.1        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.52\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -186       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 130        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 39         |\n",
      "|    total_timesteps      | 5120       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01465846 |\n",
      "|    clip_fraction        | 0.272      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | 0.0305     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.18       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0621    |\n",
      "|    value_loss           | 8.54       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.155555555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015769873 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0648      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.15        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0608     |\n",
      "|    value_loss           | 7.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.685714285714285\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 55         |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01688042 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0556     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.62       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0618    |\n",
      "|    value_loss           | 6.58       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.716666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 64         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01767724 |\n",
      "|    clip_fraction        | 0.374      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0531     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.34       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0613    |\n",
      "|    value_loss           | 6.12       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -102.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.94074074074074\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 72         |\n",
      "|    total_timesteps      | 9216       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01824728 |\n",
      "|    clip_fraction        | 0.394      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0641     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.18       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0602    |\n",
      "|    value_loss           | 5.69       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.033333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020919636 |\n",
      "|    clip_fraction        | 0.461       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0725      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0646     |\n",
      "|    value_loss           | 5.25        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -136.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.363636363636363\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021303304 |\n",
      "|    clip_fraction        | 0.475       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0996      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.244       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0631     |\n",
      "|    value_loss           | 4.77        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.95\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024497222 |\n",
      "|    clip_fraction        | 0.537       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.42        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.068      |\n",
      "|    value_loss           | 4.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.25128205128205\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024179662 |\n",
      "|    clip_fraction        | 0.524       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.065      |\n",
      "|    value_loss           | 4.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.733333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025825128 |\n",
      "|    clip_fraction        | 0.565       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 4.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.90222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027318876 |\n",
      "|    clip_fraction        | 0.594       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.16        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 4.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.654166666666665\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 129        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02573724 |\n",
      "|    clip_fraction        | 0.555      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.373      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 3.16       |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0672    |\n",
      "|    value_loss           | 4.3        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.149019607843137\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022994887 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 3.62        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0643     |\n",
      "|    value_loss           | 4.37        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.874074074074073\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 145         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026191123 |\n",
      "|    clip_fraction        | 0.535       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.16        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 4.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.62456140350877\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026039135 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.693       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 4.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.22\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022590242 |\n",
      "|    clip_fraction        | 0.49        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.519       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 4.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.857142857142854\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025094707 |\n",
      "|    clip_fraction        | 0.531       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.85        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 4.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -92.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.15151515151515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023100562 |\n",
      "|    clip_fraction        | 0.466       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.581       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.626       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 3.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.45217391304348\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021408767 |\n",
      "|    clip_fraction        | 0.453       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 3.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.7\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020712517 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.597       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 4.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.165333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020376138 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.617       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.511       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 3.77        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.33076923076923\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018982524 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.3         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 3.81        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -96.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.45679012345679\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019243596 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.629       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.681       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 3.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.75476190476191\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019489074 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 3.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.12873563218391\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019476548 |\n",
      "|    clip_fraction        | 0.398       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.17        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 3.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.88\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 244        |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02001497 |\n",
      "|    clip_fraction        | 0.406      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.676      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.24       |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.0703    |\n",
      "|    value_loss           | 3.55       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.520430107526884\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019641476 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 3.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.93125\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 260         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017903745 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.685       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.405       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 3.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.71313131313131\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018025383 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.376       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 3.16        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.66470588235294\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 277         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018285021 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.717       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.42        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 3.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.394285714285715\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 285        |\n",
      "|    total_timesteps      | 35840      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01796766 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.724      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.532      |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.067     |\n",
      "|    value_loss           | 3.27       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.13703703703704\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 293         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017394776 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 2.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.913513513513514\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017010298 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.683       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 3.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.42982456140351\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 308         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017723264 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.769       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.614       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.0034188034188\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 317        |\n",
      "|    total_timesteps      | 39936      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01734376 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.777      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.458      |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0679    |\n",
      "|    value_loss           | 2.76       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.49666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 325         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018256236 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.157       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.86666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 333         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018787479 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.859       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0724     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.03968253968254\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 341         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017111156 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.446       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.071      |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.48217054263566\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 349         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016929185 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.867       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.86818181818182\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 358         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017665932 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.203       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0718     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.108148148148146\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 366         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019304931 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.488       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0759     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.176811594202896\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 375         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017288413 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.923       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0682     |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.248226950354606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017520417 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.601       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0729     |\n",
      "|    value_loss           | 2.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.32222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 391         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016843319 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.42        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.183673469387756\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 400         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018448615 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.345       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 2.2         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.49333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 408         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018296938 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.438       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0736     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.93333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 415        |\n",
      "|    total_timesteps      | 52224      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01817432 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.39       |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0755    |\n",
      "|    value_loss           | 2.26       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.37948717948718\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 423         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018575193 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.7937106918239\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 430         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017531654 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.928       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.05679012345679\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 438         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018398866 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.385       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0759     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.39151515151515\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 447         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017873349 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0735     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.87261904761905\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 456         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017596183 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.366       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0766     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.37894736842105\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 464         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019841129 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.319       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0788     |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.9\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017493518 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0153      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0748     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.44632768361582\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 479        |\n",
      "|    total_timesteps      | 60416      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01757396 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.873      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0379    |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0732    |\n",
      "|    value_loss           | 1.83       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.97666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 486        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01847297 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.871      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0758    |\n",
      "|    value_loss           | 1.91       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.4775956284153\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 494         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018419297 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.422       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0774     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.97741935483871\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 501        |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01897557 |\n",
      "|    clip_fraction        | 0.382      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.877      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.14       |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.0759    |\n",
      "|    value_loss           | 1.78       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.48148148148148\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 509         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017396074 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.15        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0746     |\n",
      "|    value_loss           | 1.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.94479166666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 516         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018512158 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0746     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0792     |\n",
      "|    value_loss           | 1.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.41538461538462\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 524         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016703136 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.534       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0741     |\n",
      "|    value_loss           | 1.86        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.87676767676768\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 532         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017020352 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.249       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0761     |\n",
      "|    value_loss           | 2.01        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.31542288557214\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 541         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017612664 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.482       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0776     |\n",
      "|    value_loss           | 1.86        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.7578431372549\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 549         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019303173 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.152       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0813     |\n",
      "|    value_loss           | 1.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.193236714975846\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 557         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017933574 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0775     |\n",
      "|    value_loss           | 1.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.57904761904762\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 566        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01799154 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.895      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.133      |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.0775    |\n",
      "|    value_loss           | 1.57       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.93427230046948\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 575         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018129434 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.437       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0774     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.29814814814815\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 583         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018248163 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.428       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0779     |\n",
      "|    value_loss           | 1.69        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.66392694063927\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 591         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019187037 |\n",
      "|    clip_fraction        | 0.405       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.238       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0812     |\n",
      "|    value_loss           | 1.64        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.01441441441442\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 599         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018854966 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.165      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0788     |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.33155555555555\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 607         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018320322 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.297      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0772     |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.685087719298245\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 616         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020250726 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0317     |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0791     |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.02597402597402\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 624         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018154461 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.15        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0796     |\n",
      "|    value_loss           | 1.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.34444444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 632         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017874513 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.106      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0769     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.64810126582279\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 640         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015917376 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.155       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0741     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 56.95333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 649         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018898647 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.336       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0789     |\n",
      "|    value_loss           | 1.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.24609053497942\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 657         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016596332 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.187       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0758     |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.51951219512195\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 665        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01772343 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.91       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0366     |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.0783    |\n",
      "|    value_loss           | 1.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 57.80401606425703\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 674         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018996399 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.431       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0805     |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.051587301587304\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 84         |\n",
      "|    time_elapsed         | 682        |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01895627 |\n",
      "|    clip_fraction        | 0.391      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.18      |\n",
      "|    explained_variance   | 0.916      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.305     |\n",
      "|    n_updates            | 830        |\n",
      "|    policy_gradient_loss | -0.0787    |\n",
      "|    value_loss           | 1.31       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.28156862745098\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 690         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018506456 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.134       |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0796     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.52170542635659\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 698        |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01784098 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.922      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0548    |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.0791    |\n",
      "|    value_loss           | 1.26       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.75019157088123\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 707        |\n",
      "|    total_timesteps      | 89088      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01782696 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.16      |\n",
      "|    explained_variance   | 0.907      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0555    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0778    |\n",
      "|    value_loss           | 1.52       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 58.93863636363636\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017266318 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0466     |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0799     |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.13333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 723         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019029312 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0303      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0806     |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.337037037037035\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 732         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018318376 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.14       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0979     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0813     |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.57142857142857\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 740         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017858941 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.133      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0812     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.77028985507246\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 748         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018582355 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.053       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0809     |\n",
      "|    value_loss           | 1.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 59.987813620071684\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 757         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020352095 |\n",
      "|    clip_fraction        | 0.437       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0851     |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.186524822695034\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -172        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 765         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018886238 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.141      |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0808     |\n",
      "|    value_loss           | 1.5         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.40701754385965\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -172       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 773        |\n",
      "|    total_timesteps      | 97280      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01975538 |\n",
      "|    clip_fraction        | 0.414      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.1       |\n",
      "|    explained_variance   | 0.92       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0742     |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.08      |\n",
      "|    value_loss           | 1.19       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.62638888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 782         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019232385 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0353      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0785     |\n",
      "|    value_loss           | 1.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 60.81718213058419\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -171       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 790        |\n",
      "|    total_timesteps      | 99328      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01686847 |\n",
      "|    clip_fraction        | 0.346      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.09      |\n",
      "|    explained_variance   | 0.907      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0596     |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.078     |\n",
      "|    value_loss           | 1.42       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.010884353741496\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 798         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020218978 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0811     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.1993265993266\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -170        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 806         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018245686 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.08       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0871     |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0788     |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 61.39333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -169        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 815         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017790738 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.07       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.601       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0796     |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -26.82\n",
      "Overall Average Successful Assignments: 43.42529757016356\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02a9d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -186.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 7.2\n",
      "All assignments history: [7, 13, 4, 5, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -185     |\n",
      "| time/              |          |\n",
      "|    fps             | 153      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -144.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.033333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009492032 |\n",
      "|    clip_fraction        | 0.0843      |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.317      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.71        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0533     |\n",
      "|    value_loss           | 17.3        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.4\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012562394 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.154      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.83        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0596     |\n",
      "|    value_loss           | 13          |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.75\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013660105 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0383      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0595     |\n",
      "|    value_loss           | 10.4        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.08\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014985306 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.000721    |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.25        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    value_loss           | 9.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.55555555555556\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015604509 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0247      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.76        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.06       |\n",
      "|    value_loss           | 8.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.05714285714286\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 56         |\n",
      "|    total_timesteps      | 7168       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01629715 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.28      |\n",
      "|    explained_variance   | 0.0379     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.73       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0593    |\n",
      "|    value_loss           | 7.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.983333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018128784 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0432      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 6.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -160.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.49629629629629\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018624036 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0465      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0604     |\n",
      "|    value_loss           | 6.12        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.22666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019528527 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0522      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.87        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 5.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.915151515151514\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021708306 |\n",
      "|    clip_fraction        | 0.472       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.065       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.19        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0615     |\n",
      "|    value_loss           | 5.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -70.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.416666666666664\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021946013 |\n",
      "|    clip_fraction        | 0.497       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.879       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 4.87        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.5025641025641\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 106        |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02266983 |\n",
      "|    clip_fraction        | 0.511      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.164      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.26       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0632    |\n",
      "|    value_loss           | 4.64       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.01428571428571\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026733078 |\n",
      "|    clip_fraction        | 0.574       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.53        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 4.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -130.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.00888888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025383247 |\n",
      "|    clip_fraction        | 0.553       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 4.4         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.425\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 124        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 131        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02800896 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.36       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.18       |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0696    |\n",
      "|    value_loss           | 4.28       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.74901960784314\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024248935 |\n",
      "|    clip_fraction        | 0.502       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.628       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 4.23        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.34814814814815\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 124        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 147        |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02215746 |\n",
      "|    clip_fraction        | 0.467      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.456      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.507      |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.066     |\n",
      "|    value_loss           | 4.43       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.76842105263158\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022262255 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.469       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 4.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.96\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023591034 |\n",
      "|    clip_fraction        | 0.493       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 4.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.22539682539683\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 173         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021979319 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.36        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0653     |\n",
      "|    value_loss           | 4.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.64848484848485\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022447113 |\n",
      "|    clip_fraction        | 0.488       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0659     |\n",
      "|    value_loss           | 4.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.107246376811595\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021992551 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.556       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 4.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.483333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021851394 |\n",
      "|    clip_fraction        | 0.449       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.77        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 4.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.78933333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020737968 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.609       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0216     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0675     |\n",
      "|    value_loss           | 3.99        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.815384615384616\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021502724 |\n",
      "|    clip_fraction        | 0.412       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.528       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 4           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.772839506172836\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 221         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019757975 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.622       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0662     |\n",
      "|    value_loss           | 3.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.819047619047616\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 230         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019946583 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.645       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.43        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 3.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -72.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.85287356321839\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 238         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020719435 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0691     |\n",
      "|    value_loss           | 3.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.03333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 246         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020175647 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.557       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 3.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.043010752688176\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 124        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 254        |\n",
      "|    total_timesteps      | 31744      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01803765 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.657      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.38       |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0629    |\n",
      "|    value_loss           | 3.72       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.25\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 262         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018246837 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.131       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 3.83        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.36363636363637\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 271         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017985942 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.6         |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 3.67        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.57843137254902\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 279         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019135483 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.174       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0679     |\n",
      "|    value_loss           | 3.85        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.93333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 287         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017964402 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 3.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.120370370370374\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018015191 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.95        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 3.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.1981981981982\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 124        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 304        |\n",
      "|    total_timesteps      | 37888      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01893234 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.685      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.32       |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0661    |\n",
      "|    value_loss           | 3.66       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.310526315789474\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 312         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020077685 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.01        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 3.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.447863247863246\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017432917 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.643       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0645     |\n",
      "|    value_loss           | 3.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.485\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016204145 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.49        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0621     |\n",
      "|    value_loss           | 3.62        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.53983739837398\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 123        |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 339        |\n",
      "|    total_timesteps      | 41984      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01899767 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.724      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.43       |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0641    |\n",
      "|    value_loss           | 3.55       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.67777777777778\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 123        |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 348        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01911948 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.714      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.992      |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | -0.0648    |\n",
      "|    value_loss           | 3.6        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.82790697674419\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019358654 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.316       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 3.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.910606060606064\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 364         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017318055 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.704       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.31        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 3.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.946666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 371         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016981605 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.79        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 3.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.96086956521739\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 380         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017861031 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.719       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 3.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.00851063829787\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 123        |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 388        |\n",
      "|    total_timesteps      | 48128      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01783805 |\n",
      "|    clip_fraction        | 0.329      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.726      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.44       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0645    |\n",
      "|    value_loss           | 3.61       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -68.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.00138888888889\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 123        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 396        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01653318 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.732      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.647      |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0623    |\n",
      "|    value_loss           | 3.45       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.179591836734694\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 404         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016571512 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0631     |\n",
      "|    value_loss           | 3.55        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.39333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 413         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018283144 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 3.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.55294117647059\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 123        |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 421        |\n",
      "|    total_timesteps      | 52224      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01714775 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.733      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.82       |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.065     |\n",
      "|    value_loss           | 3.42       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.73717948717949\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 429         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018582182 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 3.47        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.86918238993711\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 437         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016988527 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.882       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0637     |\n",
      "|    value_loss           | 3.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.044444444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018740375 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.721       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.884       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0667     |\n",
      "|    value_loss           | 3.54        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.20727272727273\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 452         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019373052 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.153       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 3.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.364285714285714\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 459         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019110598 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.278       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0677     |\n",
      "|    value_loss           | 3.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.538011695906434\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 467         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019455742 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0681     |\n",
      "|    value_loss           | 3.3         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.701149425287355\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 474         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019909287 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.84858757062147\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 482        |\n",
      "|    total_timesteps      | 60416      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01768085 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.739      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.275      |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0655    |\n",
      "|    value_loss           | 3.31       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.958888888888886\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 489         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018852547 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.38        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 3.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.12896174863388\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 496         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017679296 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.21720430107527\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020417076 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.12        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0666     |\n",
      "|    value_loss           | 3.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.34497354497355\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 511         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018123053 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0635     |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.47291666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 519         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018518656 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.01        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.066      |\n",
      "|    value_loss           | 3.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.53435897435897\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 526         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019380467 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.946       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.60808080808081\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 533         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018202009 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.00812    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0668     |\n",
      "|    value_loss           | 3.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.63383084577114\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 540         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018342339 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.989       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0653     |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.63725490196079\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 548         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019435443 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.507       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 3.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.68405797101449\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 555         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017608974 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.514       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.065      |\n",
      "|    value_loss           | 3.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.743809523809524\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020027807 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0704     |\n",
      "|    value_loss           | 3.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.75962441314554\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 571         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019255036 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.329       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0634     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.76759259259259\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 578         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017440524 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.439       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0652     |\n",
      "|    value_loss           | 3.26        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -60.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.76986301369863\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 586         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019239802 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.802       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 3.16        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.75855855855856\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 593         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020066135 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.292       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 2.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.781333333333336\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 600         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019399777 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.501       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 3.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.76929824561404\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -179      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 127       |\n",
      "|    iterations           | 76        |\n",
      "|    time_elapsed         | 608       |\n",
      "|    total_timesteps      | 77824     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0178172 |\n",
      "|    clip_fraction        | 0.354     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.22     |\n",
      "|    explained_variance   | 0.766     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.978     |\n",
      "|    n_updates            | 750       |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 3.07      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -64.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.76190476190476\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 615         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020404384 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.353       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.812820512820515\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -178      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 128       |\n",
      "|    iterations           | 78        |\n",
      "|    time_elapsed         | 622       |\n",
      "|    total_timesteps      | 79872     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0182871 |\n",
      "|    clip_fraction        | 0.362     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.22     |\n",
      "|    explained_variance   | 0.76      |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.694     |\n",
      "|    n_updates            | 770       |\n",
      "|    policy_gradient_loss | -0.066    |\n",
      "|    value_loss           | 2.98      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.88523206751055\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 630         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019823322 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.563       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 3.05        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.9625\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 637        |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01715203 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.783      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.476      |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | -0.0658    |\n",
      "|    value_loss           | 2.8        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.0238683127572\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 645         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021705793 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.709       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.07       |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.06260162601626\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 652         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018756527 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.48        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 3.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.11244979919679\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 659         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020432783 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.533       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0703     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.18412698412698\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 667         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018288068 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.578       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.372549019607845\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 675         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022144496 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1           |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.073      |\n",
      "|    value_loss           | 2.68        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.55503875968992\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 684         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022773093 |\n",
      "|    clip_fraction        | 0.466       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.479       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0722     |\n",
      "|    value_loss           | 2.74        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.75095785440613\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 692         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020598993 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.298       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.92045454545455\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 701         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022488846 |\n",
      "|    clip_fraction        | 0.458       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.12        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0751     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.09662921348315\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 709         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019603143 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.902       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.245925925925924\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 718        |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01980574 |\n",
      "|    clip_fraction        | 0.401      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.798      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.478      |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.07      |\n",
      "|    value_loss           | 2.77       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.39413919413919\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 726         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018603675 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.688       |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.553623188405794\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 734         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018681452 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.257       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.72114695340502\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 743         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014768742 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.497       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0617     |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.86241134751773\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 751         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015887344 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.728       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.00561403508772\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 760         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016563915 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.15902777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 768         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014753744 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.286       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0651     |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.292096219931274\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 776         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014927759 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.389       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0701     |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.42721088435374\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 784         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017254286 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.17       |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00197     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0748     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.561616161616165\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 794         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018732682 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0765     |\n",
      "|    value_loss           | 1.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.68866666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 803         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017980352 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.303       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0773     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -52.32\n",
      "Overall Average Successful Assignments: 45.604608809654785\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8703c01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -184.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 7.666666666666667\n",
      "All assignments history: [3, 4, 10, 7, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    fps             | 163      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.566666666666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -187       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 138        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 2048       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00853355 |\n",
      "|    clip_fraction        | 0.0588     |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | -0.26      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.39       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0522    |\n",
      "|    value_loss           | 17         |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -168.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 12.822222222222223\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -186        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011430418 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0177     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -190.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.216666666666667\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -185       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 31         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01332061 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | 0.00195    |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.51       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0603    |\n",
      "|    value_loss           | 9.82       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 15.066666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014336356 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0459      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.17        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0606     |\n",
      "|    value_loss           | 8.6         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -106.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.144444444444446\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015440722 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0823      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.25        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0631     |\n",
      "|    value_loss           | 7.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.295238095238094\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015970638 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0838      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 7.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -146.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.533333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016996313 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0601     |\n",
      "|    value_loss           | 6.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.05185185185185\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018231213 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0613     |\n",
      "|    value_loss           | 5.78        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -134.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.666666666666668\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019335331 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.757       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0607     |\n",
      "|    value_loss           | 5.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -100.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.175757575757576\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021519616 |\n",
      "|    clip_fraction        | 0.472       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0376      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 4.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -196.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.72222222222222\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 124        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 98         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02209729 |\n",
      "|    clip_fraction        | 0.496      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.196      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.65       |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0638    |\n",
      "|    value_loss           | 4.54       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -190.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.72820512820513\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022898454 |\n",
      "|    clip_fraction        | 0.505       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.249       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.558       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 4.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -158.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.557142857142857\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024937496 |\n",
      "|    clip_fraction        | 0.563       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.18        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0679     |\n",
      "|    value_loss           | 4.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -174.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.09777777777778\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024643535 |\n",
      "|    clip_fraction        | 0.526       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 4.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -188.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.420833333333334\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -184      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 124       |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 131       |\n",
      "|    total_timesteps      | 16384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0240153 |\n",
      "|    clip_fraction        | 0.526     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.27     |\n",
      "|    explained_variance   | 0.48      |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 2.37      |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | -0.069    |\n",
      "|    value_loss           | 3.99      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024374217 |\n",
      "|    clip_fraction        | 0.531       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.903       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 4.1         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -180.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.522222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025391018 |\n",
      "|    clip_fraction        | 0.542       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0714     |\n",
      "|    value_loss           | 3.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -168.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.340350877192982\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 155         |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023712792 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0714     |\n",
      "|    value_loss           | 4.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.10333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 124        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 164        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02012093 |\n",
      "|    clip_fraction        | 0.435      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.586      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.45       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0684    |\n",
      "|    value_loss           | 4.07       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.701587301587303\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 124        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 172        |\n",
      "|    total_timesteps      | 21504      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02295975 |\n",
      "|    clip_fraction        | 0.478      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.594      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.08       |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0691    |\n",
      "|    value_loss           | 3.86       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -62.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.106060606060606\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022181597 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.789       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 3.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.47536231884058\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 124        |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 188        |\n",
      "|    total_timesteps      | 23552      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02309536 |\n",
      "|    clip_fraction        | 0.478      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.632      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.814      |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.072     |\n",
      "|    value_loss           | 3.79       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.788888888888888\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021395504 |\n",
      "|    clip_fraction        | 0.453       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0684     |\n",
      "|    value_loss           | 3.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.928\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 204         |\n",
      "|    total_timesteps      | 25600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018570807 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.42        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 3.77        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.01025641025641\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 212         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020771975 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0682     |\n",
      "|    value_loss           | 3.75        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.839506172839506\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 221         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019726902 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 3.66        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.364285714285714\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019913692 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.797       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 3.52        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.85287356321839\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 236         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019920696 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.186       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 3.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.362222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017759113 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.741       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0647     |\n",
      "|    value_loss           | 3.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.548387096774192\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 252        |\n",
      "|    total_timesteps      | 31744      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01820892 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.712      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.62       |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0667    |\n",
      "|    value_loss           | 3.31       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -116.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.672916666666666\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 261        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01694059 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.731      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.52       |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0614    |\n",
      "|    value_loss           | 3.14       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.18989898989899\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 33792       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016104106 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.709       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 3.25        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.641176470588235\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 277        |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01513356 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.738      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.733      |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.0643    |\n",
      "|    value_loss           | 3.3        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -76.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.097142857142856\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 285         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017314658 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.319       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 3.03        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.47222222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 293         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015745357 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.775       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0664     |\n",
      "|    value_loss           | 3.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.084684684684685\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016136521 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.769       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.8         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0649     |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -56.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.62456140350877\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018884089 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.678       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0695     |\n",
      "|    value_loss           | 2.84        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.244444444444444\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 318         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015689697 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.556       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.81833333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 326         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018093627 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.593       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0731     |\n",
      "|    value_loss           | 2.49        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -54.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.31056910569106\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 334         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016870733 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.79047619047619\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 342         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018730354 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.63        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0731     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.248062015503876\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018936723 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0438     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0737     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -50.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.70454545454545\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 358         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018427894 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.391       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0722     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.07703703703704\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 366         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017848616 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.448       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0743     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -58.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.43333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018861596 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.681       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0742     |\n",
      "|    value_loss           | 2           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -48.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.85390070921986\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 382         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018228428 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0189      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0745     |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.275\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018746829 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.258       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.71700680272109\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 399         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018652104 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 1.93        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.26533333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 407         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018885985 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.444       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0782     |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.81437908496732\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 415         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020598425 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.297       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0793     |\n",
      "|    value_loss           | 1.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.30769230769231\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 423         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019252427 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.274       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.078      |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.666666666666664\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 53         |\n",
      "|    time_elapsed         | 431        |\n",
      "|    total_timesteps      | 54272      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02002696 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.875      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.385      |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0771    |\n",
      "|    value_loss           | 1.8        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.141975308641975\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 439         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019358743 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.157       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0745     |\n",
      "|    value_loss           | 1.79        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.4969696969697\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 447         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020636063 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.49        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.078      |\n",
      "|    value_loss           | 1.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.845238095238095\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019403875 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.367       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0772     |\n",
      "|    value_loss           | 1.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.26081871345029\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 464         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019869331 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.214       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0762     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.71264367816092\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -180      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 125       |\n",
      "|    iterations           | 58        |\n",
      "|    time_elapsed         | 471       |\n",
      "|    total_timesteps      | 59392     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0196969 |\n",
      "|    clip_fraction        | 0.405     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.24     |\n",
      "|    explained_variance   | 0.896     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.169     |\n",
      "|    n_updates            | 570       |\n",
      "|    policy_gradient_loss | -0.0784   |\n",
      "|    value_loss           | 1.72      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.15819209039548\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 479        |\n",
      "|    total_timesteps      | 60416      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02081342 |\n",
      "|    clip_fraction        | 0.409      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.9        |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.107      |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0781    |\n",
      "|    value_loss           | 1.49       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.64\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 488         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019980323 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.138       |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0786     |\n",
      "|    value_loss           | 1.61        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.10710382513661\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 495         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021549068 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.116      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0789     |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.56236559139785\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 503         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019787438 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.266       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0757     |\n",
      "|    value_loss           | 1.53        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.972486772486775\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 512         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021045782 |\n",
      "|    clip_fraction        | 0.421       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.144      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0785     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.39270833333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 520         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020727262 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.316       |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0799     |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.76512820512821\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 528        |\n",
      "|    total_timesteps      | 66560      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01958521 |\n",
      "|    clip_fraction        | 0.41       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0462    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.079     |\n",
      "|    value_loss           | 1.52       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.10808080808081\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 125        |\n",
      "|    iterations           | 66         |\n",
      "|    time_elapsed         | 537        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01801666 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.91       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.173     |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | -0.0777    |\n",
      "|    value_loss           | 1.58       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.492537313432834\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 545         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021163717 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.225      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0767     |\n",
      "|    value_loss           | 1.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.88823529411765\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 552         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021053206 |\n",
      "|    clip_fraction        | 0.43        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0565     |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0794     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.35458937198068\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 560         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021993857 |\n",
      "|    clip_fraction        | 0.44        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0052      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0816     |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.815238095238094\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 125         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 568         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021778986 |\n",
      "|    clip_fraction        | 0.447       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.18        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.08       |\n",
      "|    value_loss           | 1.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.244131455399064\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 576         |\n",
      "|    total_timesteps      | 72704       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022000423 |\n",
      "|    clip_fraction        | 0.439       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.104      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0799     |\n",
      "|    value_loss           | 1.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.650925925925925\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 584        |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02207305 |\n",
      "|    clip_fraction        | 0.451      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.925      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.0971    |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.0805    |\n",
      "|    value_loss           | 1.22       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -4.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.98082191780822\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 592         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021804232 |\n",
      "|    clip_fraction        | 0.457       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0457     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0818     |\n",
      "|    value_loss           | 1.16        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.32612612612613\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 600        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01866072 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.00273    |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0782    |\n",
      "|    value_loss           | 1.4        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.64088888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 608         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020681292 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0822     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0803     |\n",
      "|    value_loss           | 1.45        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.97982456140351\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 76         |\n",
      "|    time_elapsed         | 616        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02018677 |\n",
      "|    clip_fraction        | 0.397      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.934      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.00183   |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | -0.0753    |\n",
      "|    value_loss           | 1.15       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.32121212121212\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 624         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018286131 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.155      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0771     |\n",
      "|    value_loss           | 1.28        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 16.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.67863247863248\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 632         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017477304 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0632      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0716     |\n",
      "|    value_loss           | 1.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.016033755274265\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 640         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016338354 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.121      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0735     |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.376666666666665\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 126        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 648        |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01811355 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.925      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.0076     |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | -0.078     |\n",
      "|    value_loss           | 1.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.754732510288065\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 656         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019425664 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.078      |\n",
      "|    value_loss           | 1.23        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.12357723577236\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 664         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020076953 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0698     |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0814     |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.46265060240964\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 672         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019922063 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.198       |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0795     |\n",
      "|    value_loss           | 1.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.823015873015876\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 682         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019340308 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.187      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0782     |\n",
      "|    value_loss           | 1.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 22.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.15764705882353\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 690         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023429986 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00658     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0838     |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.50077519379845\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 697         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021005077 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.14        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0834     |\n",
      "|    value_loss           | 1.29        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 50.836781609195405\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 704         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022498142 |\n",
      "|    clip_fraction        | 0.461       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0835     |\n",
      "|    value_loss           | 1.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.18712121212121\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 711         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020443754 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0953     |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0816     |\n",
      "|    value_loss           | 1.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 51.60299625468165\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 719         |\n",
      "|    total_timesteps      | 91136       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019981792 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0859     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0807     |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.007407407407406\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 726         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019761331 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.00985     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0809     |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 52.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.40512820512821\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 733         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018081233 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0494     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.079      |\n",
      "|    value_loss           | 1.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 52.77463768115942\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 740         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022020962 |\n",
      "|    clip_fraction        | 0.459       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.199       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0835     |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 46.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.13548387096774\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 748         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022959407 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.158      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0855     |\n",
      "|    value_loss           | 1.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.48085106382979\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -177       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 755        |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01942997 |\n",
      "|    clip_fraction        | 0.401      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.936      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.136     |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0827    |\n",
      "|    value_loss           | 1.13       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 42.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 53.81333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 763         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023254953 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.179      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0864     |\n",
      "|    value_loss           | 1           |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.12638888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 771         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018076055 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.072      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0784     |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.43917525773196\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -176       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 779        |\n",
      "|    total_timesteps      | 99328      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01936515 |\n",
      "|    clip_fraction        | 0.391      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.18      |\n",
      "|    explained_variance   | 0.936      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.143     |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0806    |\n",
      "|    value_loss           | 1.12       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 54.73401360544218\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 787         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021395272 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.0488     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0832     |\n",
      "|    value_loss           | 1.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.00875420875421\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 795         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021908874 |\n",
      "|    clip_fraction        | 0.466       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | -0.101      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0851     |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 55.27133333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -175       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 127        |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 803        |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01800102 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.17      |\n",
      "|    explained_variance   | 0.928      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.0785    |\n",
      "|    value_loss           | 1.25       |\n",
      "----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -44.36\n",
      "Overall Average Successful Assignments: 35.36493794846205\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0409dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -194.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 4.0\n",
      "All assignments history: [9, 7, 3, 6, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -188     |\n",
      "| time/              |          |\n",
      "|    fps             | 497      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_average_success to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_average_success` for environment variables or `env.get_wrapper_attr('get_average_success')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\asdra\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.successful_history to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.successful_history` for environment variables or `env.get_wrapper_attr('successful_history')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -172.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 8.033333333333333\n",
      "All assignments history: []\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080239875 |\n",
      "|    clip_fraction        | 0.0525       |\n",
      "|    clip_range           | 0.15         |\n",
      "|    entropy_loss         | -8.29        |\n",
      "|    explained_variance   | -0.167       |\n",
      "|    learning_rate        | 0.00018      |\n",
      "|    loss                 | 2.28         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0481      |\n",
      "|    value_loss           | 16.7         |\n",
      "------------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 11.333333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 399         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 3072        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012324089 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.158      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -142.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.933333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013265025 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | -0.0209     |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.33        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0587     |\n",
      "|    value_loss           | 9.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -178.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 13.253333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 5120        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013914642 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.0298      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.32        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0569     |\n",
      "|    value_loss           | 9.19        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.033333333333333\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01617932 |\n",
      "|    clip_fraction        | 0.326      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.29      |\n",
      "|    explained_variance   | 0.0406     |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.29       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0609    |\n",
      "|    value_loss           | 7.59       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -156.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.438095238095238\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 376         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016899325 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0477      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0615     |\n",
      "|    value_loss           | 7.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -152.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.966666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016891267 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0576      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 6.55        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -184.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 14.192592592592593\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -185        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018541045 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.0688      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0592     |\n",
      "|    value_loss           | 5.69        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -88.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 16.866666666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020075016 |\n",
      "|    clip_fraction        | 0.452       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.0798      |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0633     |\n",
      "|    value_loss           | 5.51        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -150.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.10909090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022836145 |\n",
      "|    clip_fraction        | 0.513       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.56        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.068      |\n",
      "|    value_loss           | 4.72        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -154.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.166666666666668\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -184      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 372       |\n",
      "|    iterations           | 12        |\n",
      "|    time_elapsed         | 33        |\n",
      "|    total_timesteps      | 12288     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0222401 |\n",
      "|    clip_fraction        | 0.494     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.27     |\n",
      "|    explained_variance   | 0.17      |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.563     |\n",
      "|    n_updates            | 110       |\n",
      "|    policy_gradient_loss | -0.0638   |\n",
      "|    value_loss           | 4.58      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -148.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 17.343589743589742\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -184       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 35         |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02392414 |\n",
      "|    clip_fraction        | 0.521      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.25       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 2.51       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0653    |\n",
      "|    value_loss           | 4.38       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.266666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 371         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023454899 |\n",
      "|    clip_fraction        | 0.513       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.769       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 4.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -140.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 18.586666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 372         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025298584 |\n",
      "|    clip_fraction        | 0.543       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.72        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0672     |\n",
      "|    value_loss           | 4.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 19.975\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 372         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025953589 |\n",
      "|    clip_fraction        | 0.547       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.45        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 4.12        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 20.76078431372549\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 371         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026992813 |\n",
      "|    clip_fraction        | 0.574       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.25        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 4.14        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -96.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 21.703703703703702\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 371         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025853176 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.5         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 3.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.431578947368422\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 370         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022294927 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.214       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0699     |\n",
      "|    value_loss           | 4.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 22.713333333333335\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 370         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026112791 |\n",
      "|    clip_fraction        | 0.566       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.649       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0771     |\n",
      "|    value_loss           | 4.11        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -110.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.203174603174602\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 371         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025272701 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.64        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0733     |\n",
      "|    value_loss           | 3.96        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 23.657575757575756\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 371         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024736755 |\n",
      "|    clip_fraction        | 0.493       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.608       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.516       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0718     |\n",
      "|    value_loss           | 4.02        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.142028985507245\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 370         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023589224 |\n",
      "|    clip_fraction        | 0.493       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.622       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.38        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 3.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -138.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.091666666666665\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 66         |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02223998 |\n",
      "|    clip_fraction        | 0.454      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.27      |\n",
      "|    explained_variance   | 0.634      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.773      |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0693    |\n",
      "|    value_loss           | 3.92       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.221333333333334\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -184      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 369       |\n",
      "|    iterations           | 25        |\n",
      "|    time_elapsed         | 69        |\n",
      "|    total_timesteps      | 25600     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0226225 |\n",
      "|    clip_fraction        | 0.47      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.27     |\n",
      "|    explained_variance   | 0.638     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 1         |\n",
      "|    n_updates            | 240       |\n",
      "|    policy_gradient_loss | -0.0704   |\n",
      "|    value_loss           | 3.91      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.48205128205128\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019362355 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.681       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0648     |\n",
      "|    value_loss           | 3.92        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.780246913580246\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022188483 |\n",
      "|    clip_fraction        | 0.434       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0692     |\n",
      "|    value_loss           | 3.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -120.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 24.947619047619046\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 77         |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01922727 |\n",
      "|    clip_fraction        | 0.401      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.661      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.19       |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0686    |\n",
      "|    value_loss           | 3.94       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -128.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.020689655172415\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019199502 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 2.23        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 3.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -132.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.031111111111112\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019046172 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0683     |\n",
      "|    value_loss           | 3.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -122.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.141935483870967\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020300949 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.698       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0673     |\n",
      "|    value_loss           | 3.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -126.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.216666666666665\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -184        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020078316 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.31        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0658     |\n",
      "|    value_loss           | 3.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -114.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.426262626262627\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -183       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 91         |\n",
      "|    total_timesteps      | 33792      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02053695 |\n",
      "|    clip_fraction        | 0.408      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.26      |\n",
      "|    explained_variance   | 0.671      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.946      |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0697    |\n",
      "|    value_loss           | 3.84       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -98.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 25.788235294117648\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020803621 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0713     |\n",
      "|    value_loss           | 3.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -82.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.253333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020069089 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0709     |\n",
      "|    value_loss           | 3.7         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -80.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.712962962962962\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019518606 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.589       |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0676     |\n",
      "|    value_loss           | 3.73        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -118.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.792792792792792\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 37888       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017629944 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.282       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0644     |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 26.850877192982455\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018483989 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.417       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0661     |\n",
      "|    value_loss           | 3.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -108.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.042735042735043\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017542731 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.391       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 3.59        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -124.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.078333333333333\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019070564 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.398       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 3.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -104.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.27479674796748\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -183        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018452603 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.99        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0712     |\n",
      "|    value_loss           | 3.46        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -86.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.5968253968254\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -183      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 367       |\n",
      "|    iterations           | 42        |\n",
      "|    time_elapsed         | 117       |\n",
      "|    total_timesteps      | 43008     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0184464 |\n",
      "|    clip_fraction        | 0.365     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.25     |\n",
      "|    explained_variance   | 0.752     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.366     |\n",
      "|    n_updates            | 410       |\n",
      "|    policy_gradient_loss | -0.0696   |\n",
      "|    value_loss           | 3.1       |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -112.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.71627906976744\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 44032       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018326785 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.343       |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 3.3         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -90.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 27.98939393939394\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018492322 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0717     |\n",
      "|    value_loss           | 3.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -94.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.214814814814815\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019624071 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.368       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 2.99        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -78.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.544927536231885\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019659532 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.66        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0696     |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 28.95035460992908\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019838288 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.769       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.154       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 3.1         |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -66.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.33888888888889\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -182       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 134        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02072924 |\n",
      "|    clip_fraction        | 0.401      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.25      |\n",
      "|    explained_variance   | 0.774      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.628      |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0718    |\n",
      "|    value_loss           | 2.97       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -74.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 29.654421768707483\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 136         |\n",
      "|    total_timesteps      | 50176       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019373314 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.477       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0703     |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.214666666666666\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -182        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019726617 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.873       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 2.82        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 30.78562091503268\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 52224       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019500822 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.891       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0752     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.294871794871796\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 145         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018819068 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.755       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0733     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -44.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 31.743396226415094\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018968359 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.814       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0697     |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.262962962962966\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 150        |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01965513 |\n",
      "|    clip_fraction        | 0.389      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.794      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.407      |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.0732    |\n",
      "|    value_loss           | 2.94       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 32.76121212121212\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 56320       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017175354 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.774       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0711     |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.233333333333334\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -181      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 366       |\n",
      "|    iterations           | 56        |\n",
      "|    time_elapsed         | 156       |\n",
      "|    total_timesteps      | 57344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0191891 |\n",
      "|    clip_fraction        | 0.386     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.24     |\n",
      "|    explained_variance   | 0.8       |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.685     |\n",
      "|    n_updates            | 550       |\n",
      "|    policy_gradient_loss | -0.0739   |\n",
      "|    value_loss           | 2.74      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 33.70175438596491\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 58368       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017705794 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.574       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0688     |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.14252873563218\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 58         |\n",
      "|    time_elapsed         | 162        |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01832936 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.795      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.172      |\n",
      "|    n_updates            | 570        |\n",
      "|    policy_gradient_loss | -0.0694    |\n",
      "|    value_loss           | 3.04       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.54011299435028\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 59         |\n",
      "|    time_elapsed         | 165        |\n",
      "|    total_timesteps      | 60416      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01740751 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.799      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.05       |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0703    |\n",
      "|    value_loss           | 2.94       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -30.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 34.96\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018151205 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.75        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0707     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.389071038251366\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 62464       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017904464 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.429       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0706     |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 35.79247311827957\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 173         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020248959 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.804       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0727     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.196825396825396\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 176         |\n",
      "|    total_timesteps      | 64512       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019379444 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.129       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 2.46        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -20.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.614583333333336\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 179        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01939097 |\n",
      "|    clip_fraction        | 0.4        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.24      |\n",
      "|    explained_variance   | 0.822      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.432      |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0743    |\n",
      "|    value_loss           | 2.59       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -40.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 36.92717948717949\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -181        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 182         |\n",
      "|    total_timesteps      | 66560       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017470267 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.283       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 2.48        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.34747474747475\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -181       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 66         |\n",
      "|    time_elapsed         | 184        |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02031543 |\n",
      "|    clip_fraction        | 0.382      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.82       |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 1.17       |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | -0.0715    |\n",
      "|    value_loss           | 2.76       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -26.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 37.70845771144278\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019939177 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.491       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0757     |\n",
      "|    value_loss           | 2.64        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.062745098039215\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018176146 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.485       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.391304347826086\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 70656       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019042008 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.862       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0716     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 38.76285714285714\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 195         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019032307 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.291       |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0752     |\n",
      "|    value_loss           | 2.44        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.2093896713615\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 198        |\n",
      "|    total_timesteps      | 72704      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01803125 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.23      |\n",
      "|    explained_variance   | 0.841      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.901      |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0732    |\n",
      "|    value_loss           | 2.34       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 39.66481481481482\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 200         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020946003 |\n",
      "|    clip_fraction        | 0.419       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.673       |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0765     |\n",
      "|    value_loss           | 2.33        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -8.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.04109589041096\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019821778 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.268       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0734     |\n",
      "|    value_loss           | 2.29        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: -32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.296396396396396\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 206        |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01729567 |\n",
      "|    clip_fraction        | 0.336      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.836      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.218      |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0719    |\n",
      "|    value_loss           | 2.52       |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.54488888888889\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 209         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018661547 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.183       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0713     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -24.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 40.833333333333336\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -180       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 76         |\n",
      "|    time_elapsed         | 212        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01997067 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.22      |\n",
      "|    explained_variance   | 0.828      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.518      |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | -0.0756    |\n",
      "|    value_loss           | 2.5        |\n",
      "----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -6.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.1948051948052\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019989215 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.332       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0746     |\n",
      "|    value_loss           | 2.23        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -18.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.4974358974359\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019328423 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.493       |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0718     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -10.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 41.81772151898734\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 80896       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020305395 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0739     |\n",
      "|    value_loss           | 2.22        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: -14.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.11333333333334\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019706834 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.923       |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0736     |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 0.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.46337448559671\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -179       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 226        |\n",
      "|    total_timesteps      | 82944      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01803228 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.843      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.193      |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0716    |\n",
      "|    value_loss           | 2.27       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 2.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 42.8089430894309\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 229         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017162599 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.666       |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0713     |\n",
      "|    value_loss           | 2.42        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.18313253012048\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 84992       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020706423 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.0819      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0719     |\n",
      "|    value_loss           | 2.17        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 12.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 43.56428571428572\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 234         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017580492 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.791       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.072      |\n",
      "|    value_loss           | 2.24        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.00078431372549\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 87040       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015988199 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.35        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0685     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.43178294573644\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -179        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017453954 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.476       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0713     |\n",
      "|    value_loss           | 2.15        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 44.8544061302682\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 243         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018478015 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.575       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0753     |\n",
      "|    value_loss           | 2.35        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.28409090909091\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 245         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019310063 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.133       |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0723     |\n",
      "|    value_loss           | 2.09        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 45.70786516853933\n",
      "All assignments history: []\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -178       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 248        |\n",
      "|    total_timesteps      | 91136      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01948407 |\n",
      "|    clip_fraction        | 0.379      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | -8.21      |\n",
      "|    explained_variance   | 0.855      |\n",
      "|    learning_rate        | 0.00018    |\n",
      "|    loss                 | 0.734      |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0734    |\n",
      "|    value_loss           | 2.19       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.080740740740744\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -178        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 251         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017785287 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.076      |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 28.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.45787545787546\n",
      "All assignments history: []\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -178      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 366       |\n",
      "|    iterations           | 91        |\n",
      "|    time_elapsed         | 254       |\n",
      "|    total_timesteps      | 93184     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0188676 |\n",
      "|    clip_fraction        | 0.373     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | -8.2      |\n",
      "|    explained_variance   | 0.863     |\n",
      "|    learning_rate        | 0.00018   |\n",
      "|    loss                 | 0.171     |\n",
      "|    n_updates            | 900       |\n",
      "|    policy_gradient_loss | -0.0717   |\n",
      "|    value_loss           | 2.07      |\n",
      "---------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 46.85072463768116\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 257         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020258464 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.255       |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0747     |\n",
      "|    value_loss           | 2.06        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 32.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.215053763440864\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 260         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018427383 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.221       |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0728     |\n",
      "|    value_loss           | 2.34        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.59078014184397\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 263         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017086163 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.2        |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.164       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0715     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 47.96771929824561\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 265         |\n",
      "|    total_timesteps      | 97280       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014177536 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.613       |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0671     |\n",
      "|    value_loss           | 2.18        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 38.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.34097222222222\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012887241 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.19       |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.617       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0622     |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 48.68041237113402\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 271         |\n",
      "|    total_timesteps      | 99328       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016332176 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.18        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0702     |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.02925170068027\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 274         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015705856 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.458       |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0678     |\n",
      "|    value_loss           | 1.95        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 36.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.370370370370374\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -177        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 277         |\n",
      "|    total_timesteps      | 101376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016110431 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.517       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0703     |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n",
      "-------- Rollout Summary --------\n",
      "Total mean reward: 34.0\n",
      "Standard deviation of reward: 0.0\n",
      "Average successful assignments: 49.69866666666667\n",
      "All assignments history: []\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 279         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015532577 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.00018     |\n",
      "|    loss                 | 0.568       |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0687     |\n",
      "|    value_loss           | 1.98        |\n",
      "-----------------------------------------\n",
      "-------- Training Summary --------\n",
      "Overall Average Mean Reward: -60.6\n",
      "Overall Average Successful Assignments: 31.438979827942898\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Load datasets\n",
    "vehicles_df = pd.read_csv('VehicleTrainingDataset_Noisy_0.1.csv')\n",
    "tasks_df = pd.read_csv('RandomTasks200.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "tasks_df.rename(columns={\n",
    "    'Required_RAM': 'RAM',\n",
    "    'Required_Storage': 'storage',\n",
    "    'Minimum_Trust_Factor': 'Trustfactor',\n",
    "    'Max_Distance': 'Distance',\n",
    "    'Min_Transmission_Rate': 'TransmissionRate'\n",
    "}, inplace=True)\n",
    "\n",
    "# Define the Gym environment for task allocation\n",
    "class TaskAllocationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, vehicles, tasks):\n",
    "        super(TaskAllocationEnv, self).__init__()\n",
    "        self.vehicles = vehicles\n",
    "        self.tasks = tasks\n",
    "        self.action_space = spaces.Discrete(len(vehicles))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(tasks.shape[1],), dtype=np.float32)\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0\n",
    "        self.successful_history = []  # Added to track successful assignments\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_task = 0\n",
    "        self.successful_assignments = 0  # Reset successful assignments\n",
    "        #print(f\"Resetting environment. Starting new episode.\")\n",
    "        return self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        task = self.tasks.iloc[self.current_task]\n",
    "        vehicle = self.vehicles.iloc[action]\n",
    "        reward = 0\n",
    "\n",
    "        # Check if the vehicle meets all the task requirements\n",
    "        meets_requirements = (\n",
    "            vehicle['RAM'] >= task['RAM'] and\n",
    "            vehicle['storage'] >= task['storage'] and\n",
    "            vehicle['Trustfactor'] >= task['Trustfactor'] and\n",
    "            vehicle['Distance'] <= task['Distance'] and\n",
    "            vehicle['TransmissionRate'] >= task['TransmissionRate']\n",
    "        )\n",
    "        reward = 1 if meets_requirements else -1\n",
    "\n",
    "        self.successful_assignments += reward > 0\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= len(self.tasks)\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.tasks.iloc[self.current_task].values.astype(np.float32)\n",
    "        else:\n",
    "            next_state = np.zeros(self.observation_space.shape[0])\n",
    "            self.successful_history.append(self.successful_assignments)\n",
    "            #print(f\"Episode completed. Successful assignments: {self.successful_assignments}.\")\n",
    "            self.successful_assignments = 0  # Reset for next episode\n",
    "\n",
    "        # Detailed printout of state, action, reward\n",
    "        \n",
    "        #lines below can be uncommented for a more detailed output\n",
    "        #print(f\"Task: {task.to_dict()}\")\n",
    "        #print(f\"Chosen Vehicle: {vehicle.to_dict()}\")\n",
    "        #print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def get_average_success(self):\n",
    "        return np.mean(self.successful_history) if self.successful_history else 0\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.total_rewards = 0\n",
    "        self.total_assignments = 0\n",
    "        self.num_episodes = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.model.get_env(), n_eval_episodes=10)\n",
    "        average_assignments = self.env.get_attr('get_average_success')[0]()\n",
    "        self.total_rewards += mean_reward\n",
    "        self.total_assignments += average_assignments\n",
    "        self.num_episodes += 1\n",
    "        \n",
    "        print(\"-------- Rollout Summary --------\")\n",
    "        print(f\"Total mean reward: {mean_reward}\")\n",
    "        print(f\"Standard deviation of reward: {std_reward}\")\n",
    "        print(f\"Average successful assignments: {average_assignments}\")\n",
    "        print(\"All assignments history:\", self.env.envs[0].successful_history)\n",
    "        self.env.envs[0].successful_history = []  # Reset history after each iteration\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        average_total_reward = self.total_rewards / self.num_episodes\n",
    "        average_total_assignments = self.total_assignments / self.num_episodes\n",
    "        print(\"-------- Training Summary --------\")\n",
    "        print(f\"Overall Average Mean Reward: {average_total_reward}\")\n",
    "        print(f\"Overall Average Successful Assignments: {average_total_assignments}\")\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "env = make_vec_env(lambda: TaskAllocationEnv(vehicles_df, tasks_df), n_envs=1)\n",
    "\n",
    "# Initialize and train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            n_steps=1024, batch_size=128, n_epochs=10, learning_rate=0.00018,\n",
    "            gamma=0.96, gae_lambda=0.87, clip_range=0.15, ent_coef=0.07)\n",
    "\n",
    "callback = CustomCallback(env)  # Use custom callback for detailed tracking and logging\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.learn(total_timesteps=1024*100, callback=callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_task_allocation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d60c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
